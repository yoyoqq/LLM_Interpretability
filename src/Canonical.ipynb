{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e02328e-108c-4cac-93ed-fcfda9e63ade",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# BASICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c48f1f61-c7d6-491f-8dc0-16d0a328c080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available ✅\n",
      "Device count: 1\n",
      "Using device: 0\n",
      "Device name: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "# !nvidia-smi\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA available ✅\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Using device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA not available ❌\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10da7ba1-eb35-436d-ab5f-8825414da695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "494d4098-9183-4445-abfe-e29d19f35dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEURONPEDIA_KEY = \"sk-np-BwxFua0jEx2cNSqsZPVlqmsfPgDKi47oEo7HAWXxiU00\"\n",
    "GEMMA2B_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fca8ba3-38ea-4278-88a3-8dbae83110a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PATH = \"/users/k24086575/inf_narrative_msc/k24086575\"\n",
    "os.environ[\"HF_HOME\"] = PATH  # e.g., $SCRATCH/hf_models\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "773eb8ab-ec5c-4046-8ce6-f013ce5b8156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x70814dd9f650>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "login(token=GEMMA2B_KEY)\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4459868-6f0e-48b6-938e-f09c3e70190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_dataset(path):\n",
    "    # df = pd.read_csv(path)\n",
    "    # df = df.dropna(subset=[\"prompt\"])  # Adjust column name\n",
    "    df = pd.read_csv(path, encoding=\"utf-8-sig\", quoting=1)\n",
    "    return df\n",
    "\n",
    "def save_dataset(path):\n",
    "    small_df.to_csv(path, encoding=\"utf-8-sig\", quoting=1, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d37ac0f-5702-4039-8f4f-7060d76aa4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8552cc1e-7f86-4c26-94b0-27f60abaa4d2",
   "metadata": {},
   "source": [
    "# INITALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb7d76c7-42ba-4ce2-9381-35099a57ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prompt_with_sae(prompt, model, sae, hook_point=None, topk_mean=10, topk_token=3):\n",
    "    \"\"\"\n",
    "    Analyzes a prompt using a model + SAE to extract top mean-activated features,\n",
    "    token-level top features, and logits.\n",
    "    \"\"\"\n",
    "    if hook_point is None:\n",
    "        hook_point = sae.cfg.hook_name\n",
    "\n",
    "    # Run model with cache\n",
    "    logits, cache = model.run_with_cache(prompt, prepend_bos=True)\n",
    "\n",
    "    # Encode activations from residuals using SAE\n",
    "    residual_activations = cache[hook_point]  # shape: [1, seq_len, d_model]\n",
    "    feature_acts = sae.encode(residual_activations)  # shape: [1, seq_len, d_sae]\n",
    "\n",
    "    # Get top-k features by mean activation across sequence\n",
    "    mean_acts = feature_acts.mean(dim=0)  # [d_sae]\n",
    "    top_vals, top_indices = torch.topk(mean_acts, topk_mean)\n",
    "    top_mean_vals = top_vals.tolist()\n",
    "    top_mean_ids = top_indices.tolist()\n",
    "\n",
    "    # Get top-k features for each token (flattened)\n",
    "    top_token_feats = torch.topk(feature_acts.squeeze(), topk_token).indices  # shape: [seq_len, topk]\n",
    "    token_feature_ids = torch.flatten(top_token_feats).tolist()\n",
    "\n",
    "    # reduce memory from flaot to int \n",
    "    for i in range(len(top_mean_vals)):\n",
    "        for j in range(len(top_mean_vals[i])):\n",
    "            top_mean_vals[i][j] = round(top_mean_vals[i][j], 2) \n",
    "    return top_mean_ids, top_mean_vals, token_feature_ids, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64107c15-e72e-46c1-b7d5-430498b35d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/volumes/hpc_data_prj/inf_narrative_msc/e8efa787-4d41-448d-a7aa-814b8f0fac1e/k24086575/jvenv/lib/python3.11/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65de9d610e9341d380a7c99435465de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "MODEL_NAME = \"gemma-2b\"\n",
    "# MODEL_NAME = \"gemma-2-2b\"\n",
    "model = HookedTransformer.from_pretrained(MODEL_NAME, device=\"cuda\")  # or CPU if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34cbe440-3b57-4233-9e92-76817793b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "re = \"gemma-scope-2b-pt-mlp-canonical\"\n",
    "sa = \"layer_0/width_16k/canonical\"\n",
    "sae = SAE.from_pretrained(release=re, sae_id=sa, device=\"cuda\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b71b34fc-65ab-4206-903d-d03702a844f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blocks.0.hook_mlp_out'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae.cfg.hook_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f577c3b-8ac6-483b-b343-bb0c2aaf7fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hook_embed', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.ln1_post.hook_scale', 'blocks.0.ln1_post.hook_normalized', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.ln2_post.hook_scale', 'blocks.0.ln2_post.hook_normalized', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_z', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_result', 'blocks.0.attn.hook_rot_k', 'blocks.0.attn.hook_rot_q', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_pre_linear', 'blocks.0.mlp.hook_post', 'blocks.0.hook_attn_in', 'blocks.0.hook_q_input', 'blocks.0.hook_k_input', 'blocks.0.hook_v_input', 'blocks.0.hook_mlp_in', 'blocks.0.hook_attn_out', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_pre', 'blocks.0.hook_resid_mid', 'blocks.0.hook_resid_post', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.ln1_post.hook_scale', 'blocks.1.ln1_post.hook_normalized', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.ln2_post.hook_scale', 'blocks.1.ln2_post.hook_normalized', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_z', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_result', 'blocks.1.attn.hook_rot_k', 'blocks.1.attn.hook_rot_q', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_pre_linear', 'blocks.1.mlp.hook_post', 'blocks.1.hook_attn_in', 'blocks.1.hook_q_input', 'blocks.1.hook_k_input', 'blocks.1.hook_v_input', 'blocks.1.hook_mlp_in', 'blocks.1.hook_attn_out', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_pre', 'blocks.1.hook_resid_mid', 'blocks.1.hook_resid_post', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.ln1_post.hook_scale', 'blocks.2.ln1_post.hook_normalized', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.ln2_post.hook_scale', 'blocks.2.ln2_post.hook_normalized', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_z', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_result', 'blocks.2.attn.hook_rot_k', 'blocks.2.attn.hook_rot_q', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_pre_linear', 'blocks.2.mlp.hook_post', 'blocks.2.hook_attn_in', 'blocks.2.hook_q_input', 'blocks.2.hook_k_input', 'blocks.2.hook_v_input', 'blocks.2.hook_mlp_in', 'blocks.2.hook_attn_out', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_pre', 'blocks.2.hook_resid_mid', 'blocks.2.hook_resid_post', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.ln1_post.hook_scale', 'blocks.3.ln1_post.hook_normalized', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.ln2_post.hook_scale', 'blocks.3.ln2_post.hook_normalized', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_z', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_result', 'blocks.3.attn.hook_rot_k', 'blocks.3.attn.hook_rot_q', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_pre_linear', 'blocks.3.mlp.hook_post', 'blocks.3.hook_attn_in', 'blocks.3.hook_q_input', 'blocks.3.hook_k_input', 'blocks.3.hook_v_input', 'blocks.3.hook_mlp_in', 'blocks.3.hook_attn_out', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_pre', 'blocks.3.hook_resid_mid', 'blocks.3.hook_resid_post', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.ln1_post.hook_scale', 'blocks.4.ln1_post.hook_normalized', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.ln2_post.hook_scale', 'blocks.4.ln2_post.hook_normalized', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_z', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_result', 'blocks.4.attn.hook_rot_k', 'blocks.4.attn.hook_rot_q', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_pre_linear', 'blocks.4.mlp.hook_post', 'blocks.4.hook_attn_in', 'blocks.4.hook_q_input', 'blocks.4.hook_k_input', 'blocks.4.hook_v_input', 'blocks.4.hook_mlp_in', 'blocks.4.hook_attn_out', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_pre', 'blocks.4.hook_resid_mid', 'blocks.4.hook_resid_post', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.ln1_post.hook_scale', 'blocks.5.ln1_post.hook_normalized', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.ln2_post.hook_scale', 'blocks.5.ln2_post.hook_normalized', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_z', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_result', 'blocks.5.attn.hook_rot_k', 'blocks.5.attn.hook_rot_q', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_pre_linear', 'blocks.5.mlp.hook_post', 'blocks.5.hook_attn_in', 'blocks.5.hook_q_input', 'blocks.5.hook_k_input', 'blocks.5.hook_v_input', 'blocks.5.hook_mlp_in', 'blocks.5.hook_attn_out', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_pre', 'blocks.5.hook_resid_mid', 'blocks.5.hook_resid_post', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.ln1_post.hook_scale', 'blocks.6.ln1_post.hook_normalized', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.ln2_post.hook_scale', 'blocks.6.ln2_post.hook_normalized', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_z', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_result', 'blocks.6.attn.hook_rot_k', 'blocks.6.attn.hook_rot_q', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_pre_linear', 'blocks.6.mlp.hook_post', 'blocks.6.hook_attn_in', 'blocks.6.hook_q_input', 'blocks.6.hook_k_input', 'blocks.6.hook_v_input', 'blocks.6.hook_mlp_in', 'blocks.6.hook_attn_out', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_pre', 'blocks.6.hook_resid_mid', 'blocks.6.hook_resid_post', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.ln1_post.hook_scale', 'blocks.7.ln1_post.hook_normalized', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.ln2_post.hook_scale', 'blocks.7.ln2_post.hook_normalized', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_z', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_result', 'blocks.7.attn.hook_rot_k', 'blocks.7.attn.hook_rot_q', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_pre_linear', 'blocks.7.mlp.hook_post', 'blocks.7.hook_attn_in', 'blocks.7.hook_q_input', 'blocks.7.hook_k_input', 'blocks.7.hook_v_input', 'blocks.7.hook_mlp_in', 'blocks.7.hook_attn_out', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_pre', 'blocks.7.hook_resid_mid', 'blocks.7.hook_resid_post', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.ln1_post.hook_scale', 'blocks.8.ln1_post.hook_normalized', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.ln2_post.hook_scale', 'blocks.8.ln2_post.hook_normalized', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_z', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_result', 'blocks.8.attn.hook_rot_k', 'blocks.8.attn.hook_rot_q', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_pre_linear', 'blocks.8.mlp.hook_post', 'blocks.8.hook_attn_in', 'blocks.8.hook_q_input', 'blocks.8.hook_k_input', 'blocks.8.hook_v_input', 'blocks.8.hook_mlp_in', 'blocks.8.hook_attn_out', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_pre', 'blocks.8.hook_resid_mid', 'blocks.8.hook_resid_post', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.ln1_post.hook_scale', 'blocks.9.ln1_post.hook_normalized', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.ln2_post.hook_scale', 'blocks.9.ln2_post.hook_normalized', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_z', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_result', 'blocks.9.attn.hook_rot_k', 'blocks.9.attn.hook_rot_q', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_pre_linear', 'blocks.9.mlp.hook_post', 'blocks.9.hook_attn_in', 'blocks.9.hook_q_input', 'blocks.9.hook_k_input', 'blocks.9.hook_v_input', 'blocks.9.hook_mlp_in', 'blocks.9.hook_attn_out', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_pre', 'blocks.9.hook_resid_mid', 'blocks.9.hook_resid_post', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.ln1_post.hook_scale', 'blocks.10.ln1_post.hook_normalized', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.ln2_post.hook_scale', 'blocks.10.ln2_post.hook_normalized', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_z', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_result', 'blocks.10.attn.hook_rot_k', 'blocks.10.attn.hook_rot_q', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_pre_linear', 'blocks.10.mlp.hook_post', 'blocks.10.hook_attn_in', 'blocks.10.hook_q_input', 'blocks.10.hook_k_input', 'blocks.10.hook_v_input', 'blocks.10.hook_mlp_in', 'blocks.10.hook_attn_out', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_pre', 'blocks.10.hook_resid_mid', 'blocks.10.hook_resid_post', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.ln1_post.hook_scale', 'blocks.11.ln1_post.hook_normalized', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.ln2_post.hook_scale', 'blocks.11.ln2_post.hook_normalized', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_z', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_result', 'blocks.11.attn.hook_rot_k', 'blocks.11.attn.hook_rot_q', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_pre_linear', 'blocks.11.mlp.hook_post', 'blocks.11.hook_attn_in', 'blocks.11.hook_q_input', 'blocks.11.hook_k_input', 'blocks.11.hook_v_input', 'blocks.11.hook_mlp_in', 'blocks.11.hook_attn_out', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_pre', 'blocks.11.hook_resid_mid', 'blocks.11.hook_resid_post', 'blocks.12.ln1.hook_scale', 'blocks.12.ln1.hook_normalized', 'blocks.12.ln1_post.hook_scale', 'blocks.12.ln1_post.hook_normalized', 'blocks.12.ln2.hook_scale', 'blocks.12.ln2.hook_normalized', 'blocks.12.ln2_post.hook_scale', 'blocks.12.ln2_post.hook_normalized', 'blocks.12.attn.hook_k', 'blocks.12.attn.hook_q', 'blocks.12.attn.hook_v', 'blocks.12.attn.hook_z', 'blocks.12.attn.hook_attn_scores', 'blocks.12.attn.hook_pattern', 'blocks.12.attn.hook_result', 'blocks.12.attn.hook_rot_k', 'blocks.12.attn.hook_rot_q', 'blocks.12.mlp.hook_pre', 'blocks.12.mlp.hook_pre_linear', 'blocks.12.mlp.hook_post', 'blocks.12.hook_attn_in', 'blocks.12.hook_q_input', 'blocks.12.hook_k_input', 'blocks.12.hook_v_input', 'blocks.12.hook_mlp_in', 'blocks.12.hook_attn_out', 'blocks.12.hook_mlp_out', 'blocks.12.hook_resid_pre', 'blocks.12.hook_resid_mid', 'blocks.12.hook_resid_post', 'blocks.13.ln1.hook_scale', 'blocks.13.ln1.hook_normalized', 'blocks.13.ln1_post.hook_scale', 'blocks.13.ln1_post.hook_normalized', 'blocks.13.ln2.hook_scale', 'blocks.13.ln2.hook_normalized', 'blocks.13.ln2_post.hook_scale', 'blocks.13.ln2_post.hook_normalized', 'blocks.13.attn.hook_k', 'blocks.13.attn.hook_q', 'blocks.13.attn.hook_v', 'blocks.13.attn.hook_z', 'blocks.13.attn.hook_attn_scores', 'blocks.13.attn.hook_pattern', 'blocks.13.attn.hook_result', 'blocks.13.attn.hook_rot_k', 'blocks.13.attn.hook_rot_q', 'blocks.13.mlp.hook_pre', 'blocks.13.mlp.hook_pre_linear', 'blocks.13.mlp.hook_post', 'blocks.13.hook_attn_in', 'blocks.13.hook_q_input', 'blocks.13.hook_k_input', 'blocks.13.hook_v_input', 'blocks.13.hook_mlp_in', 'blocks.13.hook_attn_out', 'blocks.13.hook_mlp_out', 'blocks.13.hook_resid_pre', 'blocks.13.hook_resid_mid', 'blocks.13.hook_resid_post', 'blocks.14.ln1.hook_scale', 'blocks.14.ln1.hook_normalized', 'blocks.14.ln1_post.hook_scale', 'blocks.14.ln1_post.hook_normalized', 'blocks.14.ln2.hook_scale', 'blocks.14.ln2.hook_normalized', 'blocks.14.ln2_post.hook_scale', 'blocks.14.ln2_post.hook_normalized', 'blocks.14.attn.hook_k', 'blocks.14.attn.hook_q', 'blocks.14.attn.hook_v', 'blocks.14.attn.hook_z', 'blocks.14.attn.hook_attn_scores', 'blocks.14.attn.hook_pattern', 'blocks.14.attn.hook_result', 'blocks.14.attn.hook_rot_k', 'blocks.14.attn.hook_rot_q', 'blocks.14.mlp.hook_pre', 'blocks.14.mlp.hook_pre_linear', 'blocks.14.mlp.hook_post', 'blocks.14.hook_attn_in', 'blocks.14.hook_q_input', 'blocks.14.hook_k_input', 'blocks.14.hook_v_input', 'blocks.14.hook_mlp_in', 'blocks.14.hook_attn_out', 'blocks.14.hook_mlp_out', 'blocks.14.hook_resid_pre', 'blocks.14.hook_resid_mid', 'blocks.14.hook_resid_post', 'blocks.15.ln1.hook_scale', 'blocks.15.ln1.hook_normalized', 'blocks.15.ln1_post.hook_scale', 'blocks.15.ln1_post.hook_normalized', 'blocks.15.ln2.hook_scale', 'blocks.15.ln2.hook_normalized', 'blocks.15.ln2_post.hook_scale', 'blocks.15.ln2_post.hook_normalized', 'blocks.15.attn.hook_k', 'blocks.15.attn.hook_q', 'blocks.15.attn.hook_v', 'blocks.15.attn.hook_z', 'blocks.15.attn.hook_attn_scores', 'blocks.15.attn.hook_pattern', 'blocks.15.attn.hook_result', 'blocks.15.attn.hook_rot_k', 'blocks.15.attn.hook_rot_q', 'blocks.15.mlp.hook_pre', 'blocks.15.mlp.hook_pre_linear', 'blocks.15.mlp.hook_post', 'blocks.15.hook_attn_in', 'blocks.15.hook_q_input', 'blocks.15.hook_k_input', 'blocks.15.hook_v_input', 'blocks.15.hook_mlp_in', 'blocks.15.hook_attn_out', 'blocks.15.hook_mlp_out', 'blocks.15.hook_resid_pre', 'blocks.15.hook_resid_mid', 'blocks.15.hook_resid_post', 'blocks.16.ln1.hook_scale', 'blocks.16.ln1.hook_normalized', 'blocks.16.ln1_post.hook_scale', 'blocks.16.ln1_post.hook_normalized', 'blocks.16.ln2.hook_scale', 'blocks.16.ln2.hook_normalized', 'blocks.16.ln2_post.hook_scale', 'blocks.16.ln2_post.hook_normalized', 'blocks.16.attn.hook_k', 'blocks.16.attn.hook_q', 'blocks.16.attn.hook_v', 'blocks.16.attn.hook_z', 'blocks.16.attn.hook_attn_scores', 'blocks.16.attn.hook_pattern', 'blocks.16.attn.hook_result', 'blocks.16.attn.hook_rot_k', 'blocks.16.attn.hook_rot_q', 'blocks.16.mlp.hook_pre', 'blocks.16.mlp.hook_pre_linear', 'blocks.16.mlp.hook_post', 'blocks.16.hook_attn_in', 'blocks.16.hook_q_input', 'blocks.16.hook_k_input', 'blocks.16.hook_v_input', 'blocks.16.hook_mlp_in', 'blocks.16.hook_attn_out', 'blocks.16.hook_mlp_out', 'blocks.16.hook_resid_pre', 'blocks.16.hook_resid_mid', 'blocks.16.hook_resid_post', 'blocks.17.ln1.hook_scale', 'blocks.17.ln1.hook_normalized', 'blocks.17.ln1_post.hook_scale', 'blocks.17.ln1_post.hook_normalized', 'blocks.17.ln2.hook_scale', 'blocks.17.ln2.hook_normalized', 'blocks.17.ln2_post.hook_scale', 'blocks.17.ln2_post.hook_normalized', 'blocks.17.attn.hook_k', 'blocks.17.attn.hook_q', 'blocks.17.attn.hook_v', 'blocks.17.attn.hook_z', 'blocks.17.attn.hook_attn_scores', 'blocks.17.attn.hook_pattern', 'blocks.17.attn.hook_result', 'blocks.17.attn.hook_rot_k', 'blocks.17.attn.hook_rot_q', 'blocks.17.mlp.hook_pre', 'blocks.17.mlp.hook_pre_linear', 'blocks.17.mlp.hook_post', 'blocks.17.hook_attn_in', 'blocks.17.hook_q_input', 'blocks.17.hook_k_input', 'blocks.17.hook_v_input', 'blocks.17.hook_mlp_in', 'blocks.17.hook_attn_out', 'blocks.17.hook_mlp_out', 'blocks.17.hook_resid_pre', 'blocks.17.hook_resid_mid', 'blocks.17.hook_resid_post', 'blocks.18.ln1.hook_scale', 'blocks.18.ln1.hook_normalized', 'blocks.18.ln1_post.hook_scale', 'blocks.18.ln1_post.hook_normalized', 'blocks.18.ln2.hook_scale', 'blocks.18.ln2.hook_normalized', 'blocks.18.ln2_post.hook_scale', 'blocks.18.ln2_post.hook_normalized', 'blocks.18.attn.hook_k', 'blocks.18.attn.hook_q', 'blocks.18.attn.hook_v', 'blocks.18.attn.hook_z', 'blocks.18.attn.hook_attn_scores', 'blocks.18.attn.hook_pattern', 'blocks.18.attn.hook_result', 'blocks.18.attn.hook_rot_k', 'blocks.18.attn.hook_rot_q', 'blocks.18.mlp.hook_pre', 'blocks.18.mlp.hook_pre_linear', 'blocks.18.mlp.hook_post', 'blocks.18.hook_attn_in', 'blocks.18.hook_q_input', 'blocks.18.hook_k_input', 'blocks.18.hook_v_input', 'blocks.18.hook_mlp_in', 'blocks.18.hook_attn_out', 'blocks.18.hook_mlp_out', 'blocks.18.hook_resid_pre', 'blocks.18.hook_resid_mid', 'blocks.18.hook_resid_post', 'blocks.19.ln1.hook_scale', 'blocks.19.ln1.hook_normalized', 'blocks.19.ln1_post.hook_scale', 'blocks.19.ln1_post.hook_normalized', 'blocks.19.ln2.hook_scale', 'blocks.19.ln2.hook_normalized', 'blocks.19.ln2_post.hook_scale', 'blocks.19.ln2_post.hook_normalized', 'blocks.19.attn.hook_k', 'blocks.19.attn.hook_q', 'blocks.19.attn.hook_v', 'blocks.19.attn.hook_z', 'blocks.19.attn.hook_attn_scores', 'blocks.19.attn.hook_pattern', 'blocks.19.attn.hook_result', 'blocks.19.attn.hook_rot_k', 'blocks.19.attn.hook_rot_q', 'blocks.19.mlp.hook_pre', 'blocks.19.mlp.hook_pre_linear', 'blocks.19.mlp.hook_post', 'blocks.19.hook_attn_in', 'blocks.19.hook_q_input', 'blocks.19.hook_k_input', 'blocks.19.hook_v_input', 'blocks.19.hook_mlp_in', 'blocks.19.hook_attn_out', 'blocks.19.hook_mlp_out', 'blocks.19.hook_resid_pre', 'blocks.19.hook_resid_mid', 'blocks.19.hook_resid_post', 'blocks.20.ln1.hook_scale', 'blocks.20.ln1.hook_normalized', 'blocks.20.ln1_post.hook_scale', 'blocks.20.ln1_post.hook_normalized', 'blocks.20.ln2.hook_scale', 'blocks.20.ln2.hook_normalized', 'blocks.20.ln2_post.hook_scale', 'blocks.20.ln2_post.hook_normalized', 'blocks.20.attn.hook_k', 'blocks.20.attn.hook_q', 'blocks.20.attn.hook_v', 'blocks.20.attn.hook_z', 'blocks.20.attn.hook_attn_scores', 'blocks.20.attn.hook_pattern', 'blocks.20.attn.hook_result', 'blocks.20.attn.hook_rot_k', 'blocks.20.attn.hook_rot_q', 'blocks.20.mlp.hook_pre', 'blocks.20.mlp.hook_pre_linear', 'blocks.20.mlp.hook_post', 'blocks.20.hook_attn_in', 'blocks.20.hook_q_input', 'blocks.20.hook_k_input', 'blocks.20.hook_v_input', 'blocks.20.hook_mlp_in', 'blocks.20.hook_attn_out', 'blocks.20.hook_mlp_out', 'blocks.20.hook_resid_pre', 'blocks.20.hook_resid_mid', 'blocks.20.hook_resid_post', 'blocks.21.ln1.hook_scale', 'blocks.21.ln1.hook_normalized', 'blocks.21.ln1_post.hook_scale', 'blocks.21.ln1_post.hook_normalized', 'blocks.21.ln2.hook_scale', 'blocks.21.ln2.hook_normalized', 'blocks.21.ln2_post.hook_scale', 'blocks.21.ln2_post.hook_normalized', 'blocks.21.attn.hook_k', 'blocks.21.attn.hook_q', 'blocks.21.attn.hook_v', 'blocks.21.attn.hook_z', 'blocks.21.attn.hook_attn_scores', 'blocks.21.attn.hook_pattern', 'blocks.21.attn.hook_result', 'blocks.21.attn.hook_rot_k', 'blocks.21.attn.hook_rot_q', 'blocks.21.mlp.hook_pre', 'blocks.21.mlp.hook_pre_linear', 'blocks.21.mlp.hook_post', 'blocks.21.hook_attn_in', 'blocks.21.hook_q_input', 'blocks.21.hook_k_input', 'blocks.21.hook_v_input', 'blocks.21.hook_mlp_in', 'blocks.21.hook_attn_out', 'blocks.21.hook_mlp_out', 'blocks.21.hook_resid_pre', 'blocks.21.hook_resid_mid', 'blocks.21.hook_resid_post', 'blocks.22.ln1.hook_scale', 'blocks.22.ln1.hook_normalized', 'blocks.22.ln1_post.hook_scale', 'blocks.22.ln1_post.hook_normalized', 'blocks.22.ln2.hook_scale', 'blocks.22.ln2.hook_normalized', 'blocks.22.ln2_post.hook_scale', 'blocks.22.ln2_post.hook_normalized', 'blocks.22.attn.hook_k', 'blocks.22.attn.hook_q', 'blocks.22.attn.hook_v', 'blocks.22.attn.hook_z', 'blocks.22.attn.hook_attn_scores', 'blocks.22.attn.hook_pattern', 'blocks.22.attn.hook_result', 'blocks.22.attn.hook_rot_k', 'blocks.22.attn.hook_rot_q', 'blocks.22.mlp.hook_pre', 'blocks.22.mlp.hook_pre_linear', 'blocks.22.mlp.hook_post', 'blocks.22.hook_attn_in', 'blocks.22.hook_q_input', 'blocks.22.hook_k_input', 'blocks.22.hook_v_input', 'blocks.22.hook_mlp_in', 'blocks.22.hook_attn_out', 'blocks.22.hook_mlp_out', 'blocks.22.hook_resid_pre', 'blocks.22.hook_resid_mid', 'blocks.22.hook_resid_post', 'blocks.23.ln1.hook_scale', 'blocks.23.ln1.hook_normalized', 'blocks.23.ln1_post.hook_scale', 'blocks.23.ln1_post.hook_normalized', 'blocks.23.ln2.hook_scale', 'blocks.23.ln2.hook_normalized', 'blocks.23.ln2_post.hook_scale', 'blocks.23.ln2_post.hook_normalized', 'blocks.23.attn.hook_k', 'blocks.23.attn.hook_q', 'blocks.23.attn.hook_v', 'blocks.23.attn.hook_z', 'blocks.23.attn.hook_attn_scores', 'blocks.23.attn.hook_pattern', 'blocks.23.attn.hook_result', 'blocks.23.attn.hook_rot_k', 'blocks.23.attn.hook_rot_q', 'blocks.23.mlp.hook_pre', 'blocks.23.mlp.hook_pre_linear', 'blocks.23.mlp.hook_post', 'blocks.23.hook_attn_in', 'blocks.23.hook_q_input', 'blocks.23.hook_k_input', 'blocks.23.hook_v_input', 'blocks.23.hook_mlp_in', 'blocks.23.hook_attn_out', 'blocks.23.hook_mlp_out', 'blocks.23.hook_resid_pre', 'blocks.23.hook_resid_mid', 'blocks.23.hook_resid_post', 'blocks.24.ln1.hook_scale', 'blocks.24.ln1.hook_normalized', 'blocks.24.ln1_post.hook_scale', 'blocks.24.ln1_post.hook_normalized', 'blocks.24.ln2.hook_scale', 'blocks.24.ln2.hook_normalized', 'blocks.24.ln2_post.hook_scale', 'blocks.24.ln2_post.hook_normalized', 'blocks.24.attn.hook_k', 'blocks.24.attn.hook_q', 'blocks.24.attn.hook_v', 'blocks.24.attn.hook_z', 'blocks.24.attn.hook_attn_scores', 'blocks.24.attn.hook_pattern', 'blocks.24.attn.hook_result', 'blocks.24.attn.hook_rot_k', 'blocks.24.attn.hook_rot_q', 'blocks.24.mlp.hook_pre', 'blocks.24.mlp.hook_pre_linear', 'blocks.24.mlp.hook_post', 'blocks.24.hook_attn_in', 'blocks.24.hook_q_input', 'blocks.24.hook_k_input', 'blocks.24.hook_v_input', 'blocks.24.hook_mlp_in', 'blocks.24.hook_attn_out', 'blocks.24.hook_mlp_out', 'blocks.24.hook_resid_pre', 'blocks.24.hook_resid_mid', 'blocks.24.hook_resid_post', 'blocks.25.ln1.hook_scale', 'blocks.25.ln1.hook_normalized', 'blocks.25.ln1_post.hook_scale', 'blocks.25.ln1_post.hook_normalized', 'blocks.25.ln2.hook_scale', 'blocks.25.ln2.hook_normalized', 'blocks.25.ln2_post.hook_scale', 'blocks.25.ln2_post.hook_normalized', 'blocks.25.attn.hook_k', 'blocks.25.attn.hook_q', 'blocks.25.attn.hook_v', 'blocks.25.attn.hook_z', 'blocks.25.attn.hook_attn_scores', 'blocks.25.attn.hook_pattern', 'blocks.25.attn.hook_result', 'blocks.25.attn.hook_rot_k', 'blocks.25.attn.hook_rot_q', 'blocks.25.mlp.hook_pre', 'blocks.25.mlp.hook_pre_linear', 'blocks.25.mlp.hook_post', 'blocks.25.hook_attn_in', 'blocks.25.hook_q_input', 'blocks.25.hook_k_input', 'blocks.25.hook_v_input', 'blocks.25.hook_mlp_in', 'blocks.25.hook_attn_out', 'blocks.25.hook_mlp_out', 'blocks.25.hook_resid_pre', 'blocks.25.hook_resid_mid', 'blocks.25.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']\n",
      "2304\n",
      "2304\n"
     ]
    }
   ],
   "source": [
    "print(list(model.hook_dict.keys()))\n",
    "# sae.cfg.hook_name in model.hook_dict, f\"Model does not expose {sae.cfg.hook_name}\"\n",
    "print(sae.cfg.d_in)\n",
    "print(model.cfg.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8481d5ee-4c0a-4d93-b5be-fb32ee433b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Solve: 2x + 3 = 7\"\n",
    "top_mean_ids, top_mean_vals, token_feature_ids, logits = analyze_prompt_with_sae(\n",
    "    prompt, model, sae, topk_mean=10, topk_token=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8908634-04d9-43bb-9574-0ec52b1cdc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15745, 2628, 10077, 8629, 9147, 3088, 10698, 15256, 3851, 11836], [9038, 3088, 10698, 13955, 86, 15256, 1024, 6214, 6305, 4965], [13955, 16069, 15745, 2379, 3088, 8978, 1024, 1707, 4157, 14654], [13955, 9063, 15251, 3088, 1024, 6702, 5199, 11215, 9038, 11401], [6117, 13955, 15022, 5086, 3088, 12810, 1212, 401, 9038, 13197], [15745, 15022, 11029, 4885, 15448, 13955, 12573, 13639, 8372, 14200], [15745, 8570, 2993, 13955, 1024, 3088, 11355, 13197, 463, 7266], [13955, 9063, 15251, 3088, 1024, 6702, 5199, 11215, 1809, 3882], [4549, 13955, 15022, 3088, 5086, 463, 1212, 5125, 16336, 401], [15745, 16345, 6010, 5356, 1024, 13955, 3088, 6196, 6557, 5087], [13955, 9063, 15251, 6702, 3088, 1024, 5199, 11215, 3882, 1809], [13955, 4499, 15022, 5086, 3088, 2665, 512, 13800, 2207, 8715]]\n",
      "[[132.36, 56.69, 44.72, 34.79, 13.81, 11.45, 11.34, 9.25, 8.89, 8.1], [21.28, 8.08, 7.15, 6.58, 6.5, 6.15, 5.98, 4.32, 4.03, 4.02], [22.62, 19.51, 13.01, 12.57, 8.16, 8.04, 4.34, 4.06, 3.81, 3.55], [34.45, 21.03, 11.18, 5.92, 5.48, 5.21, 3.67, 3.14, 1.08, 1.05], [18.85, 17.24, 14.95, 9.53, 8.95, 4.87, 4.65, 3.84, 2.18, 2.16], [18.04, 12.8, 11.59, 11.02, 9.47, 9.01, 8.85, 7.51, 7.06, 6.88], [18.11, 16.05, 14.5, 10.1, 9.61, 8.06, 4.84, 4.03, 3.45, 3.01], [35.13, 21.62, 8.67, 5.36, 5.1, 4.99, 3.2, 2.81, 2.1, 1.63], [18.51, 15.46, 12.41, 6.75, 6.4, 3.58, 3.54, 2.77, 2.69, 2.34], [21.99, 12.35, 12.05, 11.22, 9.15, 8.74, 7.63, 6.76, 6.12, 6.0], [35.64, 21.85, 7.38, 5.68, 5.06, 4.76, 2.73, 2.02, 1.53, 1.36], [11.62, 11.39, 9.78, 8.59, 7.15, 6.46, 3.77, 3.75, 3.18, 2.94]]\n",
      "[15745, 2628, 10077, 9038, 3088, 10698, 13955, 16069, 15745, 13955, 9063, 15251, 6117, 13955, 15022, 15745, 15022, 11029, 15745, 8570, 2993, 13955, 9063, 15251, 4549, 13955, 15022, 15745, 16345, 6010, 13955, 9063, 15251, 13955, 4499, 15022]\n",
      "tensor([[[-24.3121,  -8.7513,  -6.9737,  ..., -18.3960, -17.4268, -24.3171],\n",
      "         [-16.4601,   3.9197,  -6.3818,  ...,  -8.5217,  -6.8182, -16.3771],\n",
      "         [-13.0133,  14.3032,  -2.9017,  ...,  -6.8615,   1.2660, -12.9732],\n",
      "         ...,\n",
      "         [-14.8424,  12.5597,  -2.0674,  ...,  -6.9634,  -1.6041, -14.7302],\n",
      "         [-14.6758,   8.3443,  -9.3954,  ...,  -5.1611,   0.6757, -14.5796],\n",
      "         [-16.6331,  17.2098,  -7.7457,  ...,  -1.7751,   1.4777, -16.5316]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(top_mean_ids)\n",
    "print(top_mean_vals) \n",
    "print(token_feature_ids)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ae0495b-3ff1-450e-adf2-2046b5c770c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446f2afd4a614c99a0504c322b153e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>What are the symptoms of burnout? More than a third of Americans say their job stress has become an ongoing problem, according to a 2016 survey by the American Psychological Association.\\n\\nSymptoms of burnout include feeling a diminished sense of achievement and a loss of connection to your professional']\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What are the symptoms of burnout?\"\n",
    "tokens = model.to_tokens(prompt, prepend_bos=True)\n",
    "\n",
    "# Generate more tokens\n",
    "generated_token_ids = model.generate(prompt, max_new_tokens=50, return_type=\"tokens\")\n",
    "\n",
    "# Decode the result\n",
    "decoded_output = model.to_string(generated_token_ids)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e7bbec-da5c-44a5-a7dc-25136b9930aa",
   "metadata": {},
   "source": [
    "# SAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e661dc0e-9682-4676-a81b-ce07d84f14ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gemma-scope-2b-pt-att-canonical']\n"
     ]
    }
   ],
   "source": [
    "gemma2b_canonical = [\n",
    "                     \"gemma-scope-2b-pt-res-canonical\",\n",
    "                    \"gemma-scope-2b-pt-att-canonical\",\n",
    "                     \"gemma-scope-2b-pt-mlp-canonical\"\n",
    "                    ]\n",
    "print(gemma2b_canonical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7239ce1-64ac-497f-9e61-c82914cf1606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['layer_0/width_16k/canonical', 'layer_1/width_16k/canonical', 'layer_2/width_16k/canonical', 'layer_3/width_16k/canonical', 'layer_4/width_16k/canonical', 'layer_5/width_16k/canonical', 'layer_6/width_16k/canonical', 'layer_7/width_16k/canonical', 'layer_8/width_16k/canonical', 'layer_9/width_16k/canonical', 'layer_10/width_16k/canonical', 'layer_11/width_16k/canonical', 'layer_12/width_16k/canonical', 'layer_13/width_16k/canonical', 'layer_14/width_16k/canonical', 'layer_15/width_16k/canonical', 'layer_16/width_16k/canonical', 'layer_17/width_16k/canonical', 'layer_18/width_16k/canonical', 'layer_19/width_16k/canonical', 'layer_20/width_16k/canonical', 'layer_21/width_16k/canonical', 'layer_22/width_16k/canonical', 'layer_23/width_16k/canonical', 'layer_24/width_16k/canonical', 'layer_25/width_16k/canonical']\n"
     ]
    }
   ],
   "source": [
    "sae_layers = [\n",
    "    f\"layer_{i}/width_16k/canonical\"\n",
    "    for i in range(0, 26)\n",
    "]\n",
    "print(sae_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "410e35be-83e6-4148-b7f3-6c217eba2749",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_NAMES = [\"emotion\", \"math\", \"mmlu\", \"programming\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7007cc6f-3a52-4548-b231-e96b68fb0b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_process_datasets(df, MODEL_NAME, LAYER, DATASET_PATH, model, SAE):\n",
    "    hook_point = SAE.cfg.hook_name\n",
    "\n",
    "    all_top_mean_ids = []\n",
    "    all_top_mean_vals = []\n",
    "    all_token_feature_ids = []\n",
    "    for prompt in tqdm(df[\"prompt\"]):\n",
    "        try:\n",
    "            top_mean_ids, top_mean_vals, token_feature_ids, logits = analyze_prompt_with_sae(\n",
    "                prompt=prompt,\n",
    "                model=model,\n",
    "                sae=SAE,\n",
    "                hook_point=hook_point,\n",
    "                topk_mean=10,\n",
    "                topk_token=3,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # print(f\"Error on prompt: {prompt[:50]}... -> {e}\")\n",
    "            top_mean_ids, top_mean_vals, token_feature_ids = None, None, None\n",
    "    \n",
    "        all_top_mean_ids.append(top_mean_ids)\n",
    "        all_top_mean_vals.append(top_mean_vals)\n",
    "        all_token_feature_ids.append(token_feature_ids)\n",
    "\n",
    "        \n",
    "    # Save to DataFrame\n",
    "    COL_NAME = f\"{MODEL_NAME}-{LAYER}-\"\n",
    "    df[COL_NAME +\"top_mean_ids\"] = all_top_mean_ids\n",
    "    df[COL_NAME +\"top_mean_vals\"] = all_top_mean_vals\n",
    "    df[COL_NAME +\"token_feature_ids\"] = all_token_feature_ids\n",
    "    \n",
    "    # Optional: save to CSV\n",
    "    df.to_csv(DATASET_PATH, index=False, encoding=\"utf-8-sig\", quoting=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6b70e3a-72e2-4735-8727-73675e7af96e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model: gemma-scope-2b-pt-att-canonical, layer: layer_0/width_16k/canonical\n",
      "  Processing dataset: emotion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4992/4992 [14:41<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing dataset: math\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 4610/4998 [14:04<01:11,  5.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m df = load_dataset(DATASET_PATH)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Processing dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mpipeline_process_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLAYER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATASET_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mpipeline_process_datasets\u001b[39m\u001b[34m(df, MODEL_NAME, LAYER, DATASET_PATH, model, SAE)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m tqdm(df[\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         top_mean_ids, top_mean_vals, token_feature_ids, logits = \u001b[43manalyze_prompt_with_sae\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m            \u001b[49m\u001b[43msae\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSAE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhook_point\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhook_point\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtopk_mean\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtopk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     18\u001b[39m         \u001b[38;5;66;03m# print(f\"Error on prompt: {prompt[:50]}... -> {e}\")\u001b[39;00m\n\u001b[32m     19\u001b[39m         top_mean_ids, top_mean_vals, token_feature_ids = \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36manalyze_prompt_with_sae\u001b[39m\u001b[34m(prompt, model, sae, hook_point, topk_mean, topk_token)\u001b[39m\n\u001b[32m      7\u001b[39m     hook_point = sae.cfg.hook_name\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Run model with cache\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m logits, cache = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Encode activations from residuals using SAE\u001b[39;00m\n\u001b[32m     13\u001b[39m residual_activations = cache[hook_point]  \u001b[38;5;66;03m# shape: [1, seq_len, d_model]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cephfs/volumes/hpc_data_prj/inf_narrative_msc/e8efa787-4d41-448d-a7aa-814b8f0fac1e/k24086575/jvenv/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:694\u001b[39m, in \u001b[36mHookedTransformer.run_with_cache\u001b[39m\u001b[34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[39m\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_with_cache\u001b[39m(\n\u001b[32m    678\u001b[39m     \u001b[38;5;28mself\u001b[39m, *model_args, return_cache_object=\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim=\u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs\n\u001b[32m    679\u001b[39m ) -> Tuple[\n\u001b[32m   (...)\u001b[39m\u001b[32m    686\u001b[39m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor]],\n\u001b[32m    687\u001b[39m ]:\n\u001b[32m    688\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[32m    689\u001b[39m \n\u001b[32m    690\u001b[39m \u001b[33;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[32m    691\u001b[39m \u001b[33;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[32m    692\u001b[39m \u001b[33;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[32m    693\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m694\u001b[39m     out, cache_dict = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    697\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[32m    698\u001b[39m         cache = ActivationCache(cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim=\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cephfs/volumes/hpc_data_prj/inf_narrative_msc/e8efa787-4d41-448d-a7aa-814b8f0fac1e/k24086575/jvenv/lib/python3.11/site-packages/transformer_lens/hook_points.py:569\u001b[39m, in \u001b[36mHookedRootModule.run_with_cache\u001b[39m\u001b[34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, pos_slice, *model_args, **model_kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m cache_dict, fwd, bwd = \u001b[38;5;28mself\u001b[39m.get_caching_hooks(\n\u001b[32m    556\u001b[39m     names_filter,\n\u001b[32m    557\u001b[39m     incl_bwd,\n\u001b[32m   (...)\u001b[39m\u001b[32m    560\u001b[39m     pos_slice=pos_slice,\n\u001b[32m    561\u001b[39m )\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hooks(\n\u001b[32m    564\u001b[39m     fwd_hooks=fwd,\n\u001b[32m    565\u001b[39m     bwd_hooks=bwd,\n\u001b[32m    566\u001b[39m     reset_hooks_end=reset_hooks_end,\n\u001b[32m    567\u001b[39m     clear_contexts=clear_contexts,\n\u001b[32m    568\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m     model_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[32m    571\u001b[39m         model_out.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cephfs/volumes/hpc_data_prj/inf_narrative_msc/e8efa787-4d41-448d-a7aa-814b8f0fac1e/k24086575/jvenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cephfs/volumes/hpc_data_prj/inf_narrative_msc/e8efa787-4d41-448d-a7aa-814b8f0fac1e/k24086575/jvenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cephfs/volumes/hpc_data_prj/inf_narrative_msc/e8efa787-4d41-448d-a7aa-814b8f0fac1e/k24086575/jvenv/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:583\u001b[39m, in \u001b[36mHookedTransformer.forward\u001b[39m\u001b[34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[39m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m utils.LocallyOverridenDefaults(\n\u001b[32m    575\u001b[39m     \u001b[38;5;28mself\u001b[39m, prepend_bos=prepend_bos, padding_side=padding_side\n\u001b[32m    576\u001b[39m ):\n\u001b[32m    577\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m start_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    578\u001b[39m         (\n\u001b[32m    579\u001b[39m             residual,\n\u001b[32m    580\u001b[39m             tokens,\n\u001b[32m    581\u001b[39m             shortformer_pos_embed,\n\u001b[32m    582\u001b[39m             attention_mask,\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m         ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_to_embed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    590\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    591\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m) == torch.Tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cephfs/volumes/hpc_data_prj/inf_narrative_msc/e8efa787-4d41-448d-a7aa-814b8f0fac1e/k24086575/jvenv/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:375\u001b[39m, in \u001b[36mHookedTransformer.input_to_embed\u001b[39m\u001b[34m(self, input, prepend_bos, padding_side, attention_mask, past_kv_cache)\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokens.device.type != \u001b[38;5;28mself\u001b[39m.cfg.device:\n\u001b[32m    373\u001b[39m     tokens = tokens.to(devices.get_device_for_block_index(\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m.cfg))\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    376\u001b[39m     (\u001b[38;5;28mself\u001b[39m.tokenizer \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokenizer.padding_side == \u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    377\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    378\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m past_kv_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    379\u001b[39m ):\n\u001b[32m    380\u001b[39m     \u001b[38;5;66;03m# This means we need to have an explicit attention mask.\u001b[39;00m\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    382\u001b[39m         \u001b[38;5;66;03m# If the padding side is left or we are using caching, we need to compute the attention\u001b[39;00m\n\u001b[32m    383\u001b[39m         \u001b[38;5;66;03m# mask for the adjustment of absolute positional embeddings and attention masking so\u001b[39;00m\n\u001b[32m    384\u001b[39m         \u001b[38;5;66;03m# that pad tokens are not attended.\u001b[39;00m\n\u001b[32m    385\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m prepend_bos \u001b[38;5;129;01mis\u001b[39;00m USE_DEFAULT_VALUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cephfs/volumes/hpc_data_prj/inf_narrative_msc/e8efa787-4d41-448d-a7aa-814b8f0fac1e/k24086575/jvenv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:279\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__len__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m    Size of the full vocabulary with the added tokens.\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_vocab_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwith_added_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Loop through models, layers, and datasets\n",
    "for MODEL_NAME in gemma2b_canonical:\n",
    "    for LAYER in sae_layers:\n",
    "        SAE = SAE.from_pretrained(release=MODEL_NAME, sae_id=LAYER, device=\"cuda\")[0]\n",
    "        print(f\"Processing model: {MODEL_NAME}, layer: {LAYER}\")\n",
    "        for DATASET_NAME in DATASETS_NAMES:\n",
    "            DATASET_PATH = f\"./{DATASET_NAME}_processed.csv\"\n",
    "            df = load_dataset(DATASET_PATH)\n",
    "            print(f\"  Processing dataset: {DATASET_NAME}\")\n",
    "            pipeline_process_datasets(df, MODEL_NAME, LAYER, DATASET_PATH, model, SAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f9ed0d-fad3-464c-8405-65fd4bbe2f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07e8656-76d6-4db8-b2ae-59d86d8a2032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
