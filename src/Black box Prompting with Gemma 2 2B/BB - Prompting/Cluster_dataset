{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPpyYWJJZ5ULyr7tmrCOqHq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1f4d0c2a0e7c4f7fb82ba51ac718ae8a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5ac21ffb93f346d097de8ea1b18020a5","IPY_MODEL_98a0174ba47a4a76a911a0fc4b90c10f","IPY_MODEL_8ace81e7476a484fac2e705420d24ae3"],"layout":"IPY_MODEL_6598accc01b64315ac32dd7da323562c"}},"5ac21ffb93f346d097de8ea1b18020a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79282cd29cf64626a76c801405eae92b","placeholder":"​","style":"IPY_MODEL_cf1c8e3440e74d92838822d3ed006dab","value":"Loading checkpoint shards: 100%"}},"98a0174ba47a4a76a911a0fc4b90c10f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_75bcf4ca1bb9423993555a5156cdc609","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_239c4d39ace442109eb35218adeef37b","value":2}},"8ace81e7476a484fac2e705420d24ae3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a45bda71dac940fcba31e879574dcc92","placeholder":"​","style":"IPY_MODEL_e69b0aa6f1a34857b1e4914bfbecceed","value":" 2/2 [00:25&lt;00:00, 10.64s/it]"}},"6598accc01b64315ac32dd7da323562c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79282cd29cf64626a76c801405eae92b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf1c8e3440e74d92838822d3ed006dab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75bcf4ca1bb9423993555a5156cdc609":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"239c4d39ace442109eb35218adeef37b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a45bda71dac940fcba31e879574dcc92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e69b0aa6f1a34857b1e4914bfbecceed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["from transformers import pipeline\n","import torch\n","\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=\"google/gemma-2-9b-it\",\n","    model_kwargs={\"torch_dtype\": torch.bfloat16},\n","    device=\"cuda\",\n",")\n","\n","messages = [\n","    {\"role\": \"user\", \"content\": \"Who are you? Please, answer in pirate-speak.\"},\n","]\n","outputs = pipe(\n","    messages,\n","    max_new_tokens=256,\n","    do_sample=False,\n",")\n","assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n","print(assistant_response)\n"],"metadata":{"id":"48YLigthYHTL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","torch.cuda.empty_cache()\n","torch.cuda.memory_summary(device=None, abbreviated=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"id":"vwARRhTQVscW","executionInfo":{"status":"ok","timestamp":1743024758504,"user_tz":0,"elapsed":24,"user":{"displayName":"Yagol Xu Chen","userId":"00929278906170525619"}},"outputId":"84d1d26e-9b13-4b79-ce31-99a109da4c4a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  14959 MiB |  14959 MiB |  14959 MiB |   3072 B   |\\n|       from large pool |  14958 MiB |  14958 MiB |  14958 MiB |      0 B   |\\n|       from small pool |      0 MiB |      0 MiB |      0 MiB |   3072 B   |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  14959 MiB |  14959 MiB |  14959 MiB |   3072 B   |\\n|       from large pool |  14958 MiB |  14958 MiB |  14958 MiB |      0 B   |\\n|       from small pool |      0 MiB |      0 MiB |      0 MiB |   3072 B   |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |  14959 MiB |  14959 MiB |  14959 MiB |     10 B   |\\n|       from large pool |  14958 MiB |  14958 MiB |  14958 MiB |      0 B   |\\n|       from small pool |      0 MiB |      0 MiB |      0 MiB |     10 B   |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  14962 MiB |  14962 MiB |  14962 MiB |      0 B   |\\n|       from large pool |  14960 MiB |  14960 MiB |  14960 MiB |      0 B   |\\n|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     507    |     507    |     513    |       6    |\\n|       from large pool |     358    |     358    |     358    |       0    |\\n|       from small pool |     149    |     152    |     155    |       6    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     507    |     507    |     513    |       6    |\\n|       from large pool |     358    |     358    |     358    |       0    |\\n|       from small pool |     149    |     152    |     155    |       6    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4kPi1dKiN74M"},"outputs":[],"source":["from huggingface_hub import login\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","left = \"Here is a list of skills required to solve a mathematical question: \"\n","right = \". Reduce the number of unique skills by grouping similar skills into categories and give a descriptive name to each category.\"\n","\n","class SkillExtractor:\n","    def __init__(self, hf_token, model_name=\"google/gemma-2-2b-it\"):\n","        \"\"\"Initialize the model, tokenizer, and device.\"\"\"\n","        # Login to Hugging Face\n","        login(token=hf_token)\n","\n","        # Set device (GPU if available, else CPU)\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        print(f\"Using device: {self.device}\")\n","        if self.device == \"cuda\":\n","            print(f\"GPU: {torch.cuda.get_device_name()}\")\n","\n","        # Load model and tokenizer\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.model = AutoModelForCausalLM.from_pretrained(\n","            model_name,\n","            # device_map=\"auto\",\n","            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n","        ).to(self.device)\n","\n","    def extract_skills(self, problem):\n","        # template = \"Consider this question. Label this question with a skill that would be required to solve the question. Basically, you should be able to use the skill as a dictionary key in python. The skill name should be lower case letters only. The skill name should be very descriptive and you may use multiple words to describe the skills required in the question. If you do use multiple words per question, then you must join them by an underscore. Your answer should be as follows: <name of the skill>, reason: <reason for the skill>.\"\n","        prompt = left + problem + right\n","        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n","\n","        # Limit the number of new tokens generated\n","        max_new_tokens = 1000\n","\n","        with torch.no_grad():\n","            outputs = self.model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n","\n","        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","        # Extract skills from response\n","        return response.strip()"]},{"cell_type":"code","source":["import os\n","\n","# gemma\n","hf_token = \"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\"\n","extractor = SkillExtractor(hf_token)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":440,"referenced_widgets":["1f4d0c2a0e7c4f7fb82ba51ac718ae8a","5ac21ffb93f346d097de8ea1b18020a5","98a0174ba47a4a76a911a0fc4b90c10f","8ace81e7476a484fac2e705420d24ae3","6598accc01b64315ac32dd7da323562c","79282cd29cf64626a76c801405eae92b","cf1c8e3440e74d92838822d3ed006dab","75bcf4ca1bb9423993555a5156cdc609","239c4d39ace442109eb35218adeef37b","a45bda71dac940fcba31e879574dcc92","e69b0aa6f1a34857b1e4914bfbecceed"]},"id":"P7TyoofAOatd","executionInfo":{"status":"error","timestamp":1743024785458,"user_tz":0,"elapsed":26899,"user":{"displayName":"Yagol Xu Chen","userId":"00929278906170525619"}},"outputId":"dc6c3366-8bdc-486c-a7e7-4dc159963d6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","GPU: Tesla T4\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f4d0c2a0e7c4f7fb82ba51ac718ae8a"}},"metadata":{}},{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1.10 GiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 10134 has 14.72 GiB memory in use. Of the allocated memory 14.61 GiB is allocated by PyTorch, and 2.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-4f770b9d0b9d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# gemma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhf_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"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\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mextractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSkillExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-0393e53d58ed>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hf_token, model_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# device_map=\"auto\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         ).to(self.device)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_skills\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3710\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3711\u001b[0m                 )\n\u001b[0;32m-> 3712\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.10 GiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 10134 has 14.72 GiB memory in use. Of the allocated memory 14.61 GiB is allocated by PyTorch, and 2.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}]},{"cell_type":"code","source":["# Load model directly\n","tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n","model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\n","# Move the model to the GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"metadata":{"id":"XS-8_NrwObM8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# Define paths\n","input_directory = \"/content/\"\n","output_directory = \"/content/clustered_output/\"\n","\n","# Ensure output directory exists\n","os.makedirs(output_directory, exist_ok=True)\n","\n","# Placeholder function to process a problem using Gemma\n","def process_with_gemma(problem):\n","    # Replace this with actual processing logic\n","    skills = extractor.extract_skills(problem.strip())\n","    return skills\n","\n","# Iterate over all .txt files in /content/\n","for filename in os.listdir(input_directory):\n","    if filename.endswith(\".txt\"):  # Process only .txt files\n","        input_path = os.path.join(input_directory, filename)\n","        output_path = os.path.join(output_directory, f\"{filename}_clustered.txt\")\n","\n","        with open(input_path, \"r\", encoding=\"utf-8\") as file:\n","            content = file.read().strip()\n","\n","        # Process each problem\n","        problems = content.split(\"========================================\")  # Split problems if applicable\n","        clustered_skills = []\n","\n","        for problem in problems:\n","            problem = problem.strip()\n","            if problem:\n","                skills = process_with_gemma(problem)\n","                clustered_skills.append(skills)\n","                print(skills)\n","\n","        # Save the clustered skills to a new file\n","        with open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n","            outfile.write(\"\\n\".join(clustered_skills))\n","\n","        print(f\"Processed and saved: {output_path}\")\n"],"metadata":{"id":"Z2c-oaF-PcYl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8SWSFlRORR40"},"execution_count":null,"outputs":[]}]}