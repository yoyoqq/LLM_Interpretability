{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1PlFzI_PWGTN9yCQLuBcSuPJUjgHL7GiD","timestamp":1742669001232}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Introduction\n","\n","This notebook provides reference implementations for [JumpReLU SAEs](https://arxiv.org/abs/2407.14435) in JAX and PyTorch, expanding on the pseudo-code provided in the paper. Specifically, we include:\n","\n","* Implementations of the `jumprelu` and `step` functions with custom backward passes;\n","* Implementations of the SAE forward pass and L0-based loss function;\n","* Training loop implementations that optionally normalise the norms of the decoder matrix.\n","\n","We don't implement some features used in the training setup described in the paper (e.g. learning rate and sparsity coefficient $\\lambda$ warm-up) which are reasonably easy to add on if desired.\n","\n","The notebook also provides comprehensive tests to check that the Jax and PyTorch implementations are consistent. This includes an end-to-end training test where we train SAEs (on synthetic data, using identical initialisations) using both the Jax and PyTorch implementations and check we get the similar parameters after three steps. You may find these tests useful for testing other implementations of JumpReLU SAEs against these reference implementations to confirm consistency.\n","\n","You should be able to run this notebook on a CPU runtime."],"metadata":{"id":"f1rvyLNB_Z4r"}},{"cell_type":"markdown","source":["## Setup\n","\n","This section imports modules needed by the rest of the notebook, sets some constants (mainly hyperparameters) and generates some synthetic data and initialises some SAE parameters that we use later for testing."],"metadata":{"id":"PejpwsV3B_k6"}},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"PhoS7Jik9FkJ"},"outputs":[],"source":["# @title Imports\n","import dataclasses\n","import functools\n","import itertools\n","\n","import chex\n","import numpy as np\n","import jax\n","import jax.numpy as jnp\n","import optax\n","import plotly.express as px\n","import torch\n","from torch import nn\n","\n","jax.config.update(\"jax_enable_x64\", True)"]},{"cell_type":"code","source":["# @title Hyperparameters and constants\n","\n","NUM_STEPS = 3\n","BATCH_SIZE = 1024\n","ACTIVATIONS_SIZE = 16\n","SAE_WIDTH = 128\n","THRESHOLD_INIT = 0.001\n","# We use a higher bandwidth than in the paper to ensure a non-zero gradient\n","# to the threshold at every step (since we'll only be taking three steps)\n","BANDWIDTH = 0.1\n","FIX_DECODER_NORMS = True\n","LEARNING_RATE = 0.001  # Note this is not the learning rate in the paper\n","ADAM_B1 = 0.0\n","DATA_SEED = 9328302\n","PARAMS_SEED = 24396"],"metadata":{"cellView":"form","id":"B-2paQs89ag6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Create some synthetic data for testing\n","\n","rng = np.random.default_rng(DATA_SEED)\n","dataset = rng.normal(\n","    size=(NUM_STEPS, BATCH_SIZE, ACTIVATIONS_SIZE)\n",") / np.sqrt(ACTIVATIONS_SIZE)"],"metadata":{"cellView":"form","id":"6UsubY3D9OpO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Choose random SAE weights for testing\n","\n","# We choose an initialization that is useful for testing. Specifically\n","# this means we initialize the biases and threshold to non-zero values\n","# and that we don't set the encoder weights to the transpose of the decoder\n","# (since they won't in general during training).\n","rng = np.random.default_rng(PARAMS_SEED)\n","W_dec = (rng.uniform(size=(SAE_WIDTH, ACTIVATIONS_SIZE)) - 0.5)\n","W_dec /= np.linalg.norm(W_dec, axis=-1, keepdims=True)\n","W_enc = (rng.uniform(size=(ACTIVATIONS_SIZE, SAE_WIDTH)) - 0.5)\n","b_enc = (rng.uniform(size=(SAE_WIDTH,)) - 0.5) * 0.1\n","b_dec = (rng.uniform(size=(ACTIVATIONS_SIZE,)) - 0.5) * 0.1\n","threshold = 0.15 * (rng.uniform(size=(SAE_WIDTH,))) * 0.1"],"metadata":{"cellView":"form","id":"q0zFe2dD-CVv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## JAX implementation\n","\n","Although a reference implementation, this JAX implementation can easily be made performant (even for wide SAEs on multiple devices) by simply applying Megatron sharding to the parameters before training (and sharding batches along the data axis). As explained in the [Gemma Scope](https://storage.googleapis.com/gemma-scope/gemma-scope-report.pdf) report, the main challenge with training SAEs efficiently is designing a dataloader that has sufficient throughput to not be the bottleneck (which is out of the scope of this notebook)."],"metadata":{"id":"zNklMMmyCGTb"}},{"cell_type":"code","source":["# @title Defining the `Params` dataclass\n","\n","@chex.dataclass\n","class Params:\n","    W_enc: jax.Array\n","    b_enc: jax.Array\n","    W_dec: jax.Array\n","    b_dec: jax.Array\n","    log_threshold: jax.Array\n","\n","params_init = Params(\n","    W_enc=W_enc,\n","    b_enc=b_enc,\n","    W_dec=W_dec,\n","    b_dec=b_dec,\n","    log_threshold=np.log(threshold),\n",")"],"metadata":{"cellView":"form","id":"anuwa2rkBLQp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title STEs, forward pass and loss function (taken from the paper)\n","\n","def rectangle(x):\n","  return ((x > -0.5) & (x < 0.5)).astype(x.dtype)\n","\n","\n","### Implementation of step function with custom backward\n","\n","@jax.custom_vjp\n","def step(x, threshold):\n","  return (x > threshold).astype(x.dtype)\n","\n","\n","def step_fwd(x, threshold):\n","  out = step(x, threshold)\n","  cache = x, threshold  # Saved for use in the backward pass\n","  return out, cache\n","\n","\n","def step_bwd(cache, output_grad):\n","  x, threshold = cache\n","  x_grad = 0.0 * output_grad  # We don't apply STE to x input\n","  threshold_grad = jnp.sum(\n","      -(1.0 / BANDWIDTH) * rectangle((x - threshold) / BANDWIDTH) * output_grad,\n","      axis=0,\n","  )\n","  return x_grad, threshold_grad\n","\n","\n","step.defvjp(step_fwd, step_bwd)\n","\n","\n","### Implementation of JumpReLU with custom backward for threshold\n","\n","@jax.custom_vjp\n","def jumprelu(x, threshold):\n","  return x * (x > threshold)\n","\n","\n","def jumprelu_fwd(x, threshold):\n","  out = jumprelu(x, threshold)\n","  cache = x, threshold  # Saved for use in the backward pass\n","  return out, cache\n","\n","\n","def jumprelu_bwd(cache, output_grad):\n","  x, threshold = cache\n","  x_grad = (x > threshold) * output_grad  # We don't apply STE to x input\n","  threshold_grad = jnp.sum(\n","      -(threshold / BANDWIDTH)\n","      * rectangle((x - threshold) / BANDWIDTH)\n","      * output_grad,\n","      axis=0,\n","  )\n","  return x_grad, threshold_grad\n","\n","\n","jumprelu.defvjp(jumprelu_fwd, jumprelu_bwd)\n","\n","\n","### Implementation of JumpReLU SAE forward pass and loss functions\n","\n","def sae(params, x, use_pre_enc_bias):\n","  # Optionally, apply pre-encoder bias\n","  if use_pre_enc_bias:\n","    x = x - params.b_dec\n","\n","  pre_activations = x @ params.W_enc + params.b_enc\n","  threshold = jnp.exp(params.log_threshold)\n","  feature_magnitudes = jumprelu(pre_activations, threshold)\n","\n","  # Decoder\n","  x_reconstructed = feature_magnitudes @ params.W_dec + params.b_dec\n","\n","  # Also return pre_activations, needed to compute sparsity loss\n","  return x_reconstructed, pre_activations\n","\n","\n","### Implementation of JumpReLU loss\n","\n","def loss(params, x, sparsity_coefficient, use_pre_enc_bias):\n","  x_reconstructed, pre_activations = sae(params, x, use_pre_enc_bias)\n","\n","  # Compute per-example reconstruction loss\n","  reconstruction_error = x - x_reconstructed\n","  reconstruction_loss = jnp.sum(reconstruction_error**2, axis=-1)\n","\n","  # Compute per-example sparsity loss\n","  threshold = jnp.exp(params.log_threshold)\n","  l0 = jnp.sum(step(pre_activations, threshold), axis=-1)\n","  sparsity_loss = sparsity_coefficient * l0\n","\n","  # Return the batch-wise mean total loss\n","  return jnp.mean(reconstruction_loss + sparsity_loss, axis=0)"],"metadata":{"cellView":"form","id":"KAD0j7k3fYyI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Training loop\n","\n","optimizer = optax.adam(LEARNING_RATE, b1=ADAM_B1)\n","\n","def remove_parallel_component(x, v):\n","    \"\"\"Returns x with component parallel to v projected away.\"\"\"\n","    v_normalised = v / (jnp.linalg.norm(v, axis=-1, keepdims=True) + 1e-6)\n","    parallel_component = jnp.einsum(\"...d,...d->...\", x, v_normalised)\n","    return x - parallel_component[..., None] * v_normalised\n","\n","@functools.partial(jax.jit, donate_argnums=(0, 1), static_argnums=(4, 5))\n","def update(\n","    params,\n","    opt_state,\n","    x,\n","    sparsity_coefficient,\n","    use_pre_enc_bias,\n","    fix_decoder_norms,\n","):\n","    # This is a standard JAX training loop, with the exception we optionally\n","    # constrain the rows of W_dec to unit norm.\n","    grads = jax.grad(loss)(params, x, sparsity_coefficient, use_pre_enc_bias)\n","    if fix_decoder_norms:\n","        grads.W_dec = remove_parallel_component(grads.W_dec, params.W_dec)\n","    updates, opt_state = optimizer.update(grads, opt_state, params)\n","    params = optax.apply_updates(params, updates)\n","    if fix_decoder_norms:\n","        W_dec_norms = jnp.linalg.norm(params.W_dec, axis=-1, keepdims=True)\n","        params.W_dec = params.W_dec / W_dec_norms\n","    return params, opt_state\n","\n","def train_jax(\n","    dataset_iterator,\n","    sparsity_coefficient,\n","    use_pre_enc_bias,\n","    fix_decoder_norms,\n","):\n","    params = params_init\n","    opt_state = optimizer.init(params)\n","    for batch in dataset_iterator:\n","        params, opt_state = update(\n","            params,\n","            opt_state,\n","            batch,\n","            sparsity_coefficient,\n","            use_pre_enc_bias,\n","            fix_decoder_norms,\n","        )\n","    return params"],"metadata":{"cellView":"form","id":"yP828a6uIlSO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Testing the JAX implementation\n","\n","In this section we check various aspects of this JAX implementation against saved outputs from a previous run (to detect any regressions). If the cell below runs without errors, the outputs of the implementation above match a previous golden run."],"metadata":{"id":"tV70dWwjf3H5"}},{"cell_type":"code","source":["# @title Regression tests\n","\n","# First we do a sanity check to make sure the dataset hasn't changed (e.g.\n","# of changing random seeds)\n","np.testing.assert_allclose(\n","    dataset.ravel()[::13001],\n","    np.array([-0.26036463,  0.07950339, -0.10517059,  0.11135695]),\n",")\n","\n","# Then let's check that the SAE's reconstruction and pre-activation outputs\n","# match previously calculated values (on a small sample of entries)\n","sae_initial_reconstruction, sae_initial_pre_activations = sae(\n","    params_init, dataset[0], use_pre_enc_bias=True\n",")\n","np.testing.assert_allclose(\n","    sae_initial_reconstruction.ravel()[::5001],\n","    np.array([-0.34825447, -0.12089294, -0.20722952,  1.4071178]),\n","    atol=1e-5,\n",")\n","np.testing.assert_allclose(\n","    sae_initial_pre_activations.ravel()[::40001],\n","    np.array([ 0.6627157 , -0.2485388 ,  0.52095324, -0.01966003]),\n","    atol=1e-5,\n",")\n","\n","# We now check the loss and its gradients match previously calculated values\n","loss_init, loss_grad = jax.value_and_grad(loss)(\n","    params_init,\n","    dataset[0],\n","    sparsity_coefficient=1.2,\n","    use_pre_enc_bias=True,\n",")\n","np.testing.assert_allclose(loss_init, 81.98928, atol=1e-5)\n","np.testing.assert_allclose(\n","    loss_grad.b_dec[::7],\n","    np.array([-1.7614671 , -2.9526186 ,  0.99276465]),\n","    atol=1e-5,\n",")\n","np.testing.assert_allclose(\n","    loss_grad.b_enc[::37],\n","    np.array([0.29240608, 0.9655949 , 0.20811056, 1.051967]),\n","    atol=1e-5,\n",")\n","np.testing.assert_allclose(\n","    loss_grad.log_threshold[::37],\n","    np.array([-0.01634991, -0.00306892, -0.00966095, -0.0034949]),\n","    atol=1e-5,\n",")\n","np.testing.assert_allclose(\n","    loss_grad.W_dec.ravel()[::601],\n","    np.array([0.02478151, -0.01372844,  0.00353589,  0.10577872]),\n","    atol=1e-5,\n",")\n","np.testing.assert_allclose(\n","    loss_grad.W_enc.ravel()[::601],\n","    np.array([0.02030803,  0.02437474, -0.01442294,  0.01866631]),\n","    atol=1e-5,\n",")\n","\n","# Finally, we train for three steps (using fixed decoder norms) and check that\n","# the resulting params matches a previous run\n","params_jax_trained: Params = train_jax(\n","    iter(dataset),\n","    sparsity_coefficient=1.2,\n","    use_pre_enc_bias=True,\n","    fix_decoder_norms=True,\n",")\n","np.testing.assert_allclose(\n","    params_jax_trained.b_dec[::7],\n","    np.array([0.04524906, -0.03720585, -0.03994906]),\n","    atol=1e-5,\n",")\n","np.testing.assert_allclose(\n","    params_jax_trained.b_enc[::37],\n","    np.array([0.01241532, -0.03432115, -0.03897703,  0.01650386]),\n","    atol=1e-5,\n",")\n","np.testing.assert_allclose(\n","    params_jax_trained.log_threshold[::37],\n","    np.array([-4.5912695, -6.2672825, -5.0748363, -6.345015]),\n","    atol=1e-5,\n",")\n","np.testing.assert_allclose(\n","    params_jax_trained.W_dec.ravel()[::601],\n","    np.array([-0.05179337, -0.08356037,  0.02901143, -0.36985442]),\n","    atol=1e-5,\n",")\n","np.testing.assert_allclose(\n","    params_jax_trained.W_enc.ravel()[::601],\n","    np.array([0.04418736, -0.1515689 ,  0.14993134, -0.12296465]),\n","    atol=1e-5,\n",")"],"metadata":{"cellView":"form","id":"SXLZn776f_2J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PyTorch implementation\n","\n","In this section we translate the JAX implementation defined in the previous section into PyTorch. We'll then check carefully that the PyTorch implementation is consistent with the JAX one, the key test being that training over multiple steps with either implementation (using synthetic data and identical initialisation) yields the same parameters (up to numerical tolerance)."],"metadata":{"id":"sHRICVB_hyaw"}},{"cell_type":"code","source":["# @title STEs, forward pass and loss function\n","\n","def rectangle_pt(x):\n","    return ((x > -0.5) & (x < 0.5)).to(x)\n","\n","\n","class Step(torch.autograd.Function):\n","    @staticmethod\n","    def forward(x, threshold):\n","        return (x > threshold).to(x)\n","\n","    @staticmethod\n","    def setup_context(ctx, inputs, output):\n","        x, threshold = inputs\n","        del output\n","        ctx.save_for_backward(x, threshold)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        x, threshold = ctx.saved_tensors\n","        x_grad = 0.0 * grad_output  # We don't apply STE to x input\n","        threshold_grad = torch.sum(\n","            -(1.0 / BANDWIDTH)\n","            * rectangle_pt((x - threshold) / BANDWIDTH)\n","            * grad_output,\n","            dim=0,\n","        )\n","        return x_grad, threshold_grad\n","\n","\n","class JumpReLU(torch.autograd.Function):\n","    @staticmethod\n","    def forward(x, threshold):\n","        return x * (x > threshold).to(x)\n","\n","    @staticmethod\n","    def setup_context(ctx, inputs, output):\n","        x, threshold = inputs\n","        del output\n","        ctx.save_for_backward(x, threshold)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        x, threshold = ctx.saved_tensors\n","        x_grad = (x > threshold) * grad_output  # We don't apply STE to x input\n","        threshold_grad = torch.sum(\n","            -(threshold / BANDWIDTH)\n","            * rectangle_pt((x - threshold) / BANDWIDTH)\n","            * grad_output,\n","            dim=0,\n","        )\n","        return x_grad, threshold_grad\n","\n","\n","class Sae(nn.Module):\n","    def __init__(self, sae_width, activations_size, use_pre_enc_bias):\n","        super().__init__()\n","        self.use_pre_enc_bias = use_pre_enc_bias\n","        self.W_enc = nn.Parameter(torch.tensor(W_enc))\n","        self.b_enc = nn.Parameter(torch.tensor(b_enc))\n","        self.W_dec = nn.Parameter(torch.tensor(W_dec))\n","        self.b_dec = nn.Parameter(torch.tensor(b_dec))\n","        self.log_threshold = nn.Parameter(\n","            torch.tensor(np.log(threshold))\n","        )\n","\n","    def __call__(self, x):\n","        if self.use_pre_enc_bias:\n","            x = x - self.b_dec\n","\n","        pre_activations = x @ self.W_enc + self.b_enc\n","        threshold = torch.exp(self.log_threshold)\n","        feature_magnitudes = JumpReLU.apply(pre_activations, threshold)\n","        x_reconstructed = feature_magnitudes @ self.W_dec + self.b_dec\n","        return x_reconstructed, pre_activations\n","\n","\n","def loss_fn_pt(sae, x, sparsity_coefficient, use_pre_enc_bias):\n","    x_reconstructed, pre_activations = sae(x)\n","\n","    # Compute per-example reconstruction loss\n","    reconstruction_error = x - x_reconstructed\n","    reconstruction_loss = torch.sum(reconstruction_error**2, dim=-1)\n","\n","    # Compute per-example sparsity loss\n","    threshold = torch.exp(sae.log_threshold)\n","    l0 = torch.sum(Step.apply(pre_activations, threshold), dim=-1)\n","    sparsity_loss = sparsity_coefficient * l0\n","\n","    # Return the batch-wise mean total loss\n","    return torch.mean(reconstruction_loss + sparsity_loss, dim=0)"],"metadata":{"cellView":"form","id":"j6KFi16Pht4a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Training loop\n","\n","def remove_parallel_component_pt(x, v):\n","    \"\"\"Returns x with component parallel to v projected away (in PyTorch).\"\"\"\n","    v_normalised = v / (torch.norm(v, dim=-1, keepdim=True) + 1e-6)\n","    parallel_component = torch.einsum(\"...d,...d->...\", x, v_normalised)\n","    return x - parallel_component[..., None] * v_normalised\n","\n","def train_pt(\n","    dataset_iterator,\n","    sparsity_coefficient,\n","    use_pre_enc_bias,\n","    fix_decoder_norms,\n","):\n","    sae = Sae(SAE_WIDTH, ACTIVATIONS_SIZE, use_pre_enc_bias)\n","    optimizer = torch.optim.Adam(\n","        sae.parameters(), lr=LEARNING_RATE, betas=(ADAM_B1, 0.999)\n","    )\n","    for batch in dataset_iterator:\n","        optimizer.zero_grad()\n","        loss_pt = loss_fn_pt(\n","            sae, torch.tensor(batch), sparsity_coefficient, use_pre_enc_bias\n","        )\n","        loss_pt.backward()\n","        if fix_decoder_norms:\n","            sae.W_dec.grad = remove_parallel_component_pt(\n","                sae.W_dec.grad, sae.W_dec.data\n","            )\n","        optimizer.step()\n","        if fix_decoder_norms:\n","            sae.W_dec.data = sae.W_dec.data / torch.norm(\n","                sae.W_dec.data, dim=-1, keepdim=True\n","            )\n","    return sae"],"metadata":{"cellView":"form","id":"Zz0TwpNCuDOP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Testing consistency with the JAX implementation"],"metadata":{"id":"F7qr9P_BEoh8"}},{"cell_type":"markdown","source":["We start with an end-to-end test: if we train using both implementations on identical (synthetic data) with identical initial parameters, do we get the same parameters at the end of training?"],"metadata":{"id":"ex4N-stfEr6_"}},{"cell_type":"code","source":["# @title End-to-end test\n","\n","# Parameter combinations that we'll loop over\n","sparsity_coefficients = [0.0, 0.01, 0.1]  # Arbitrarily chosen\n","use_pre_enc_bias_l = [True, False]\n","fix_decoder_norms_l = [True, False]\n","\n","\n","for sparsity_coefficient, use_pre_enc_bias, fix_decoder_norms in itertools.product(\n","    sparsity_coefficients, use_pre_enc_bias_l, fix_decoder_norms_l\n","):\n","    print(\n","        f\"Testing {sparsity_coefficient=}, {use_pre_enc_bias=}, \"\n","        f\"{fix_decoder_norms=}... \",\n","        end=\"\",\n","        flush=True,\n","    )\n","\n","    # Train using the JAX implementation\n","    params_jax_trained = train_jax(\n","        iter(dataset),\n","        sparsity_coefficient=sparsity_coefficient,\n","        use_pre_enc_bias=use_pre_enc_bias,\n","        fix_decoder_norms=fix_decoder_norms,\n","    )\n","\n","    # Train using the PyTorch implementation\n","    sae_pt_trained = train_pt(\n","        iter(dataset),\n","        sparsity_coefficient=sparsity_coefficient,\n","        use_pre_enc_bias=use_pre_enc_bias,\n","        fix_decoder_norms=fix_decoder_norms,\n","    )\n","\n","    # First we want to make sure the params have actually evolved, otherwise\n","    # this test isn't meaningful!\n","    chex.assert_trees_all_close(\n","        jax.tree.map(\n","            lambda x, y: np.mean(np.abs(x - y)) > 0.001,\n","            params_init,\n","            params_jax_trained,\n","        ),\n","        jax.tree.map(lambda _: True, params_init),\n","    )\n","\n","    # Now we check whether the parameters obtained using either implementation\n","    # are close\n","    chex.assert_trees_all_close(\n","        dataclasses.asdict(params_jax_trained),\n","        jax.tree.map(lambda x: x.numpy(), dict(sae_pt_trained.state_dict())),\n","    )\n","\n","    print(\"OK.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"0yIRU1pjmQJT","executionInfo":{"status":"ok","timestamp":1723223922304,"user_tz":-60,"elapsed":2633,"user":{"displayName":"Senthooran Rajamanoharan","userId":"06434430717005283688"}},"outputId":"2bc9ddd6-0b8d-4381-d5cf-9bd0bdce777e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing sparsity_coefficient=0.0, use_pre_enc_bias=True, fix_decoder_norms=True... OK.\n","Testing sparsity_coefficient=0.0, use_pre_enc_bias=True, fix_decoder_norms=False... OK.\n","Testing sparsity_coefficient=0.0, use_pre_enc_bias=False, fix_decoder_norms=True... OK.\n","Testing sparsity_coefficient=0.0, use_pre_enc_bias=False, fix_decoder_norms=False... OK.\n","Testing sparsity_coefficient=0.01, use_pre_enc_bias=True, fix_decoder_norms=True... OK.\n","Testing sparsity_coefficient=0.01, use_pre_enc_bias=True, fix_decoder_norms=False... OK.\n","Testing sparsity_coefficient=0.01, use_pre_enc_bias=False, fix_decoder_norms=True... OK.\n","Testing sparsity_coefficient=0.01, use_pre_enc_bias=False, fix_decoder_norms=False... OK.\n","Testing sparsity_coefficient=0.1, use_pre_enc_bias=True, fix_decoder_norms=True... OK.\n","Testing sparsity_coefficient=0.1, use_pre_enc_bias=True, fix_decoder_norms=False... OK.\n","Testing sparsity_coefficient=0.1, use_pre_enc_bias=False, fix_decoder_norms=True... OK.\n","Testing sparsity_coefficient=0.1, use_pre_enc_bias=False, fix_decoder_norms=False... OK.\n"]}]},{"cell_type":"markdown","source":["We can also do the following more granular tests, which may be helpful e.g. if we don't see consistency between implementations when doing end-to-end training:\n","\n","* The outputs of the SAE (reconstructions and pre-activations) are close;\n","* Whether the calculated loss is close;\n","* Whether the gradients of the loss are close.\n","\n","Running the following cell, we find that these are the same up to the default tolerances for `np.assert_allclose`:"],"metadata":{"id":"927Ci1_WK70m"}},{"cell_type":"code","source":["# @title Additional tests\n","\n","# First we check that the SAE outputs are the same (using the first batch of\n","# the dataset and initial parameters)\n","sae_reconstruction_jax, sae_pre_activations_jax = sae(\n","    params_init, dataset[0], use_pre_enc_bias=True\n",")\n","sae_pt = Sae(SAE_WIDTH, ACTIVATIONS_SIZE, use_pre_enc_bias=True)\n","sae_reconstruction_pt, sae_pre_activations_pt = sae_pt(\n","    torch.tensor(dataset[0])\n",")\n","np.testing.assert_allclose(\n","    sae_reconstruction_jax, sae_reconstruction_pt.detach().numpy()\n",")\n","np.testing.assert_allclose(\n","    sae_pre_activations_jax, sae_pre_activations_pt.detach().numpy()\n",")\n","\n","# Now we check that the losses and loss gradients match too\n","loss_jax, loss_grad_jax = jax.value_and_grad(loss)(\n","    params_init,\n","    dataset[0],\n","    sparsity_coefficient=1.2,\n","    use_pre_enc_bias=True,\n",")\n","loss_pt = loss_fn_pt(\n","    sae_pt,\n","    torch.tensor(dataset[0]),\n","    sparsity_coefficient=1.2,\n","    use_pre_enc_bias=True,\n",")\n","loss_pt.backward()\n","\n","# Check loss is close\n","np.testing.assert_allclose(loss_jax, loss_pt.detach().numpy())\n","\n","# Check gradients are close\n","for field in dataclasses.fields(params_init):\n","    np.testing.assert_allclose(\n","        getattr(loss_grad_jax, field.name),\n","        getattr(sae_pt, field.name).grad.numpy(),\n","        err_msg=f\"grad for {field.name} does not match\",\n","    )"],"metadata":{"cellView":"form","id":"-ts57xxg7Zpd"},"execution_count":null,"outputs":[]}]}