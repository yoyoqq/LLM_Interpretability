{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afc0e08b-5f7a-43e5-8cde-1de092e936e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 0. Set up / Installation / Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac76860-ea22-448e-baaa-9fa3c6e33103",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 15 14:59:33 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000000:84:00.0 Off |                    0 |\n",
      "| N/A   30C    P0              41W / 300W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef924f65-27df-4e62-a615-1c8728e03e30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path                            Type      Quota GB    Usage GB    Avail GB    Usage %  Updated\n",
      "------------------------------  ------  ----------  ----------  ----------  ---------  -------------------\n",
      "/scratch/prj/inf_narrative_msc  Group         1000         406         594         40  2025-07-15 15:00:05\n",
      "/scratch/users/k24086575        User           200           0         200          0  2025-07-15 15:00:05\n",
      "/users/k24086575                User            50          42           8         84  2025-07-15 15:00:05\n"
     ]
    }
   ],
   "source": [
    "!ceph_quota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ff6ff0-62df-4048-945f-94a28bc3f4e8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NEURONPEDIA_KEY = \"sk-np-BwxFua0jEx2cNSqsZPVlqmsfPgDKi47oEo7HAWXxiU00\"\n",
    "GEMMA2B_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1453fa36-d195-43ef-8bdb-bbf18fcd8883",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PATH = \"/users/k24086575/inf_narrative_msc/k24086575\"\n",
    "os.environ[\"HF_HOME\"] = PATH  # e.g., $SCRATCH/hf_models\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61a46935-5cd9-40ed-805e-21b3235b0d07",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x75b192ef2f90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import torch\n",
    "GEMMA2B_KEY = \"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\"\n",
    "login(token=GEMMA2B_KEY)\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39937e-d695-47f6-9f78-9701e95e1788",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 1. Load/Save Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f159baf-453b-491e-886f-8070a7cca070",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_dataset(path):\n",
    "    df = pd.read_csv(path, encoding=\"utf-8-sig\", quoting=1)\n",
    "    return df\n",
    "\n",
    "def save_dataset(path):\n",
    "    small_df.to_csv(path, encoding=\"utf-8-sig\", quoting=1, index=False)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25d43164-dfcc-40b5-8068-6b2b7b6b4c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./programming.csv\"\n",
    "df = load_dataset(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8701161-cc9b-4e0a-a604-f5f703893243",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>prompt</th>\n",
       "      <th>category</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer0-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer0-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer0-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer1-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer1-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer1-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer2-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer2-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer2-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer3-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer3-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer3-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer4-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer4-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer4-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer5-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer5-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer5-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer6-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer6-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer6-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer7-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer7-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer7-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer8-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer8-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer8-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer9-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer9-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer9-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer10-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer10-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer10-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer11-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer11-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer11-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer12-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer12-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer12-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer13-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer13-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer13-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer14-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer14-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer14-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer15-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer15-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer15-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer16-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer16-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer16-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer17-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer17-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer17-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer18-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer18-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer18-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer19-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer19-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer19-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer20-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer20-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer20-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer21-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer21-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer21-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer22-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer22-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer22-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer23-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer23-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer23-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer24-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer24-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer24-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer25-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer25-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer25-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer0-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer0-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer0-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer1-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer2-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer3-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer4-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer5-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer6-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer7-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer8-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer9-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer10-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer11-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer12-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer13-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer14-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer15-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer16-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer17-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer18-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer19-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer20-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer21-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer22-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer23-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer24-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical-layer25-token_feature_ids</th>\n",
       "      <th>gemma-scope-2b-pt-att-canonical-layer0-token_feature_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>programming</td>\n",
       "      <td>Polo, the Penguin, likes lucky strings - the s...</td>\n",
       "      <td>hard</td>\n",
       "      <td>[[8920, 12838, 12950, 275, 15454, 10006, 1381,...</td>\n",
       "      <td>[[296.66, 121.98, 110.27, 41.6, 32.33, 27.85, ...</td>\n",
       "      <td>[8920, 12838, 12950, 4213, 3036, 13323, 3148, ...</td>\n",
       "      <td>[[9770, 5146, 12054, 740, 13412, 10589, 12539,...</td>\n",
       "      <td>[[371.31, 249.65, 216.22, 81.79, 76.65, 69.85,...</td>\n",
       "      <td>[9770, 5146, 12054, 12054, 6443, 16357, 6688, ...</td>\n",
       "      <td>[[15089, 14059, 7132, 7361, 4885, 13977, 11527...</td>\n",
       "      <td>[[501.75, 87.74, 72.01, 69.0, 66.38, 56.41, 42...</td>\n",
       "      <td>[15089, 14059, 7132, 16363, 13096, 15818, 5098...</td>\n",
       "      <td>[[9134, 6486, 14238, 58, 3013, 16178, 2604, 79...</td>\n",
       "      <td>[[579.41, 183.42, 158.95, 115.84, 50.69, 49.05...</td>\n",
       "      <td>[9134, 6486, 14238, 895, 4303, 13975, 3132, 91...</td>\n",
       "      <td>[[12690, 11570, 6680, 14155, 13325, 5575, 1118...</td>\n",
       "      <td>[[743.31, 95.4, 86.55, 61.71, 52.7, 48.11, 39....</td>\n",
       "      <td>[12690, 11570, 6680, 8265, 6717, 2346, 5924, 1...</td>\n",
       "      <td>[[1059, 8392, 5148, 1515, 13960, 10966, 9380, ...</td>\n",
       "      <td>[[832.61, 225.61, 202.2, 71.07, 64.67, 64.17, ...</td>\n",
       "      <td>[1059, 8392, 5148, 3235, 3793, 2712, 1386, 105...</td>\n",
       "      <td>[[9743, 6201, 14077, 4478, 4485, 8883, 8765, 4...</td>\n",
       "      <td>[[801.94, 560.02, 114.18, 102.17, 99.94, 94.5,...</td>\n",
       "      <td>[9743, 6201, 14077, 15854, 15079, 13997, 5938,...</td>\n",
       "      <td>[[12287, 14119, 14537, 516, 13236, 13027, 6142...</td>\n",
       "      <td>[[819.16, 369.21, 350.83, 185.37, 182.52, 159....</td>\n",
       "      <td>[12287, 14119, 14537, 2523, 9664, 12429, 6855,...</td>\n",
       "      <td>[[9213, 4399, 6069, 3397, 13188, 12695, 6524, ...</td>\n",
       "      <td>[[868.85, 240.27, 224.58, 217.49, 134.63, 127....</td>\n",
       "      <td>[9213, 4399, 6069, 6291, 1592, 564, 855, 9213,...</td>\n",
       "      <td>[[12102, 5877, 3736, 9892, 10966, 12253, 4212,...</td>\n",
       "      <td>[[1051.21, 380.23, 202.0, 168.1, 167.91, 135.7...</td>\n",
       "      <td>[12102, 5877, 3736, 381, 12102, 14717, 222, 12...</td>\n",
       "      <td>[[4392, 2843, 3736, 2575, 5501, 6655, 9892, 57...</td>\n",
       "      <td>[[1203.91, 499.68, 185.55, 177.32, 153.59, 137...</td>\n",
       "      <td>[4392, 2843, 3736, 4392, 3736, 14372, 855, 223...</td>\n",
       "      <td>[[12945, 13423, 10772, 7228, 6222, 8238, 7675,...</td>\n",
       "      <td>[[1365.93, 254.19, 229.56, 216.8, 200.8, 160.8...</td>\n",
       "      <td>[12945, 13423, 10772, 12945, 13423, 14717, 316...</td>\n",
       "      <td>[[1041, 7507, 11087, 3220, 11767, 11752, 14669...</td>\n",
       "      <td>[[1436.49, 288.36, 274.5, 177.84, 175.78, 169....</td>\n",
       "      <td>[1041, 7507, 11087, 4667, 11087, 1178, 12358, ...</td>\n",
       "      <td>[[11248, 13423, 15319, 12322, 15392, 12879, 15...</td>\n",
       "      <td>[[1599.96, 427.65, 387.72, 267.32, 151.79, 147...</td>\n",
       "      <td>[11248, 13423, 15319, 13423, 11248, 13296, 635...</td>\n",
       "      <td>[[15567, 7214, 6812, 9165, 7601, 8995, 8818, 3...</td>\n",
       "      <td>[[1327.58, 937.92, 207.4, 193.68, 179.55, 165....</td>\n",
       "      <td>[15567, 7214, 6812, 12313, 14818, 7214, 11958,...</td>\n",
       "      <td>[[10716, 8610, 13870, 14717, 6266, 10404, 7546...</td>\n",
       "      <td>[[1806.8, 784.7, 486.79, 369.48, 286.7, 204.34...</td>\n",
       "      <td>[10716, 8610, 13870, 10716, 14717, 8610, 1695,...</td>\n",
       "      <td>[[16028, 10480, 14919, 4500, 2717, 4631, 4005,...</td>\n",
       "      <td>[[1901.41, 718.13, 386.08, 328.92, 205.49, 198...</td>\n",
       "      <td>[16028, 10480, 14919, 4500, 16028, 9572, 5169,...</td>\n",
       "      <td>[[7127, 7921, 9095, 12507, 10848, 999, 1467, 1...</td>\n",
       "      <td>[[1963.47, 777.35, 402.1, 298.72, 265.57, 229....</td>\n",
       "      <td>[7127, 7921, 9095, 11527, 7921, 2873, 11828, 7...</td>\n",
       "      <td>[[3851, 15394, 12685, 1222, 2336, 5803, 4677, ...</td>\n",
       "      <td>[[2052.88, 763.03, 348.91, 271.26, 259.58, 222...</td>\n",
       "      <td>[3851, 15394, 12685, 15394, 3232, 9392, 12368,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[12025, 11046, 3543, 5149, 5989, 4534, 1751, 1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[6631, 743, 5052, 14993, 3019, 7482, 10881, 30...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[4138, 13557, 15170, 4138, 14723, 7402, 12475,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[15056, 2037, 630, 15056, 14921, 15986, 4608, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3956, 6961, 8321, 6961, 1346, 3956, 12475, 69...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[1802, 14391, 14836, 3173, 501, 5305, 3848, 13...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7479, 5149, 14186, 1625, 8002, 1734, 14325, 1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[15745, 2628, 10077, 9038, 16026, 15746, 1213,...</td>\n",
       "      <td>[7101, 890, 11950, 12057, 11926, 13552, 12057,...</td>\n",
       "      <td>[5817, 4198, 2493, 2493, 5817, 15465, 5817, 62...</td>\n",
       "      <td>[4730, 6258, 5254, 3710, 4730, 4185, 4730, 587...</td>\n",
       "      <td>[12597, 10159, 14164, 12597, 2793, 14164, 2793...</td>\n",
       "      <td>[15592, 12904, 4700, 686, 15592, 5758, 15592, ...</td>\n",
       "      <td>[7589, 6941, 7590, 8424, 9583, 440, 7589, 1125...</td>\n",
       "      <td>[12096, 11676, 7040, 12096, 12542, 9778, 12096...</td>\n",
       "      <td>[1367, 10938, 5451, 1367, 10312, 13, 1367, 155...</td>\n",
       "      <td>[11525, 11530, 2788, 11525, 2788, 7622, 11525,...</td>\n",
       "      <td>[7495, 3117, 6381, 7495, 8202, 5376, 3971, 906...</td>\n",
       "      <td>[6316, 3458, 15102, 6316, 3318, 6910, 6316, 54...</td>\n",
       "      <td>[9338, 2350, 11394, 2636, 8191, 10598, 1486, 5...</td>\n",
       "      <td>[13617, 4166, 16213, 249, 3869, 7095, 13617, 8...</td>\n",
       "      <td>[14672, 2673, 3001, 8681, 7488, 13451, 14672, ...</td>\n",
       "      <td>[75, 12355, 8833, 75, 14877, 13234, 75, 13782,...</td>\n",
       "      <td>[9218, 7586, 6883, 11167, 11377, 2604, 9218, 7...</td>\n",
       "      <td>[5845, 11167, 1359, 2506, 1348, 1600, 5845, 81...</td>\n",
       "      <td>[77, 7228, 16359, 6022, 3244, 8081, 77, 8541, ...</td>\n",
       "      <td>[9071, 3170, 11447, 11447, 4032, 5063, 11447, ...</td>\n",
       "      <td>[8468, 14729, 12522, 88, 13882, 8501, 15663, 8...</td>\n",
       "      <td>[10730, 13775, 6168, 13966, 10103, 11567, 1124...</td>\n",
       "      <td>[4723, 2531, 12567, 4723, 6259, 7769, 4723, 54...</td>\n",
       "      <td>[3567, 14957, 0, 10184, 15497, 15829, 15829, 1...</td>\n",
       "      <td>[16058, 282, 102, 6437, 9329, 282, 9329, 282, ...</td>\n",
       "      <td>[15890, 12642, 10593, 8364, 11862, 4589, 4589,...</td>\n",
       "      <td>[3880, 1421, 1608, 3880, 11654, 1608, 15477, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>programming</td>\n",
       "      <td>Sergey works as a programmer. Like all program...</td>\n",
       "      <td>hard</td>\n",
       "      <td>[[8920, 12838, 12950, 275, 15454, 10006, 1381,...</td>\n",
       "      <td>[[296.66, 121.98, 110.27, 41.6, 32.33, 27.85, ...</td>\n",
       "      <td>[8920, 12838, 12950, 8920, 10998, 4467, 1442, ...</td>\n",
       "      <td>[[9770, 5146, 12054, 740, 13412, 10589, 12539,...</td>\n",
       "      <td>[[371.31, 249.65, 216.22, 81.79, 76.65, 69.85,...</td>\n",
       "      <td>[9770, 5146, 12054, 12054, 8967, 100, 9970, 12...</td>\n",
       "      <td>[[15089, 14059, 7132, 7361, 4885, 13977, 11527...</td>\n",
       "      <td>[[501.75, 87.74, 72.01, 69.0, 66.37, 56.4, 42....</td>\n",
       "      <td>[15089, 14059, 7132, 4604, 1147, 10673, 14363,...</td>\n",
       "      <td>[[9134, 6486, 14238, 58, 3013, 16178, 2604, 79...</td>\n",
       "      <td>[[579.41, 183.43, 158.96, 115.85, 50.67, 49.05...</td>\n",
       "      <td>[9134, 6486, 14238, 4436, 16028, 9134, 9134, 1...</td>\n",
       "      <td>[[12690, 11570, 6680, 14155, 13325, 5575, 1118...</td>\n",
       "      <td>[[743.17, 95.38, 86.7, 61.71, 52.69, 48.1, 39....</td>\n",
       "      <td>[12690, 11570, 6680, 6717, 9668, 3406, 12690, ...</td>\n",
       "      <td>[[1059, 8392, 5148, 1515, 13960, 10966, 9380, ...</td>\n",
       "      <td>[[832.5, 225.58, 201.76, 71.06, 64.67, 64.16, ...</td>\n",
       "      <td>[1059, 8392, 5148, 3235, 1059, 9399, 1059, 964...</td>\n",
       "      <td>[[9743, 6201, 14077, 4478, 4485, 8883, 8765, 4...</td>\n",
       "      <td>[[801.48, 560.17, 114.19, 102.15, 99.93, 94.48...</td>\n",
       "      <td>[9743, 6201, 14077, 15854, 8980, 14397, 9743, ...</td>\n",
       "      <td>[[12287, 14119, 14537, 516, 13236, 13027, 6142...</td>\n",
       "      <td>[[818.68, 369.25, 350.78, 185.39, 182.49, 159....</td>\n",
       "      <td>[12287, 14119, 14537, 12287, 9664, 2523, 12287...</td>\n",
       "      <td>[[9213, 4399, 6069, 3397, 13188, 12695, 6524, ...</td>\n",
       "      <td>[[868.39, 240.36, 224.62, 217.68, 134.67, 127....</td>\n",
       "      <td>[9213, 4399, 6069, 6291, 15069, 9213, 9213, 50...</td>\n",
       "      <td>[[12102, 5877, 3736, 9892, 10966, 12253, 4212,...</td>\n",
       "      <td>[[1050.26, 380.52, 202.04, 168.18, 167.94, 135...</td>\n",
       "      <td>[12102, 5877, 3736, 12102, 381, 14717, 12102, ...</td>\n",
       "      <td>[[4392, 2843, 3736, 2575, 5501, 6655, 9892, 57...</td>\n",
       "      <td>[[1202.96, 500.0, 185.55, 177.2, 153.67, 137.9...</td>\n",
       "      <td>[4392, 2843, 3736, 4392, 3736, 14372, 13321, 2...</td>\n",
       "      <td>[[12945, 13423, 10772, 7228, 6222, 8238, 7675,...</td>\n",
       "      <td>[[1364.97, 254.04, 229.63, 216.92, 200.95, 160...</td>\n",
       "      <td>[12945, 13423, 10772, 12945, 13423, 10772, 129...</td>\n",
       "      <td>[[1041, 7507, 11087, 3220, 11767, 11752, 14669...</td>\n",
       "      <td>[[1435.57, 288.27, 274.5, 177.88, 175.84, 169....</td>\n",
       "      <td>[1041, 7507, 11087, 4667, 11087, 1178, 2686, 4...</td>\n",
       "      <td>[[11248, 13423, 15319, 12322, 15392, 12879, 15...</td>\n",
       "      <td>[[1599.98, 427.68, 387.68, 267.37, 151.81, 147...</td>\n",
       "      <td>[11248, 13423, 15319, 13423, 11248, 13296, 621...</td>\n",
       "      <td>[[15567, 7214, 6812, 9165, 7601, 8995, 8818, 3...</td>\n",
       "      <td>[[1327.57, 937.92, 207.5, 193.71, 179.61, 165....</td>\n",
       "      <td>[15567, 7214, 6812, 12313, 14818, 16123, 12801...</td>\n",
       "      <td>[[10716, 8610, 13870, 14717, 6266, 10404, 7546...</td>\n",
       "      <td>[[1806.8, 784.64, 486.95, 369.44, 286.78, 204....</td>\n",
       "      <td>[10716, 8610, 13870, 10716, 14717, 8610, 1442,...</td>\n",
       "      <td>[[16028, 10480, 14919, 4500, 2717, 4631, 4005,...</td>\n",
       "      <td>[[1901.42, 718.07, 386.14, 328.87, 205.53, 198...</td>\n",
       "      <td>[16028, 10480, 14919, 4500, 16028, 9572, 16028...</td>\n",
       "      <td>[[7127, 7921, 9095, 12507, 10848, 999, 1467, 1...</td>\n",
       "      <td>[[1963.47, 777.35, 402.1, 298.72, 265.57, 229....</td>\n",
       "      <td>[7127, 7921, 9095, 4633, 11527, 1966, 10198, 2...</td>\n",
       "      <td>[[3851, 15394, 12685, 1222, 2336, 5803, 4677, ...</td>\n",
       "      <td>[[2052.88, 763.03, 348.91, 271.26, 259.58, 222...</td>\n",
       "      <td>[3851, 15394, 12685, 11706, 16336, 12187, 8799...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[12025, 11046, 3543, 9920, 445, 1792, 517, 128...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[6631, 743, 5052, 1792, 1116, 5956, 5958, 6376...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[4138, 13557, 15170, 15468, 9149, 3158, 2050, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[15056, 2037, 630, 13234, 15056, 14398, 10229,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3956, 6961, 8321, 4225, 1923, 1650, 13915, 12...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[1802, 14391, 14836, 1778, 14870, 14753, 12565...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7479, 5149, 14186, 5755, 14325, 3017, 3988, 1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[15745, 2628, 10077, 8617, 10445, 9038, 12046,...</td>\n",
       "      <td>[7101, 890, 11950, 12057, 11926, 3738, 13568, ...</td>\n",
       "      <td>[5817, 4198, 2493, 10322, 5675, 13332, 13276, ...</td>\n",
       "      <td>[4730, 6258, 5254, 3710, 4730, 13224, 12538, 1...</td>\n",
       "      <td>[12597, 10159, 14164, 2793, 14164, 12597, 2793...</td>\n",
       "      <td>[15592, 12904, 4700, 686, 15592, 4640, 15592, ...</td>\n",
       "      <td>[7589, 6941, 7590, 10287, 7589, 3783, 6941, 74...</td>\n",
       "      <td>[12096, 11676, 7040, 12096, 9778, 6117, 12096,...</td>\n",
       "      <td>[1367, 10938, 5451, 8248, 14653, 1367, 1367, 9...</td>\n",
       "      <td>[11525, 11530, 2788, 11525, 2788, 7622, 11525,...</td>\n",
       "      <td>[7495, 3117, 6381, 7495, 8202, 5376, 7495, 397...</td>\n",
       "      <td>[6316, 3458, 15102, 6316, 3318, 6910, 6316, 54...</td>\n",
       "      <td>[9338, 2350, 11394, 2636, 8191, 2613, 5793, 58...</td>\n",
       "      <td>[13617, 4166, 16213, 249, 3869, 4290, 15550, 2...</td>\n",
       "      <td>[14672, 2673, 3001, 8681, 7488, 13451, 14672, ...</td>\n",
       "      <td>[75, 12355, 8833, 75, 13234, 14877, 5721, 75, ...</td>\n",
       "      <td>[9218, 7586, 6883, 11167, 11377, 3146, 9218, 7...</td>\n",
       "      <td>[5845, 11167, 1359, 2506, 1348, 1600, 2505, 10...</td>\n",
       "      <td>[77, 7228, 16359, 5608, 4214, 9163, 5140, 3886...</td>\n",
       "      <td>[9071, 3170, 11447, 14584, 10987, 11447, 3877,...</td>\n",
       "      <td>[8468, 14729, 12522, 4366, 16382, 2247, 6445, ...</td>\n",
       "      <td>[10730, 13775, 6168, 13966, 238, 15592, 8977, ...</td>\n",
       "      <td>[4723, 2531, 12567, 4723, 12222, 15987, 1764, ...</td>\n",
       "      <td>[3567, 14957, 0, 10184, 7987, 24, 15497, 14757...</td>\n",
       "      <td>[16058, 282, 102, 9329, 282, 6087, 9329, 282, ...</td>\n",
       "      <td>[15890, 12642, 10593, 4589, 11648, 5433, 13264...</td>\n",
       "      <td>[3880, 1421, 1608, 11654, 9944, 3880, 9944, 65...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>programming</td>\n",
       "      <td>Chef recently cooked a big cake that can be re...</td>\n",
       "      <td>hard</td>\n",
       "      <td>[[8920, 12838, 12950, 275, 15454, 10006, 1381,...</td>\n",
       "      <td>[[296.45, 121.83, 110.23, 41.58, 32.31, 27.84,...</td>\n",
       "      <td>[8920, 12838, 12950, 6177, 8920, 9163, 13668, ...</td>\n",
       "      <td>[[9770, 5146, 12054, 740, 13412, 10589, 12539,...</td>\n",
       "      <td>[[371.16, 249.53, 216.09, 81.92, 76.67, 69.79,...</td>\n",
       "      <td>[9770, 5146, 12054, 14290, 12054, 8328, 505, 1...</td>\n",
       "      <td>[[15089, 14059, 7132, 7361, 4885, 13977, 11527...</td>\n",
       "      <td>[[501.76, 87.85, 72.01, 69.01, 66.39, 56.41, 4...</td>\n",
       "      <td>[15089, 14059, 7132, 4120, 14359, 3064, 2986, ...</td>\n",
       "      <td>[[9134, 6486, 14238, 58, 3013, 16178, 2604, 79...</td>\n",
       "      <td>[[579.44, 183.46, 158.98, 115.86, 50.67, 49.15...</td>\n",
       "      <td>[9134, 6486, 14238, 13144, 2475, 5806, 8594, 9...</td>\n",
       "      <td>[[12690, 11570, 6680, 14155, 13325, 5575, 1118...</td>\n",
       "      <td>[[743.46, 95.41, 86.38, 61.69, 52.69, 48.11, 3...</td>\n",
       "      <td>[12690, 11570, 6680, 15419, 12690, 6717, 12629...</td>\n",
       "      <td>[[1059, 8392, 5148, 1515, 13960, 10966, 9380, ...</td>\n",
       "      <td>[[832.62, 225.6, 202.19, 71.06, 64.64, 64.16, ...</td>\n",
       "      <td>[1059, 8392, 5148, 3235, 5351, 1059, 1059, 185...</td>\n",
       "      <td>[[9743, 6201, 14077, 4478, 4485, 8883, 8765, 4...</td>\n",
       "      <td>[[801.95, 560.02, 114.18, 102.17, 99.91, 94.47...</td>\n",
       "      <td>[9743, 6201, 14077, 15854, 10273, 10367, 7121,...</td>\n",
       "      <td>[[12287, 14119, 14537, 516, 13236, 13027, 6142...</td>\n",
       "      <td>[[819.17, 369.21, 350.81, 185.37, 182.5, 159.4...</td>\n",
       "      <td>[12287, 14119, 14537, 12287, 2523, 9664, 589, ...</td>\n",
       "      <td>[[9213, 4399, 6069, 3397, 13188, 12695, 6524, ...</td>\n",
       "      <td>[[868.86, 240.25, 224.56, 217.49, 134.62, 127....</td>\n",
       "      <td>[9213, 4399, 6069, 6291, 10304, 9213, 2592, 20...</td>\n",
       "      <td>[[12102, 5877, 3736, 9892, 10966, 12253, 4212,...</td>\n",
       "      <td>[[1051.21, 380.22, 201.99, 168.09, 167.9, 135....</td>\n",
       "      <td>[12102, 5877, 3736, 12102, 381, 14717, 326, 54...</td>\n",
       "      <td>[[4392, 2843, 3736, 2575, 5501, 6655, 9892, 57...</td>\n",
       "      <td>[[1203.92, 499.68, 185.54, 177.32, 153.59, 137...</td>\n",
       "      <td>[4392, 2843, 3736, 4392, 3736, 14372, 15181, 1...</td>\n",
       "      <td>[[12945, 13423, 10772, 7228, 6222, 8238, 7675,...</td>\n",
       "      <td>[[1365.93, 254.19, 229.56, 216.78, 200.8, 160....</td>\n",
       "      <td>[12945, 13423, 10772, 12945, 13423, 10772, 123...</td>\n",
       "      <td>[[1041, 7507, 11087, 3220, 11767, 11752, 14669...</td>\n",
       "      <td>[[1436.48, 288.43, 274.5, 177.83, 175.78, 169....</td>\n",
       "      <td>[1041, 7507, 11087, 4667, 11087, 1178, 14240, ...</td>\n",
       "      <td>[[11248, 13423, 15319, 12322, 15392, 12879, 15...</td>\n",
       "      <td>[[1599.96, 427.66, 387.75, 267.3, 151.79, 147....</td>\n",
       "      <td>[11248, 13423, 15319, 13423, 11248, 13296, 206...</td>\n",
       "      <td>[[15567, 7214, 6812, 9165, 7601, 8995, 8818, 3...</td>\n",
       "      <td>[[1327.56, 937.95, 207.39, 193.68, 179.55, 165...</td>\n",
       "      <td>[15567, 7214, 6812, 12313, 16123, 14818, 15726...</td>\n",
       "      <td>[[10716, 8610, 13870, 14717, 6266, 10404, 7546...</td>\n",
       "      <td>[[1806.78, 784.74, 486.8, 369.5, 286.7, 204.35...</td>\n",
       "      <td>[10716, 8610, 13870, 10716, 14717, 8610, 11296...</td>\n",
       "      <td>[[16028, 10480, 14919, 4500, 2717, 4631, 4005,...</td>\n",
       "      <td>[[1901.39, 718.14, 386.09, 328.93, 205.49, 198...</td>\n",
       "      <td>[16028, 10480, 14919, 4500, 16028, 10480, 7168...</td>\n",
       "      <td>[[7127, 7921, 9095, 12507, 10848, 999, 1467, 1...</td>\n",
       "      <td>[[1963.47, 777.35, 402.1, 298.72, 265.57, 229....</td>\n",
       "      <td>[7127, 7921, 9095, 11527, 7921, 6492, 13605, 1...</td>\n",
       "      <td>[[3851, 15394, 12685, 1222, 2336, 5803, 4677, ...</td>\n",
       "      <td>[[2052.88, 763.03, 348.91, 271.26, 259.58, 222...</td>\n",
       "      <td>[3851, 15394, 12685, 11635, 7373, 8738, 1460, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[12025, 11046, 3543, 1494, 5149, 6788, 13509, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[6631, 743, 5052, 8191, 674, 3019, 4853, 14619...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[4138, 13557, 15170, 1674, 14723, 7632, 13411,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[15056, 2037, 630, 7392, 11901, 15916, 4101, 1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3956, 6961, 8321, 10217, 14332, 11796, 12829,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[1802, 14391, 14836, 5328, 14870, 9188, 2952, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7479, 5149, 14186, 14325, 8456, 3017, 13757, ...</td>\n",
       "      <td>[[15745, 2628, 10077, 8629, 9147, 3088, 10698,...</td>\n",
       "      <td>[[132.27, 56.69, 44.7, 34.79, 13.75, 11.44, 11...</td>\n",
       "      <td>[15745, 2628, 10077, 9038, 1024, 7694, 1024, 1...</td>\n",
       "      <td>[7101, 890, 11950, 12057, 11141, 11926, 16307,...</td>\n",
       "      <td>[5817, 4198, 2493, 5817, 13276, 11575, 633, 11...</td>\n",
       "      <td>[4730, 6258, 5254, 4185, 4730, 3710, 8338, 473...</td>\n",
       "      <td>[12597, 10159, 14164, 12597, 2793, 14768, 2793...</td>\n",
       "      <td>[15592, 12904, 4700, 686, 15592, 5758, 15592, ...</td>\n",
       "      <td>[7589, 6941, 7590, 7589, 6941, 4682, 11752, 14...</td>\n",
       "      <td>[12096, 11676, 7040, 12096, 6117, 12542, 12096...</td>\n",
       "      <td>[1367, 10938, 5451, 1367, 13, 10312, 1367, 142...</td>\n",
       "      <td>[11525, 11530, 2788, 11525, 2788, 7622, 11525,...</td>\n",
       "      <td>[7495, 3117, 6381, 7495, 8202, 5376, 3966, 397...</td>\n",
       "      <td>[6316, 3458, 15102, 6316, 3318, 6910, 6316, 54...</td>\n",
       "      <td>[9338, 2350, 11394, 2636, 8191, 2613, 5793, 93...</td>\n",
       "      <td>[13617, 4166, 16213, 249, 3869, 4290, 13617, 7...</td>\n",
       "      <td>[14672, 2673, 3001, 8681, 7488, 2564, 13946, 9...</td>\n",
       "      <td>[75, 12355, 8833, 75, 14877, 3670, 75, 13978, ...</td>\n",
       "      <td>[9218, 7586, 6883, 11167, 2604, 11377, 9218, 9...</td>\n",
       "      <td>[5845, 11167, 1359, 2506, 1600, 1348, 3265, 71...</td>\n",
       "      <td>[77, 7228, 16359, 199, 15323, 3757, 77, 15340,...</td>\n",
       "      <td>[9071, 3170, 11447, 11447, 5324, 14557, 4675, ...</td>\n",
       "      <td>[8468, 14729, 12522, 13882, 9232, 6443, 10306,...</td>\n",
       "      <td>[10730, 13775, 6168, 1527, 12069, 8048, 14050,...</td>\n",
       "      <td>[4723, 2531, 12567, 4723, 11010, 9108, 1172, 1...</td>\n",
       "      <td>[3567, 14957, 0, 10184, 15497, 1926, 10184, 16...</td>\n",
       "      <td>[16058, 282, 102, 282, 9329, 14105, 282, 9329,...</td>\n",
       "      <td>[15890, 12642, 10593, 4589, 13264, 7327, 13264...</td>\n",
       "      <td>[3880, 1421, 1608, 11654, 3880, 9944, 9944, 79...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>programming</td>\n",
       "      <td>Problem description\\nRudreshwar likes random n...</td>\n",
       "      <td>hard</td>\n",
       "      <td>[[8920, 12838, 12950, 275, 15454, 10006, 1381,...</td>\n",
       "      <td>[[296.66, 121.99, 110.26, 41.59, 32.33, 27.85,...</td>\n",
       "      <td>[8920, 12838, 12950, 9079, 11686, 11410, 97, 6...</td>\n",
       "      <td>[[9770, 5146, 12054, 740, 13412, 10589, 12539,...</td>\n",
       "      <td>[[371.31, 249.65, 216.23, 81.79, 76.65, 69.84,...</td>\n",
       "      <td>[9770, 5146, 12054, 9794, 1982, 15214, 3800, 1...</td>\n",
       "      <td>[[15089, 14059, 7132, 7361, 4885, 13977, 11527...</td>\n",
       "      <td>[[501.75, 87.74, 72.01, 69.0, 66.38, 56.41, 42...</td>\n",
       "      <td>[15089, 14059, 7132, 2634, 6102, 1650, 1724, 6...</td>\n",
       "      <td>[[9134, 6486, 14238, 58, 3013, 16178, 2604, 79...</td>\n",
       "      <td>[[579.41, 183.44, 158.95, 115.85, 50.67, 49.05...</td>\n",
       "      <td>[9134, 6486, 14238, 11917, 5806, 4303, 9432, 9...</td>\n",
       "      <td>[[12690, 11570, 6680, 14155, 13325, 5575, 1118...</td>\n",
       "      <td>[[743.32, 95.39, 86.54, 61.7, 52.69, 48.1, 39....</td>\n",
       "      <td>[12690, 11570, 6680, 9364, 6717, 1261, 5973, 1...</td>\n",
       "      <td>[[1059, 8392, 5148, 1515, 13960, 10966, 9380, ...</td>\n",
       "      <td>[[832.56, 225.58, 201.97, 71.06, 64.64, 64.15,...</td>\n",
       "      <td>[1059, 8392, 5148, 3235, 5871, 1059, 5749, 105...</td>\n",
       "      <td>[[9743, 6201, 14077, 4478, 4485, 8883, 8765, 4...</td>\n",
       "      <td>[[801.49, 560.17, 114.18, 102.15, 99.9, 94.46,...</td>\n",
       "      <td>[9743, 6201, 14077, 12483, 15854, 12062, 16025...</td>\n",
       "      <td>[[12287, 14119, 14537, 516, 13236, 13027, 6142...</td>\n",
       "      <td>[[818.69, 369.27, 350.74, 185.39, 182.48, 159....</td>\n",
       "      <td>[12287, 14119, 14537, 9664, 12287, 11454, 1511...</td>\n",
       "      <td>[[9213, 4399, 6069, 3397, 13188, 12695, 6524, ...</td>\n",
       "      <td>[[868.38, 240.24, 224.51, 217.53, 134.62, 127....</td>\n",
       "      <td>[9213, 4399, 6069, 6291, 5235, 9213, 12661, 44...</td>\n",
       "      <td>[[12102, 5877, 3736, 9892, 10966, 12253, 4212,...</td>\n",
       "      <td>[[1050.26, 380.28, 201.96, 168.09, 167.88, 135...</td>\n",
       "      <td>[12102, 5877, 3736, 12102, 381, 4148, 12354, 5...</td>\n",
       "      <td>[[4392, 2843, 3736, 2575, 5501, 6655, 9892, 57...</td>\n",
       "      <td>[[1202.96, 499.71, 185.49, 177.23, 153.6, 137....</td>\n",
       "      <td>[4392, 2843, 3736, 4392, 3736, 14372, 4993, 26...</td>\n",
       "      <td>[[12945, 13423, 10772, 7228, 6222, 8238, 7675,...</td>\n",
       "      <td>[[1364.97, 254.08, 229.52, 216.84, 200.82, 160...</td>\n",
       "      <td>[12945, 13423, 10772, 12945, 13423, 10772, 150...</td>\n",
       "      <td>[[1041, 7507, 11087, 3220, 11767, 11752, 14669...</td>\n",
       "      <td>[[1435.57, 288.33, 274.46, 177.82, 175.78, 169...</td>\n",
       "      <td>[1041, 7507, 11087, 4667, 11087, 1178, 1724, 9...</td>\n",
       "      <td>[[11248, 13423, 15319, 12322, 15392, 12879, 15...</td>\n",
       "      <td>[[1599.97, 427.63, 387.73, 267.28, 151.79, 147...</td>\n",
       "      <td>[11248, 13423, 15319, 13423, 11248, 14396, 104...</td>\n",
       "      <td>[[15567, 7214, 6812, 9165, 7601, 8995, 8818, 3...</td>\n",
       "      <td>[[1327.59, 937.92, 207.37, 193.67, 179.55, 165...</td>\n",
       "      <td>[15567, 7214, 6812, 16123, 12313, 15567, 1998,...</td>\n",
       "      <td>[[10716, 8610, 13870, 14717, 6266, 10404, 7546...</td>\n",
       "      <td>[[1806.82, 784.65, 486.78, 369.46, 286.71, 204...</td>\n",
       "      <td>[10716, 8610, 13870, 10716, 14717, 8610, 5900,...</td>\n",
       "      <td>[[16028, 10480, 14919, 4500, 2717, 4631, 4005,...</td>\n",
       "      <td>[[1901.43, 718.07, 386.08, 328.89, 205.48, 198...</td>\n",
       "      <td>[16028, 10480, 14919, 4500, 16028, 15992, 7616...</td>\n",
       "      <td>[[7127, 7921, 9095, 12507, 10848, 999, 1467, 1...</td>\n",
       "      <td>[[1963.47, 777.35, 402.1, 298.72, 265.57, 229....</td>\n",
       "      <td>[7127, 7921, 9095, 1003, 13378, 11527, 12118, ...</td>\n",
       "      <td>[[3851, 15394, 12685, 1222, 2336, 5803, 4677, ...</td>\n",
       "      <td>[[2052.88, 763.03, 348.91, 271.26, 259.58, 222...</td>\n",
       "      <td>[3851, 15394, 12685, 7373, 10793, 9948, 15388,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[12025, 11046, 3543, 6207, 10441, 4346, 3164, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[6631, 743, 5052, 15222, 11527, 8684, 16111, 1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[4138, 13557, 15170, 12579, 3461, 1003, 6438, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[15056, 2037, 630, 6629, 10752, 8962, 16154, 3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3956, 6961, 8321, 3611, 11271, 15859, 6229, 1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[1802, 14391, 14836, 2566, 11326, 12205, 261, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7479, 5149, 14186, 1734, 16354, 14325, 13749,...</td>\n",
       "      <td>[[15745, 2628, 10077, 8629, 9147, 3088, 10698,...</td>\n",
       "      <td>[[132.37, 56.71, 44.72, 34.79, 13.8, 11.45, 11...</td>\n",
       "      <td>[15745, 2628, 10077, 9038, 4965, 1024, 15745, ...</td>\n",
       "      <td>[7101, 890, 11950, 12057, 322, 4528, 6329, 109...</td>\n",
       "      <td>[5817, 4198, 2493, 13276, 12676, 11924, 2115, ...</td>\n",
       "      <td>[4730, 6258, 5254, 3710, 4185, 7851, 6819, 100...</td>\n",
       "      <td>[12597, 10159, 14164, 12597, 2793, 14164, 990,...</td>\n",
       "      <td>[15592, 12904, 4700, 686, 15592, 4542, 15592, ...</td>\n",
       "      <td>[7589, 6941, 7590, 5371, 5046, 816, 7589, 816,...</td>\n",
       "      <td>[12096, 11676, 7040, 12096, 9778, 4053, 12096,...</td>\n",
       "      <td>[1367, 10938, 5451, 8286, 9743, 7282, 4968, 76...</td>\n",
       "      <td>[11525, 11530, 2788, 11525, 2788, 7622, 11525,...</td>\n",
       "      <td>[7495, 3117, 6381, 7495, 8202, 5376, 7495, 258...</td>\n",
       "      <td>[6316, 3458, 15102, 6316, 3318, 3458, 6316, 56...</td>\n",
       "      <td>[9338, 2350, 11394, 2636, 8191, 9338, 9338, 89...</td>\n",
       "      <td>[13617, 4166, 16213, 249, 3869, 7095, 249, 133...</td>\n",
       "      <td>[14672, 2673, 3001, 8681, 2564, 13451, 6311, 1...</td>\n",
       "      <td>[75, 12355, 8833, 75, 8833, 14877, 11232, 75, ...</td>\n",
       "      <td>[9218, 7586, 6883, 11167, 2604, 2791, 9218, 22...</td>\n",
       "      <td>[5845, 11167, 1359, 2506, 1600, 13466, 5845, 1...</td>\n",
       "      <td>[77, 7228, 16359, 11129, 6192, 8081, 1344, 77,...</td>\n",
       "      <td>[9071, 3170, 11447, 3170, 3763, 11413, 10080, ...</td>\n",
       "      <td>[8468, 14729, 12522, 13882, 14729, 13198, 1486...</td>\n",
       "      <td>[10730, 13775, 6168, 6634, 14648, 15701, 13966...</td>\n",
       "      <td>[4723, 2531, 12567, 11983, 16196, 10945, 4723,...</td>\n",
       "      <td>[3567, 14957, 0, 10184, 2923, 15497, 10184, 24...</td>\n",
       "      <td>[16058, 282, 102, 282, 9329, 1990, 9329, 282, ...</td>\n",
       "      <td>[15890, 12642, 10593, 8171, 2996, 8364, 5799, ...</td>\n",
       "      <td>[3880, 1421, 1608, 3880, 9944, 1608, 10825, 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>programming</td>\n",
       "      <td>The chef has turned into an entrepreneur runni...</td>\n",
       "      <td>hard</td>\n",
       "      <td>[[8920, 12838, 12950, 275, 15454, 10006, 1381,...</td>\n",
       "      <td>[[296.66, 121.98, 110.27, 41.6, 32.33, 27.85, ...</td>\n",
       "      <td>[8920, 12838, 12950, 9149, 8920, 11930, 6177, ...</td>\n",
       "      <td>[[9770, 5146, 12054, 740, 13412, 10589, 12539,...</td>\n",
       "      <td>[[371.32, 249.65, 216.22, 81.79, 76.64, 69.85,...</td>\n",
       "      <td>[9770, 5146, 12054, 9394, 12054, 9770, 12054, ...</td>\n",
       "      <td>[[15089, 14059, 7132, 7361, 4885, 13977, 11527...</td>\n",
       "      <td>[[501.75, 87.74, 72.01, 68.99, 66.36, 56.39, 4...</td>\n",
       "      <td>[15089, 14059, 7132, 2007, 454, 15089, 14359, ...</td>\n",
       "      <td>[[9134, 6486, 14238, 58, 3013, 16178, 2604, 79...</td>\n",
       "      <td>[[579.42, 183.41, 158.93, 115.83, 50.67, 49.05...</td>\n",
       "      <td>[9134, 6486, 14238, 13699, 2007, 9134, 13144, ...</td>\n",
       "      <td>[[12690, 11570, 6680, 14155, 13325, 5575, 1118...</td>\n",
       "      <td>[[743.31, 95.4, 86.55, 61.71, 52.7, 48.1, 39.2...</td>\n",
       "      <td>[12690, 11570, 6680, 15634, 7392, 6717, 12690,...</td>\n",
       "      <td>[[1059, 8392, 5148, 1515, 13960, 10966, 9380, ...</td>\n",
       "      <td>[[832.55, 225.6, 201.98, 71.07, 64.66, 64.15, ...</td>\n",
       "      <td>[1059, 8392, 5148, 3235, 1059, 7423, 5351, 102...</td>\n",
       "      <td>[[9743, 6201, 14077, 4478, 4485, 8883, 8765, 4...</td>\n",
       "      <td>[[801.48, 560.16, 114.19, 102.15, 99.93, 94.49...</td>\n",
       "      <td>[9743, 6201, 14077, 12062, 15854, 9743, 9743, ...</td>\n",
       "      <td>[[12287, 14119, 14537, 516, 13236, 13027, 6142...</td>\n",
       "      <td>[[818.67, 369.26, 350.8, 185.39, 182.51, 159.4...</td>\n",
       "      <td>[12287, 14119, 14537, 9664, 12287, 11774, 1228...</td>\n",
       "      <td>[[9213, 4399, 6069, 3397, 13188, 12695, 6524, ...</td>\n",
       "      <td>[[868.38, 240.37, 224.65, 217.68, 134.69, 127....</td>\n",
       "      <td>[9213, 4399, 6069, 6291, 12062, 9213, 9213, 10...</td>\n",
       "      <td>[[12102, 5877, 3736, 9892, 10966, 12253, 4212,...</td>\n",
       "      <td>[[1050.26, 380.51, 202.07, 168.18, 167.95, 135...</td>\n",
       "      <td>[12102, 5877, 3736, 12102, 381, 4148, 12102, 1...</td>\n",
       "      <td>[[4392, 2843, 3736, 2575, 5501, 6655, 9892, 57...</td>\n",
       "      <td>[[1202.96, 500.0, 185.56, 177.21, 153.68, 137....</td>\n",
       "      <td>[4392, 2843, 3736, 4392, 3736, 14372, 4392, 42...</td>\n",
       "      <td>[[12945, 13423, 10772, 7228, 6222, 8238, 7675,...</td>\n",
       "      <td>[[1364.97, 254.09, 229.63, 216.95, 200.95, 160...</td>\n",
       "      <td>[12945, 13423, 10772, 12945, 13423, 10772, 129...</td>\n",
       "      <td>[[1041, 7507, 11087, 3220, 11767, 11752, 14669...</td>\n",
       "      <td>[[1435.56, 288.28, 274.52, 177.9, 175.85, 169....</td>\n",
       "      <td>[1041, 7507, 11087, 4667, 11087, 1178, 7507, 2...</td>\n",
       "      <td>[[11248, 13423, 15319, 12322, 15392, 12879, 15...</td>\n",
       "      <td>[[1599.98, 427.7, 387.69, 267.39, 151.81, 147....</td>\n",
       "      <td>[11248, 13423, 15319, 13423, 11248, 4825, 1531...</td>\n",
       "      <td>[[15567, 7214, 6812, 9165, 7601, 8995, 8818, 3...</td>\n",
       "      <td>[[1327.56, 937.93, 207.52, 193.72, 179.61, 165...</td>\n",
       "      <td>[15567, 7214, 6812, 16123, 15567, 12313, 7214,...</td>\n",
       "      <td>[[10716, 8610, 13870, 14717, 6266, 10404, 7546...</td>\n",
       "      <td>[[1806.8, 784.66, 486.97, 369.46, 286.76, 204....</td>\n",
       "      <td>[10716, 8610, 13870, 10716, 14717, 8610, 8610,...</td>\n",
       "      <td>[[16028, 10480, 14919, 4500, 2717, 4631, 4005,...</td>\n",
       "      <td>[[1901.41, 718.08, 386.13, 328.9, 205.54, 198....</td>\n",
       "      <td>[16028, 10480, 14919, 4500, 16028, 10976, 1048...</td>\n",
       "      <td>[[7127, 7921, 9095, 12507, 10848, 999, 1467, 1...</td>\n",
       "      <td>[[1963.47, 777.35, 402.1, 298.72, 265.57, 229....</td>\n",
       "      <td>[7127, 7921, 9095, 1003, 5373, 11527, 7921, 12...</td>\n",
       "      <td>[[3851, 15394, 12685, 1222, 2336, 5803, 4677, ...</td>\n",
       "      <td>[[2052.88, 763.03, 348.91, 271.26, 259.58, 222...</td>\n",
       "      <td>[3851, 15394, 12685, 11527, 10793, 12927, 8738...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[12025, 11046, 3543, 15771, 12564, 10441, 1202...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[6631, 743, 5052, 11795, 11527, 3458, 8191, 66...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[4138, 13557, 15170, 7795, 4422, 1003, 1674, 3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[15056, 2037, 630, 14781, 15502, 10265, 7392, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3956, 6961, 8321, 10611, 3142, 5286, 14332, 1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[1802, 14391, 14836, 13969, 15621, 3848, 3848,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7479, 5149, 14186, 12942, 14325, 14491, 14325...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[15745, 2628, 10077, 9038, 3638, 15022, 1024, ...</td>\n",
       "      <td>[7101, 890, 11950, 12057, 9929, 322, 11141, 12...</td>\n",
       "      <td>[5817, 4198, 2493, 16166, 11249, 3303, 5817, 1...</td>\n",
       "      <td>[4730, 6258, 5254, 4730, 4185, 2420, 4730, 596...</td>\n",
       "      <td>[12597, 10159, 14164, 12597, 2793, 14164, 1259...</td>\n",
       "      <td>[15592, 12904, 4700, 686, 15592, 11145, 15592,...</td>\n",
       "      <td>[7589, 6941, 7590, 178, 6941, 5371, 7589, 6941...</td>\n",
       "      <td>[12096, 11676, 7040, 12096, 9778, 4053, 12096,...</td>\n",
       "      <td>[1367, 10938, 5451, 8286, 9743, 7282, 1367, 19...</td>\n",
       "      <td>[11525, 11530, 2788, 11525, 2788, 7622, 11525,...</td>\n",
       "      <td>[7495, 3117, 6381, 7495, 8202, 13140, 7495, 39...</td>\n",
       "      <td>[6316, 3458, 15102, 6316, 3318, 3458, 6316, 54...</td>\n",
       "      <td>[9338, 2350, 11394, 2636, 9338, 8191, 5793, 15...</td>\n",
       "      <td>[13617, 4166, 16213, 249, 3869, 7095, 13617, 1...</td>\n",
       "      <td>[14672, 2673, 3001, 8681, 2564, 9646, 1540, 11...</td>\n",
       "      <td>[75, 12355, 8833, 75, 8833, 6575, 75, 3879, 66...</td>\n",
       "      <td>[9218, 7586, 6883, 11167, 2604, 3467, 9218, 53...</td>\n",
       "      <td>[5845, 11167, 1359, 2506, 1600, 13466, 310, 78...</td>\n",
       "      <td>[77, 7228, 16359, 10631, 6192, 3675, 15323, 19...</td>\n",
       "      <td>[9071, 3170, 11447, 1883, 8193, 4103, 5324, 11...</td>\n",
       "      <td>[8468, 14729, 12522, 12449, 14505, 4569, 8703,...</td>\n",
       "      <td>[10730, 13775, 6168, 12508, 14050, 14734, 1527...</td>\n",
       "      <td>[4723, 2531, 12567, 4029, 281, 2833, 1255, 472...</td>\n",
       "      <td>[3567, 14957, 0, 9173, 15497, 10184, 10184, 19...</td>\n",
       "      <td>[16058, 282, 102, 282, 9329, 11490, 282, 9329,...</td>\n",
       "      <td>[15890, 12642, 10593, 14401, 13264, 13383, 458...</td>\n",
       "      <td>[3880, 1421, 1608, 3880, 1421, 1608, 9944, 388...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          type                                             prompt category  \\\n",
       "0  programming  Polo, the Penguin, likes lucky strings - the s...     hard   \n",
       "1  programming  Sergey works as a programmer. Like all program...     hard   \n",
       "2  programming  Chef recently cooked a big cake that can be re...     hard   \n",
       "3  programming  Problem description\\nRudreshwar likes random n...     hard   \n",
       "4  programming  The chef has turned into an entrepreneur runni...     hard   \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer0-top_mean_ids  \\\n",
       "0  [[8920, 12838, 12950, 275, 15454, 10006, 1381,...    \n",
       "1  [[8920, 12838, 12950, 275, 15454, 10006, 1381,...    \n",
       "2  [[8920, 12838, 12950, 275, 15454, 10006, 1381,...    \n",
       "3  [[8920, 12838, 12950, 275, 15454, 10006, 1381,...    \n",
       "4  [[8920, 12838, 12950, 275, 15454, 10006, 1381,...    \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer0-top_mean_vals  \\\n",
       "0  [[296.66, 121.98, 110.27, 41.6, 32.33, 27.85, ...     \n",
       "1  [[296.66, 121.98, 110.27, 41.6, 32.33, 27.85, ...     \n",
       "2  [[296.45, 121.83, 110.23, 41.58, 32.31, 27.84,...     \n",
       "3  [[296.66, 121.99, 110.26, 41.59, 32.33, 27.85,...     \n",
       "4  [[296.66, 121.98, 110.27, 41.6, 32.33, 27.85, ...     \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer0-token_feature_ids  \\\n",
       "0  [8920, 12838, 12950, 4213, 3036, 13323, 3148, ...         \n",
       "1  [8920, 12838, 12950, 8920, 10998, 4467, 1442, ...         \n",
       "2  [8920, 12838, 12950, 6177, 8920, 9163, 13668, ...         \n",
       "3  [8920, 12838, 12950, 9079, 11686, 11410, 97, 6...         \n",
       "4  [8920, 12838, 12950, 9149, 8920, 11930, 6177, ...         \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer1-top_mean_ids  \\\n",
       "0  [[9770, 5146, 12054, 740, 13412, 10589, 12539,...    \n",
       "1  [[9770, 5146, 12054, 740, 13412, 10589, 12539,...    \n",
       "2  [[9770, 5146, 12054, 740, 13412, 10589, 12539,...    \n",
       "3  [[9770, 5146, 12054, 740, 13412, 10589, 12539,...    \n",
       "4  [[9770, 5146, 12054, 740, 13412, 10589, 12539,...    \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer1-top_mean_vals  \\\n",
       "0  [[371.31, 249.65, 216.22, 81.79, 76.65, 69.85,...     \n",
       "1  [[371.31, 249.65, 216.22, 81.79, 76.65, 69.85,...     \n",
       "2  [[371.16, 249.53, 216.09, 81.92, 76.67, 69.79,...     \n",
       "3  [[371.31, 249.65, 216.23, 81.79, 76.65, 69.84,...     \n",
       "4  [[371.32, 249.65, 216.22, 81.79, 76.64, 69.85,...     \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer1-token_feature_ids  \\\n",
       "0  [9770, 5146, 12054, 12054, 6443, 16357, 6688, ...         \n",
       "1  [9770, 5146, 12054, 12054, 8967, 100, 9970, 12...         \n",
       "2  [9770, 5146, 12054, 14290, 12054, 8328, 505, 1...         \n",
       "3  [9770, 5146, 12054, 9794, 1982, 15214, 3800, 1...         \n",
       "4  [9770, 5146, 12054, 9394, 12054, 9770, 12054, ...         \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer2-top_mean_ids  \\\n",
       "0  [[15089, 14059, 7132, 7361, 4885, 13977, 11527...    \n",
       "1  [[15089, 14059, 7132, 7361, 4885, 13977, 11527...    \n",
       "2  [[15089, 14059, 7132, 7361, 4885, 13977, 11527...    \n",
       "3  [[15089, 14059, 7132, 7361, 4885, 13977, 11527...    \n",
       "4  [[15089, 14059, 7132, 7361, 4885, 13977, 11527...    \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer2-top_mean_vals  \\\n",
       "0  [[501.75, 87.74, 72.01, 69.0, 66.38, 56.41, 42...     \n",
       "1  [[501.75, 87.74, 72.01, 69.0, 66.37, 56.4, 42....     \n",
       "2  [[501.76, 87.85, 72.01, 69.01, 66.39, 56.41, 4...     \n",
       "3  [[501.75, 87.74, 72.01, 69.0, 66.38, 56.41, 42...     \n",
       "4  [[501.75, 87.74, 72.01, 68.99, 66.36, 56.39, 4...     \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer2-token_feature_ids  \\\n",
       "0  [15089, 14059, 7132, 16363, 13096, 15818, 5098...         \n",
       "1  [15089, 14059, 7132, 4604, 1147, 10673, 14363,...         \n",
       "2  [15089, 14059, 7132, 4120, 14359, 3064, 2986, ...         \n",
       "3  [15089, 14059, 7132, 2634, 6102, 1650, 1724, 6...         \n",
       "4  [15089, 14059, 7132, 2007, 454, 15089, 14359, ...         \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer3-top_mean_ids  \\\n",
       "0  [[9134, 6486, 14238, 58, 3013, 16178, 2604, 79...    \n",
       "1  [[9134, 6486, 14238, 58, 3013, 16178, 2604, 79...    \n",
       "2  [[9134, 6486, 14238, 58, 3013, 16178, 2604, 79...    \n",
       "3  [[9134, 6486, 14238, 58, 3013, 16178, 2604, 79...    \n",
       "4  [[9134, 6486, 14238, 58, 3013, 16178, 2604, 79...    \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer3-top_mean_vals  \\\n",
       "0  [[579.41, 183.42, 158.95, 115.84, 50.69, 49.05...     \n",
       "1  [[579.41, 183.43, 158.96, 115.85, 50.67, 49.05...     \n",
       "2  [[579.44, 183.46, 158.98, 115.86, 50.67, 49.15...     \n",
       "3  [[579.41, 183.44, 158.95, 115.85, 50.67, 49.05...     \n",
       "4  [[579.42, 183.41, 158.93, 115.83, 50.67, 49.05...     \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer3-token_feature_ids  \\\n",
       "0  [9134, 6486, 14238, 895, 4303, 13975, 3132, 91...         \n",
       "1  [9134, 6486, 14238, 4436, 16028, 9134, 9134, 1...         \n",
       "2  [9134, 6486, 14238, 13144, 2475, 5806, 8594, 9...         \n",
       "3  [9134, 6486, 14238, 11917, 5806, 4303, 9432, 9...         \n",
       "4  [9134, 6486, 14238, 13699, 2007, 9134, 13144, ...         \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer4-top_mean_ids  \\\n",
       "0  [[12690, 11570, 6680, 14155, 13325, 5575, 1118...    \n",
       "1  [[12690, 11570, 6680, 14155, 13325, 5575, 1118...    \n",
       "2  [[12690, 11570, 6680, 14155, 13325, 5575, 1118...    \n",
       "3  [[12690, 11570, 6680, 14155, 13325, 5575, 1118...    \n",
       "4  [[12690, 11570, 6680, 14155, 13325, 5575, 1118...    \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer4-top_mean_vals  \\\n",
       "0  [[743.31, 95.4, 86.55, 61.71, 52.7, 48.11, 39....     \n",
       "1  [[743.17, 95.38, 86.7, 61.71, 52.69, 48.1, 39....     \n",
       "2  [[743.46, 95.41, 86.38, 61.69, 52.69, 48.11, 3...     \n",
       "3  [[743.32, 95.39, 86.54, 61.7, 52.69, 48.1, 39....     \n",
       "4  [[743.31, 95.4, 86.55, 61.71, 52.7, 48.1, 39.2...     \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer4-token_feature_ids  \\\n",
       "0  [12690, 11570, 6680, 8265, 6717, 2346, 5924, 1...         \n",
       "1  [12690, 11570, 6680, 6717, 9668, 3406, 12690, ...         \n",
       "2  [12690, 11570, 6680, 15419, 12690, 6717, 12629...         \n",
       "3  [12690, 11570, 6680, 9364, 6717, 1261, 5973, 1...         \n",
       "4  [12690, 11570, 6680, 15634, 7392, 6717, 12690,...         \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer5-top_mean_ids  \\\n",
       "0  [[1059, 8392, 5148, 1515, 13960, 10966, 9380, ...    \n",
       "1  [[1059, 8392, 5148, 1515, 13960, 10966, 9380, ...    \n",
       "2  [[1059, 8392, 5148, 1515, 13960, 10966, 9380, ...    \n",
       "3  [[1059, 8392, 5148, 1515, 13960, 10966, 9380, ...    \n",
       "4  [[1059, 8392, 5148, 1515, 13960, 10966, 9380, ...    \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer5-top_mean_vals  \\\n",
       "0  [[832.61, 225.61, 202.2, 71.07, 64.67, 64.17, ...     \n",
       "1  [[832.5, 225.58, 201.76, 71.06, 64.67, 64.16, ...     \n",
       "2  [[832.62, 225.6, 202.19, 71.06, 64.64, 64.16, ...     \n",
       "3  [[832.56, 225.58, 201.97, 71.06, 64.64, 64.15,...     \n",
       "4  [[832.55, 225.6, 201.98, 71.07, 64.66, 64.15, ...     \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer5-token_feature_ids  \\\n",
       "0  [1059, 8392, 5148, 3235, 3793, 2712, 1386, 105...         \n",
       "1  [1059, 8392, 5148, 3235, 1059, 9399, 1059, 964...         \n",
       "2  [1059, 8392, 5148, 3235, 5351, 1059, 1059, 185...         \n",
       "3  [1059, 8392, 5148, 3235, 5871, 1059, 5749, 105...         \n",
       "4  [1059, 8392, 5148, 3235, 1059, 7423, 5351, 102...         \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer6-top_mean_ids  \\\n",
       "0  [[9743, 6201, 14077, 4478, 4485, 8883, 8765, 4...    \n",
       "1  [[9743, 6201, 14077, 4478, 4485, 8883, 8765, 4...    \n",
       "2  [[9743, 6201, 14077, 4478, 4485, 8883, 8765, 4...    \n",
       "3  [[9743, 6201, 14077, 4478, 4485, 8883, 8765, 4...    \n",
       "4  [[9743, 6201, 14077, 4478, 4485, 8883, 8765, 4...    \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer6-top_mean_vals  \\\n",
       "0  [[801.94, 560.02, 114.18, 102.17, 99.94, 94.5,...     \n",
       "1  [[801.48, 560.17, 114.19, 102.15, 99.93, 94.48...     \n",
       "2  [[801.95, 560.02, 114.18, 102.17, 99.91, 94.47...     \n",
       "3  [[801.49, 560.17, 114.18, 102.15, 99.9, 94.46,...     \n",
       "4  [[801.48, 560.16, 114.19, 102.15, 99.93, 94.49...     \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer6-token_feature_ids  \\\n",
       "0  [9743, 6201, 14077, 15854, 15079, 13997, 5938,...         \n",
       "1  [9743, 6201, 14077, 15854, 8980, 14397, 9743, ...         \n",
       "2  [9743, 6201, 14077, 15854, 10273, 10367, 7121,...         \n",
       "3  [9743, 6201, 14077, 12483, 15854, 12062, 16025...         \n",
       "4  [9743, 6201, 14077, 12062, 15854, 9743, 9743, ...         \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer7-top_mean_ids  \\\n",
       "0  [[12287, 14119, 14537, 516, 13236, 13027, 6142...    \n",
       "1  [[12287, 14119, 14537, 516, 13236, 13027, 6142...    \n",
       "2  [[12287, 14119, 14537, 516, 13236, 13027, 6142...    \n",
       "3  [[12287, 14119, 14537, 516, 13236, 13027, 6142...    \n",
       "4  [[12287, 14119, 14537, 516, 13236, 13027, 6142...    \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer7-top_mean_vals  \\\n",
       "0  [[819.16, 369.21, 350.83, 185.37, 182.52, 159....     \n",
       "1  [[818.68, 369.25, 350.78, 185.39, 182.49, 159....     \n",
       "2  [[819.17, 369.21, 350.81, 185.37, 182.5, 159.4...     \n",
       "3  [[818.69, 369.27, 350.74, 185.39, 182.48, 159....     \n",
       "4  [[818.67, 369.26, 350.8, 185.39, 182.51, 159.4...     \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer7-token_feature_ids  \\\n",
       "0  [12287, 14119, 14537, 2523, 9664, 12429, 6855,...         \n",
       "1  [12287, 14119, 14537, 12287, 9664, 2523, 12287...         \n",
       "2  [12287, 14119, 14537, 12287, 2523, 9664, 589, ...         \n",
       "3  [12287, 14119, 14537, 9664, 12287, 11454, 1511...         \n",
       "4  [12287, 14119, 14537, 9664, 12287, 11774, 1228...         \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer8-top_mean_ids  \\\n",
       "0  [[9213, 4399, 6069, 3397, 13188, 12695, 6524, ...    \n",
       "1  [[9213, 4399, 6069, 3397, 13188, 12695, 6524, ...    \n",
       "2  [[9213, 4399, 6069, 3397, 13188, 12695, 6524, ...    \n",
       "3  [[9213, 4399, 6069, 3397, 13188, 12695, 6524, ...    \n",
       "4  [[9213, 4399, 6069, 3397, 13188, 12695, 6524, ...    \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer8-top_mean_vals  \\\n",
       "0  [[868.85, 240.27, 224.58, 217.49, 134.63, 127....     \n",
       "1  [[868.39, 240.36, 224.62, 217.68, 134.67, 127....     \n",
       "2  [[868.86, 240.25, 224.56, 217.49, 134.62, 127....     \n",
       "3  [[868.38, 240.24, 224.51, 217.53, 134.62, 127....     \n",
       "4  [[868.38, 240.37, 224.65, 217.68, 134.69, 127....     \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer8-token_feature_ids  \\\n",
       "0  [9213, 4399, 6069, 6291, 1592, 564, 855, 9213,...         \n",
       "1  [9213, 4399, 6069, 6291, 15069, 9213, 9213, 50...         \n",
       "2  [9213, 4399, 6069, 6291, 10304, 9213, 2592, 20...         \n",
       "3  [9213, 4399, 6069, 6291, 5235, 9213, 12661, 44...         \n",
       "4  [9213, 4399, 6069, 6291, 12062, 9213, 9213, 10...         \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer9-top_mean_ids  \\\n",
       "0  [[12102, 5877, 3736, 9892, 10966, 12253, 4212,...    \n",
       "1  [[12102, 5877, 3736, 9892, 10966, 12253, 4212,...    \n",
       "2  [[12102, 5877, 3736, 9892, 10966, 12253, 4212,...    \n",
       "3  [[12102, 5877, 3736, 9892, 10966, 12253, 4212,...    \n",
       "4  [[12102, 5877, 3736, 9892, 10966, 12253, 4212,...    \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer9-top_mean_vals  \\\n",
       "0  [[1051.21, 380.23, 202.0, 168.1, 167.91, 135.7...     \n",
       "1  [[1050.26, 380.52, 202.04, 168.18, 167.94, 135...     \n",
       "2  [[1051.21, 380.22, 201.99, 168.09, 167.9, 135....     \n",
       "3  [[1050.26, 380.28, 201.96, 168.09, 167.88, 135...     \n",
       "4  [[1050.26, 380.51, 202.07, 168.18, 167.95, 135...     \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer9-token_feature_ids  \\\n",
       "0  [12102, 5877, 3736, 381, 12102, 14717, 222, 12...         \n",
       "1  [12102, 5877, 3736, 12102, 381, 14717, 12102, ...         \n",
       "2  [12102, 5877, 3736, 12102, 381, 14717, 326, 54...         \n",
       "3  [12102, 5877, 3736, 12102, 381, 4148, 12354, 5...         \n",
       "4  [12102, 5877, 3736, 12102, 381, 4148, 12102, 1...         \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer10-top_mean_ids  \\\n",
       "0  [[4392, 2843, 3736, 2575, 5501, 6655, 9892, 57...     \n",
       "1  [[4392, 2843, 3736, 2575, 5501, 6655, 9892, 57...     \n",
       "2  [[4392, 2843, 3736, 2575, 5501, 6655, 9892, 57...     \n",
       "3  [[4392, 2843, 3736, 2575, 5501, 6655, 9892, 57...     \n",
       "4  [[4392, 2843, 3736, 2575, 5501, 6655, 9892, 57...     \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer10-top_mean_vals  \\\n",
       "0  [[1203.91, 499.68, 185.55, 177.32, 153.59, 137...      \n",
       "1  [[1202.96, 500.0, 185.55, 177.2, 153.67, 137.9...      \n",
       "2  [[1203.92, 499.68, 185.54, 177.32, 153.59, 137...      \n",
       "3  [[1202.96, 499.71, 185.49, 177.23, 153.6, 137....      \n",
       "4  [[1202.96, 500.0, 185.56, 177.21, 153.68, 137....      \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer10-token_feature_ids  \\\n",
       "0  [4392, 2843, 3736, 4392, 3736, 14372, 855, 223...          \n",
       "1  [4392, 2843, 3736, 4392, 3736, 14372, 13321, 2...          \n",
       "2  [4392, 2843, 3736, 4392, 3736, 14372, 15181, 1...          \n",
       "3  [4392, 2843, 3736, 4392, 3736, 14372, 4993, 26...          \n",
       "4  [4392, 2843, 3736, 4392, 3736, 14372, 4392, 42...          \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer11-top_mean_ids  \\\n",
       "0  [[12945, 13423, 10772, 7228, 6222, 8238, 7675,...     \n",
       "1  [[12945, 13423, 10772, 7228, 6222, 8238, 7675,...     \n",
       "2  [[12945, 13423, 10772, 7228, 6222, 8238, 7675,...     \n",
       "3  [[12945, 13423, 10772, 7228, 6222, 8238, 7675,...     \n",
       "4  [[12945, 13423, 10772, 7228, 6222, 8238, 7675,...     \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer11-top_mean_vals  \\\n",
       "0  [[1365.93, 254.19, 229.56, 216.8, 200.8, 160.8...      \n",
       "1  [[1364.97, 254.04, 229.63, 216.92, 200.95, 160...      \n",
       "2  [[1365.93, 254.19, 229.56, 216.78, 200.8, 160....      \n",
       "3  [[1364.97, 254.08, 229.52, 216.84, 200.82, 160...      \n",
       "4  [[1364.97, 254.09, 229.63, 216.95, 200.95, 160...      \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer11-token_feature_ids  \\\n",
       "0  [12945, 13423, 10772, 12945, 13423, 14717, 316...          \n",
       "1  [12945, 13423, 10772, 12945, 13423, 10772, 129...          \n",
       "2  [12945, 13423, 10772, 12945, 13423, 10772, 123...          \n",
       "3  [12945, 13423, 10772, 12945, 13423, 10772, 150...          \n",
       "4  [12945, 13423, 10772, 12945, 13423, 10772, 129...          \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer12-top_mean_ids  \\\n",
       "0  [[1041, 7507, 11087, 3220, 11767, 11752, 14669...     \n",
       "1  [[1041, 7507, 11087, 3220, 11767, 11752, 14669...     \n",
       "2  [[1041, 7507, 11087, 3220, 11767, 11752, 14669...     \n",
       "3  [[1041, 7507, 11087, 3220, 11767, 11752, 14669...     \n",
       "4  [[1041, 7507, 11087, 3220, 11767, 11752, 14669...     \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer12-top_mean_vals  \\\n",
       "0  [[1436.49, 288.36, 274.5, 177.84, 175.78, 169....      \n",
       "1  [[1435.57, 288.27, 274.5, 177.88, 175.84, 169....      \n",
       "2  [[1436.48, 288.43, 274.5, 177.83, 175.78, 169....      \n",
       "3  [[1435.57, 288.33, 274.46, 177.82, 175.78, 169...      \n",
       "4  [[1435.56, 288.28, 274.52, 177.9, 175.85, 169....      \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer12-token_feature_ids  \\\n",
       "0  [1041, 7507, 11087, 4667, 11087, 1178, 12358, ...          \n",
       "1  [1041, 7507, 11087, 4667, 11087, 1178, 2686, 4...          \n",
       "2  [1041, 7507, 11087, 4667, 11087, 1178, 14240, ...          \n",
       "3  [1041, 7507, 11087, 4667, 11087, 1178, 1724, 9...          \n",
       "4  [1041, 7507, 11087, 4667, 11087, 1178, 7507, 2...          \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer13-top_mean_ids  \\\n",
       "0  [[11248, 13423, 15319, 12322, 15392, 12879, 15...     \n",
       "1  [[11248, 13423, 15319, 12322, 15392, 12879, 15...     \n",
       "2  [[11248, 13423, 15319, 12322, 15392, 12879, 15...     \n",
       "3  [[11248, 13423, 15319, 12322, 15392, 12879, 15...     \n",
       "4  [[11248, 13423, 15319, 12322, 15392, 12879, 15...     \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer13-top_mean_vals  \\\n",
       "0  [[1599.96, 427.65, 387.72, 267.32, 151.79, 147...      \n",
       "1  [[1599.98, 427.68, 387.68, 267.37, 151.81, 147...      \n",
       "2  [[1599.96, 427.66, 387.75, 267.3, 151.79, 147....      \n",
       "3  [[1599.97, 427.63, 387.73, 267.28, 151.79, 147...      \n",
       "4  [[1599.98, 427.7, 387.69, 267.39, 151.81, 147....      \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer13-token_feature_ids  \\\n",
       "0  [11248, 13423, 15319, 13423, 11248, 13296, 635...          \n",
       "1  [11248, 13423, 15319, 13423, 11248, 13296, 621...          \n",
       "2  [11248, 13423, 15319, 13423, 11248, 13296, 206...          \n",
       "3  [11248, 13423, 15319, 13423, 11248, 14396, 104...          \n",
       "4  [11248, 13423, 15319, 13423, 11248, 4825, 1531...          \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer14-top_mean_ids  \\\n",
       "0  [[15567, 7214, 6812, 9165, 7601, 8995, 8818, 3...     \n",
       "1  [[15567, 7214, 6812, 9165, 7601, 8995, 8818, 3...     \n",
       "2  [[15567, 7214, 6812, 9165, 7601, 8995, 8818, 3...     \n",
       "3  [[15567, 7214, 6812, 9165, 7601, 8995, 8818, 3...     \n",
       "4  [[15567, 7214, 6812, 9165, 7601, 8995, 8818, 3...     \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer14-top_mean_vals  \\\n",
       "0  [[1327.58, 937.92, 207.4, 193.68, 179.55, 165....      \n",
       "1  [[1327.57, 937.92, 207.5, 193.71, 179.61, 165....      \n",
       "2  [[1327.56, 937.95, 207.39, 193.68, 179.55, 165...      \n",
       "3  [[1327.59, 937.92, 207.37, 193.67, 179.55, 165...      \n",
       "4  [[1327.56, 937.93, 207.52, 193.72, 179.61, 165...      \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer14-token_feature_ids  \\\n",
       "0  [15567, 7214, 6812, 12313, 14818, 7214, 11958,...          \n",
       "1  [15567, 7214, 6812, 12313, 14818, 16123, 12801...          \n",
       "2  [15567, 7214, 6812, 12313, 16123, 14818, 15726...          \n",
       "3  [15567, 7214, 6812, 16123, 12313, 15567, 1998,...          \n",
       "4  [15567, 7214, 6812, 16123, 15567, 12313, 7214,...          \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer15-top_mean_ids  \\\n",
       "0  [[10716, 8610, 13870, 14717, 6266, 10404, 7546...     \n",
       "1  [[10716, 8610, 13870, 14717, 6266, 10404, 7546...     \n",
       "2  [[10716, 8610, 13870, 14717, 6266, 10404, 7546...     \n",
       "3  [[10716, 8610, 13870, 14717, 6266, 10404, 7546...     \n",
       "4  [[10716, 8610, 13870, 14717, 6266, 10404, 7546...     \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer15-top_mean_vals  \\\n",
       "0  [[1806.8, 784.7, 486.79, 369.48, 286.7, 204.34...      \n",
       "1  [[1806.8, 784.64, 486.95, 369.44, 286.78, 204....      \n",
       "2  [[1806.78, 784.74, 486.8, 369.5, 286.7, 204.35...      \n",
       "3  [[1806.82, 784.65, 486.78, 369.46, 286.71, 204...      \n",
       "4  [[1806.8, 784.66, 486.97, 369.46, 286.76, 204....      \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer15-token_feature_ids  \\\n",
       "0  [10716, 8610, 13870, 10716, 14717, 8610, 1695,...          \n",
       "1  [10716, 8610, 13870, 10716, 14717, 8610, 1442,...          \n",
       "2  [10716, 8610, 13870, 10716, 14717, 8610, 11296...          \n",
       "3  [10716, 8610, 13870, 10716, 14717, 8610, 5900,...          \n",
       "4  [10716, 8610, 13870, 10716, 14717, 8610, 8610,...          \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer16-top_mean_ids  \\\n",
       "0  [[16028, 10480, 14919, 4500, 2717, 4631, 4005,...     \n",
       "1  [[16028, 10480, 14919, 4500, 2717, 4631, 4005,...     \n",
       "2  [[16028, 10480, 14919, 4500, 2717, 4631, 4005,...     \n",
       "3  [[16028, 10480, 14919, 4500, 2717, 4631, 4005,...     \n",
       "4  [[16028, 10480, 14919, 4500, 2717, 4631, 4005,...     \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer16-top_mean_vals  \\\n",
       "0  [[1901.41, 718.13, 386.08, 328.92, 205.49, 198...      \n",
       "1  [[1901.42, 718.07, 386.14, 328.87, 205.53, 198...      \n",
       "2  [[1901.39, 718.14, 386.09, 328.93, 205.49, 198...      \n",
       "3  [[1901.43, 718.07, 386.08, 328.89, 205.48, 198...      \n",
       "4  [[1901.41, 718.08, 386.13, 328.9, 205.54, 198....      \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer16-token_feature_ids  \\\n",
       "0  [16028, 10480, 14919, 4500, 16028, 9572, 5169,...          \n",
       "1  [16028, 10480, 14919, 4500, 16028, 9572, 16028...          \n",
       "2  [16028, 10480, 14919, 4500, 16028, 10480, 7168...          \n",
       "3  [16028, 10480, 14919, 4500, 16028, 15992, 7616...          \n",
       "4  [16028, 10480, 14919, 4500, 16028, 10976, 1048...          \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer17-top_mean_ids  \\\n",
       "0  [[7127, 7921, 9095, 12507, 10848, 999, 1467, 1...     \n",
       "1  [[7127, 7921, 9095, 12507, 10848, 999, 1467, 1...     \n",
       "2  [[7127, 7921, 9095, 12507, 10848, 999, 1467, 1...     \n",
       "3  [[7127, 7921, 9095, 12507, 10848, 999, 1467, 1...     \n",
       "4  [[7127, 7921, 9095, 12507, 10848, 999, 1467, 1...     \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer17-top_mean_vals  \\\n",
       "0  [[1963.47, 777.35, 402.1, 298.72, 265.57, 229....      \n",
       "1  [[1963.47, 777.35, 402.1, 298.72, 265.57, 229....      \n",
       "2  [[1963.47, 777.35, 402.1, 298.72, 265.57, 229....      \n",
       "3  [[1963.47, 777.35, 402.1, 298.72, 265.57, 229....      \n",
       "4  [[1963.47, 777.35, 402.1, 298.72, 265.57, 229....      \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer17-token_feature_ids  \\\n",
       "0  [7127, 7921, 9095, 11527, 7921, 2873, 11828, 7...          \n",
       "1  [7127, 7921, 9095, 4633, 11527, 1966, 10198, 2...          \n",
       "2  [7127, 7921, 9095, 11527, 7921, 6492, 13605, 1...          \n",
       "3  [7127, 7921, 9095, 1003, 13378, 11527, 12118, ...          \n",
       "4  [7127, 7921, 9095, 1003, 5373, 11527, 7921, 12...          \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer18-top_mean_ids  \\\n",
       "0  [[3851, 15394, 12685, 1222, 2336, 5803, 4677, ...     \n",
       "1  [[3851, 15394, 12685, 1222, 2336, 5803, 4677, ...     \n",
       "2  [[3851, 15394, 12685, 1222, 2336, 5803, 4677, ...     \n",
       "3  [[3851, 15394, 12685, 1222, 2336, 5803, 4677, ...     \n",
       "4  [[3851, 15394, 12685, 1222, 2336, 5803, 4677, ...     \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer18-top_mean_vals  \\\n",
       "0  [[2052.88, 763.03, 348.91, 271.26, 259.58, 222...      \n",
       "1  [[2052.88, 763.03, 348.91, 271.26, 259.58, 222...      \n",
       "2  [[2052.88, 763.03, 348.91, 271.26, 259.58, 222...      \n",
       "3  [[2052.88, 763.03, 348.91, 271.26, 259.58, 222...      \n",
       "4  [[2052.88, 763.03, 348.91, 271.26, 259.58, 222...      \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer18-token_feature_ids  \\\n",
       "0  [3851, 15394, 12685, 15394, 3232, 9392, 12368,...          \n",
       "1  [3851, 15394, 12685, 11706, 16336, 12187, 8799...          \n",
       "2  [3851, 15394, 12685, 11635, 7373, 8738, 1460, ...          \n",
       "3  [3851, 15394, 12685, 7373, 10793, 9948, 15388,...          \n",
       "4  [3851, 15394, 12685, 11527, 10793, 12927, 8738...          \n",
       "\n",
       "   gemma-scope-2b-pt-res-canonical-layer19-top_mean_ids  \\\n",
       "0                                                NaN      \n",
       "1                                                NaN      \n",
       "2                                                NaN      \n",
       "3                                                NaN      \n",
       "4                                                NaN      \n",
       "\n",
       "   gemma-scope-2b-pt-res-canonical-layer19-top_mean_vals  \\\n",
       "0                                                NaN       \n",
       "1                                                NaN       \n",
       "2                                                NaN       \n",
       "3                                                NaN       \n",
       "4                                                NaN       \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer19-token_feature_ids  \\\n",
       "0  [12025, 11046, 3543, 5149, 5989, 4534, 1751, 1...          \n",
       "1  [12025, 11046, 3543, 9920, 445, 1792, 517, 128...          \n",
       "2  [12025, 11046, 3543, 1494, 5149, 6788, 13509, ...          \n",
       "3  [12025, 11046, 3543, 6207, 10441, 4346, 3164, ...          \n",
       "4  [12025, 11046, 3543, 15771, 12564, 10441, 1202...          \n",
       "\n",
       "   gemma-scope-2b-pt-res-canonical-layer20-top_mean_ids  \\\n",
       "0                                                NaN      \n",
       "1                                                NaN      \n",
       "2                                                NaN      \n",
       "3                                                NaN      \n",
       "4                                                NaN      \n",
       "\n",
       "   gemma-scope-2b-pt-res-canonical-layer20-top_mean_vals  \\\n",
       "0                                                NaN       \n",
       "1                                                NaN       \n",
       "2                                                NaN       \n",
       "3                                                NaN       \n",
       "4                                                NaN       \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer20-token_feature_ids  \\\n",
       "0  [6631, 743, 5052, 14993, 3019, 7482, 10881, 30...          \n",
       "1  [6631, 743, 5052, 1792, 1116, 5956, 5958, 6376...          \n",
       "2  [6631, 743, 5052, 8191, 674, 3019, 4853, 14619...          \n",
       "3  [6631, 743, 5052, 15222, 11527, 8684, 16111, 1...          \n",
       "4  [6631, 743, 5052, 11795, 11527, 3458, 8191, 66...          \n",
       "\n",
       "   gemma-scope-2b-pt-res-canonical-layer21-top_mean_ids  \\\n",
       "0                                                NaN      \n",
       "1                                                NaN      \n",
       "2                                                NaN      \n",
       "3                                                NaN      \n",
       "4                                                NaN      \n",
       "\n",
       "   gemma-scope-2b-pt-res-canonical-layer21-top_mean_vals  \\\n",
       "0                                                NaN       \n",
       "1                                                NaN       \n",
       "2                                                NaN       \n",
       "3                                                NaN       \n",
       "4                                                NaN       \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer21-token_feature_ids  \\\n",
       "0  [4138, 13557, 15170, 4138, 14723, 7402, 12475,...          \n",
       "1  [4138, 13557, 15170, 15468, 9149, 3158, 2050, ...          \n",
       "2  [4138, 13557, 15170, 1674, 14723, 7632, 13411,...          \n",
       "3  [4138, 13557, 15170, 12579, 3461, 1003, 6438, ...          \n",
       "4  [4138, 13557, 15170, 7795, 4422, 1003, 1674, 3...          \n",
       "\n",
       "   gemma-scope-2b-pt-res-canonical-layer22-top_mean_ids  \\\n",
       "0                                                NaN      \n",
       "1                                                NaN      \n",
       "2                                                NaN      \n",
       "3                                                NaN      \n",
       "4                                                NaN      \n",
       "\n",
       "   gemma-scope-2b-pt-res-canonical-layer22-top_mean_vals  \\\n",
       "0                                                NaN       \n",
       "1                                                NaN       \n",
       "2                                                NaN       \n",
       "3                                                NaN       \n",
       "4                                                NaN       \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer22-token_feature_ids  \\\n",
       "0  [15056, 2037, 630, 15056, 14921, 15986, 4608, ...          \n",
       "1  [15056, 2037, 630, 13234, 15056, 14398, 10229,...          \n",
       "2  [15056, 2037, 630, 7392, 11901, 15916, 4101, 1...          \n",
       "3  [15056, 2037, 630, 6629, 10752, 8962, 16154, 3...          \n",
       "4  [15056, 2037, 630, 14781, 15502, 10265, 7392, ...          \n",
       "\n",
       "   gemma-scope-2b-pt-res-canonical-layer23-top_mean_ids  \\\n",
       "0                                                NaN      \n",
       "1                                                NaN      \n",
       "2                                                NaN      \n",
       "3                                                NaN      \n",
       "4                                                NaN      \n",
       "\n",
       "   gemma-scope-2b-pt-res-canonical-layer23-top_mean_vals  \\\n",
       "0                                                NaN       \n",
       "1                                                NaN       \n",
       "2                                                NaN       \n",
       "3                                                NaN       \n",
       "4                                                NaN       \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer23-token_feature_ids  \\\n",
       "0  [3956, 6961, 8321, 6961, 1346, 3956, 12475, 69...          \n",
       "1  [3956, 6961, 8321, 4225, 1923, 1650, 13915, 12...          \n",
       "2  [3956, 6961, 8321, 10217, 14332, 11796, 12829,...          \n",
       "3  [3956, 6961, 8321, 3611, 11271, 15859, 6229, 1...          \n",
       "4  [3956, 6961, 8321, 10611, 3142, 5286, 14332, 1...          \n",
       "\n",
       "   gemma-scope-2b-pt-res-canonical-layer24-top_mean_ids  \\\n",
       "0                                                NaN      \n",
       "1                                                NaN      \n",
       "2                                                NaN      \n",
       "3                                                NaN      \n",
       "4                                                NaN      \n",
       "\n",
       "   gemma-scope-2b-pt-res-canonical-layer24-top_mean_vals  \\\n",
       "0                                                NaN       \n",
       "1                                                NaN       \n",
       "2                                                NaN       \n",
       "3                                                NaN       \n",
       "4                                                NaN       \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer24-token_feature_ids  \\\n",
       "0  [1802, 14391, 14836, 3173, 501, 5305, 3848, 13...          \n",
       "1  [1802, 14391, 14836, 1778, 14870, 14753, 12565...          \n",
       "2  [1802, 14391, 14836, 5328, 14870, 9188, 2952, ...          \n",
       "3  [1802, 14391, 14836, 2566, 11326, 12205, 261, ...          \n",
       "4  [1802, 14391, 14836, 13969, 15621, 3848, 3848,...          \n",
       "\n",
       "   gemma-scope-2b-pt-res-canonical-layer25-top_mean_ids  \\\n",
       "0                                                NaN      \n",
       "1                                                NaN      \n",
       "2                                                NaN      \n",
       "3                                                NaN      \n",
       "4                                                NaN      \n",
       "\n",
       "   gemma-scope-2b-pt-res-canonical-layer25-top_mean_vals  \\\n",
       "0                                                NaN       \n",
       "1                                                NaN       \n",
       "2                                                NaN       \n",
       "3                                                NaN       \n",
       "4                                                NaN       \n",
       "\n",
       "  gemma-scope-2b-pt-res-canonical-layer25-token_feature_ids  \\\n",
       "0  [7479, 5149, 14186, 1625, 8002, 1734, 14325, 1...          \n",
       "1  [7479, 5149, 14186, 5755, 14325, 3017, 3988, 1...          \n",
       "2  [7479, 5149, 14186, 14325, 8456, 3017, 13757, ...          \n",
       "3  [7479, 5149, 14186, 1734, 16354, 14325, 13749,...          \n",
       "4  [7479, 5149, 14186, 12942, 14325, 14491, 14325...          \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer0-top_mean_ids  \\\n",
       "0                                                NaN    \n",
       "1                                                NaN    \n",
       "2  [[15745, 2628, 10077, 8629, 9147, 3088, 10698,...    \n",
       "3  [[15745, 2628, 10077, 8629, 9147, 3088, 10698,...    \n",
       "4                                                NaN    \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer0-top_mean_vals  \\\n",
       "0                                                NaN     \n",
       "1                                                NaN     \n",
       "2  [[132.27, 56.69, 44.7, 34.79, 13.75, 11.44, 11...     \n",
       "3  [[132.37, 56.71, 44.72, 34.79, 13.8, 11.45, 11...     \n",
       "4                                                NaN     \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer0-token_feature_ids  \\\n",
       "0  [15745, 2628, 10077, 9038, 16026, 15746, 1213,...         \n",
       "1  [15745, 2628, 10077, 8617, 10445, 9038, 12046,...         \n",
       "2  [15745, 2628, 10077, 9038, 1024, 7694, 1024, 1...         \n",
       "3  [15745, 2628, 10077, 9038, 4965, 1024, 15745, ...         \n",
       "4  [15745, 2628, 10077, 9038, 3638, 15022, 1024, ...         \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer1-token_feature_ids  \\\n",
       "0  [7101, 890, 11950, 12057, 11926, 13552, 12057,...         \n",
       "1  [7101, 890, 11950, 12057, 11926, 3738, 13568, ...         \n",
       "2  [7101, 890, 11950, 12057, 11141, 11926, 16307,...         \n",
       "3  [7101, 890, 11950, 12057, 322, 4528, 6329, 109...         \n",
       "4  [7101, 890, 11950, 12057, 9929, 322, 11141, 12...         \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer2-token_feature_ids  \\\n",
       "0  [5817, 4198, 2493, 2493, 5817, 15465, 5817, 62...         \n",
       "1  [5817, 4198, 2493, 10322, 5675, 13332, 13276, ...         \n",
       "2  [5817, 4198, 2493, 5817, 13276, 11575, 633, 11...         \n",
       "3  [5817, 4198, 2493, 13276, 12676, 11924, 2115, ...         \n",
       "4  [5817, 4198, 2493, 16166, 11249, 3303, 5817, 1...         \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer3-token_feature_ids  \\\n",
       "0  [4730, 6258, 5254, 3710, 4730, 4185, 4730, 587...         \n",
       "1  [4730, 6258, 5254, 3710, 4730, 13224, 12538, 1...         \n",
       "2  [4730, 6258, 5254, 4185, 4730, 3710, 8338, 473...         \n",
       "3  [4730, 6258, 5254, 3710, 4185, 7851, 6819, 100...         \n",
       "4  [4730, 6258, 5254, 4730, 4185, 2420, 4730, 596...         \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer4-token_feature_ids  \\\n",
       "0  [12597, 10159, 14164, 12597, 2793, 14164, 2793...         \n",
       "1  [12597, 10159, 14164, 2793, 14164, 12597, 2793...         \n",
       "2  [12597, 10159, 14164, 12597, 2793, 14768, 2793...         \n",
       "3  [12597, 10159, 14164, 12597, 2793, 14164, 990,...         \n",
       "4  [12597, 10159, 14164, 12597, 2793, 14164, 1259...         \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer5-token_feature_ids  \\\n",
       "0  [15592, 12904, 4700, 686, 15592, 5758, 15592, ...         \n",
       "1  [15592, 12904, 4700, 686, 15592, 4640, 15592, ...         \n",
       "2  [15592, 12904, 4700, 686, 15592, 5758, 15592, ...         \n",
       "3  [15592, 12904, 4700, 686, 15592, 4542, 15592, ...         \n",
       "4  [15592, 12904, 4700, 686, 15592, 11145, 15592,...         \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer6-token_feature_ids  \\\n",
       "0  [7589, 6941, 7590, 8424, 9583, 440, 7589, 1125...         \n",
       "1  [7589, 6941, 7590, 10287, 7589, 3783, 6941, 74...         \n",
       "2  [7589, 6941, 7590, 7589, 6941, 4682, 11752, 14...         \n",
       "3  [7589, 6941, 7590, 5371, 5046, 816, 7589, 816,...         \n",
       "4  [7589, 6941, 7590, 178, 6941, 5371, 7589, 6941...         \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer7-token_feature_ids  \\\n",
       "0  [12096, 11676, 7040, 12096, 12542, 9778, 12096...         \n",
       "1  [12096, 11676, 7040, 12096, 9778, 6117, 12096,...         \n",
       "2  [12096, 11676, 7040, 12096, 6117, 12542, 12096...         \n",
       "3  [12096, 11676, 7040, 12096, 9778, 4053, 12096,...         \n",
       "4  [12096, 11676, 7040, 12096, 9778, 4053, 12096,...         \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer8-token_feature_ids  \\\n",
       "0  [1367, 10938, 5451, 1367, 10312, 13, 1367, 155...         \n",
       "1  [1367, 10938, 5451, 8248, 14653, 1367, 1367, 9...         \n",
       "2  [1367, 10938, 5451, 1367, 13, 10312, 1367, 142...         \n",
       "3  [1367, 10938, 5451, 8286, 9743, 7282, 4968, 76...         \n",
       "4  [1367, 10938, 5451, 8286, 9743, 7282, 1367, 19...         \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer9-token_feature_ids  \\\n",
       "0  [11525, 11530, 2788, 11525, 2788, 7622, 11525,...         \n",
       "1  [11525, 11530, 2788, 11525, 2788, 7622, 11525,...         \n",
       "2  [11525, 11530, 2788, 11525, 2788, 7622, 11525,...         \n",
       "3  [11525, 11530, 2788, 11525, 2788, 7622, 11525,...         \n",
       "4  [11525, 11530, 2788, 11525, 2788, 7622, 11525,...         \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer10-token_feature_ids  \\\n",
       "0  [7495, 3117, 6381, 7495, 8202, 5376, 3971, 906...          \n",
       "1  [7495, 3117, 6381, 7495, 8202, 5376, 7495, 397...          \n",
       "2  [7495, 3117, 6381, 7495, 8202, 5376, 3966, 397...          \n",
       "3  [7495, 3117, 6381, 7495, 8202, 5376, 7495, 258...          \n",
       "4  [7495, 3117, 6381, 7495, 8202, 13140, 7495, 39...          \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer11-token_feature_ids  \\\n",
       "0  [6316, 3458, 15102, 6316, 3318, 6910, 6316, 54...          \n",
       "1  [6316, 3458, 15102, 6316, 3318, 6910, 6316, 54...          \n",
       "2  [6316, 3458, 15102, 6316, 3318, 6910, 6316, 54...          \n",
       "3  [6316, 3458, 15102, 6316, 3318, 3458, 6316, 56...          \n",
       "4  [6316, 3458, 15102, 6316, 3318, 3458, 6316, 54...          \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer12-token_feature_ids  \\\n",
       "0  [9338, 2350, 11394, 2636, 8191, 10598, 1486, 5...          \n",
       "1  [9338, 2350, 11394, 2636, 8191, 2613, 5793, 58...          \n",
       "2  [9338, 2350, 11394, 2636, 8191, 2613, 5793, 93...          \n",
       "3  [9338, 2350, 11394, 2636, 8191, 9338, 9338, 89...          \n",
       "4  [9338, 2350, 11394, 2636, 9338, 8191, 5793, 15...          \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer13-token_feature_ids  \\\n",
       "0  [13617, 4166, 16213, 249, 3869, 7095, 13617, 8...          \n",
       "1  [13617, 4166, 16213, 249, 3869, 4290, 15550, 2...          \n",
       "2  [13617, 4166, 16213, 249, 3869, 4290, 13617, 7...          \n",
       "3  [13617, 4166, 16213, 249, 3869, 7095, 249, 133...          \n",
       "4  [13617, 4166, 16213, 249, 3869, 7095, 13617, 1...          \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer14-token_feature_ids  \\\n",
       "0  [14672, 2673, 3001, 8681, 7488, 13451, 14672, ...          \n",
       "1  [14672, 2673, 3001, 8681, 7488, 13451, 14672, ...          \n",
       "2  [14672, 2673, 3001, 8681, 7488, 2564, 13946, 9...          \n",
       "3  [14672, 2673, 3001, 8681, 2564, 13451, 6311, 1...          \n",
       "4  [14672, 2673, 3001, 8681, 2564, 9646, 1540, 11...          \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer15-token_feature_ids  \\\n",
       "0  [75, 12355, 8833, 75, 14877, 13234, 75, 13782,...          \n",
       "1  [75, 12355, 8833, 75, 13234, 14877, 5721, 75, ...          \n",
       "2  [75, 12355, 8833, 75, 14877, 3670, 75, 13978, ...          \n",
       "3  [75, 12355, 8833, 75, 8833, 14877, 11232, 75, ...          \n",
       "4  [75, 12355, 8833, 75, 8833, 6575, 75, 3879, 66...          \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer16-token_feature_ids  \\\n",
       "0  [9218, 7586, 6883, 11167, 11377, 2604, 9218, 7...          \n",
       "1  [9218, 7586, 6883, 11167, 11377, 3146, 9218, 7...          \n",
       "2  [9218, 7586, 6883, 11167, 2604, 11377, 9218, 9...          \n",
       "3  [9218, 7586, 6883, 11167, 2604, 2791, 9218, 22...          \n",
       "4  [9218, 7586, 6883, 11167, 2604, 3467, 9218, 53...          \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer17-token_feature_ids  \\\n",
       "0  [5845, 11167, 1359, 2506, 1348, 1600, 5845, 81...          \n",
       "1  [5845, 11167, 1359, 2506, 1348, 1600, 2505, 10...          \n",
       "2  [5845, 11167, 1359, 2506, 1600, 1348, 3265, 71...          \n",
       "3  [5845, 11167, 1359, 2506, 1600, 13466, 5845, 1...          \n",
       "4  [5845, 11167, 1359, 2506, 1600, 13466, 310, 78...          \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer18-token_feature_ids  \\\n",
       "0  [77, 7228, 16359, 6022, 3244, 8081, 77, 8541, ...          \n",
       "1  [77, 7228, 16359, 5608, 4214, 9163, 5140, 3886...          \n",
       "2  [77, 7228, 16359, 199, 15323, 3757, 77, 15340,...          \n",
       "3  [77, 7228, 16359, 11129, 6192, 8081, 1344, 77,...          \n",
       "4  [77, 7228, 16359, 10631, 6192, 3675, 15323, 19...          \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer19-token_feature_ids  \\\n",
       "0  [9071, 3170, 11447, 11447, 4032, 5063, 11447, ...          \n",
       "1  [9071, 3170, 11447, 14584, 10987, 11447, 3877,...          \n",
       "2  [9071, 3170, 11447, 11447, 5324, 14557, 4675, ...          \n",
       "3  [9071, 3170, 11447, 3170, 3763, 11413, 10080, ...          \n",
       "4  [9071, 3170, 11447, 1883, 8193, 4103, 5324, 11...          \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer20-token_feature_ids  \\\n",
       "0  [8468, 14729, 12522, 88, 13882, 8501, 15663, 8...          \n",
       "1  [8468, 14729, 12522, 4366, 16382, 2247, 6445, ...          \n",
       "2  [8468, 14729, 12522, 13882, 9232, 6443, 10306,...          \n",
       "3  [8468, 14729, 12522, 13882, 14729, 13198, 1486...          \n",
       "4  [8468, 14729, 12522, 12449, 14505, 4569, 8703,...          \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer21-token_feature_ids  \\\n",
       "0  [10730, 13775, 6168, 13966, 10103, 11567, 1124...          \n",
       "1  [10730, 13775, 6168, 13966, 238, 15592, 8977, ...          \n",
       "2  [10730, 13775, 6168, 1527, 12069, 8048, 14050,...          \n",
       "3  [10730, 13775, 6168, 6634, 14648, 15701, 13966...          \n",
       "4  [10730, 13775, 6168, 12508, 14050, 14734, 1527...          \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer22-token_feature_ids  \\\n",
       "0  [4723, 2531, 12567, 4723, 6259, 7769, 4723, 54...          \n",
       "1  [4723, 2531, 12567, 4723, 12222, 15987, 1764, ...          \n",
       "2  [4723, 2531, 12567, 4723, 11010, 9108, 1172, 1...          \n",
       "3  [4723, 2531, 12567, 11983, 16196, 10945, 4723,...          \n",
       "4  [4723, 2531, 12567, 4029, 281, 2833, 1255, 472...          \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer23-token_feature_ids  \\\n",
       "0  [3567, 14957, 0, 10184, 15497, 15829, 15829, 1...          \n",
       "1  [3567, 14957, 0, 10184, 7987, 24, 15497, 14757...          \n",
       "2  [3567, 14957, 0, 10184, 15497, 1926, 10184, 16...          \n",
       "3  [3567, 14957, 0, 10184, 2923, 15497, 10184, 24...          \n",
       "4  [3567, 14957, 0, 9173, 15497, 10184, 10184, 19...          \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer24-token_feature_ids  \\\n",
       "0  [16058, 282, 102, 6437, 9329, 282, 9329, 282, ...          \n",
       "1  [16058, 282, 102, 9329, 282, 6087, 9329, 282, ...          \n",
       "2  [16058, 282, 102, 282, 9329, 14105, 282, 9329,...          \n",
       "3  [16058, 282, 102, 282, 9329, 1990, 9329, 282, ...          \n",
       "4  [16058, 282, 102, 282, 9329, 11490, 282, 9329,...          \n",
       "\n",
       "  gemma-scope-2b-pt-mlp-canonical-layer25-token_feature_ids  \\\n",
       "0  [15890, 12642, 10593, 8364, 11862, 4589, 4589,...          \n",
       "1  [15890, 12642, 10593, 4589, 11648, 5433, 13264...          \n",
       "2  [15890, 12642, 10593, 4589, 13264, 7327, 13264...          \n",
       "3  [15890, 12642, 10593, 8171, 2996, 8364, 5799, ...          \n",
       "4  [15890, 12642, 10593, 14401, 13264, 13383, 458...          \n",
       "\n",
       "  gemma-scope-2b-pt-att-canonical-layer0-token_feature_ids  \n",
       "0  [3880, 1421, 1608, 3880, 11654, 1608, 15477, 1...        \n",
       "1  [3880, 1421, 1608, 11654, 9944, 3880, 9944, 65...        \n",
       "2  [3880, 1421, 1608, 11654, 3880, 9944, 9944, 79...        \n",
       "3  [3880, 1421, 1608, 3880, 9944, 1608, 10825, 26...        \n",
       "4  [3880, 1421, 1608, 3880, 1421, 1608, 9944, 388...        "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e0ce05f-2755-4a82-ba43-4922ecc09dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer19-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer19-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer20-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer20-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer21-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer21-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer22-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer22-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer23-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer23-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer24-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer24-top_mean_vals</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer25-top_mean_ids</th>\n",
       "      <th>gemma-scope-2b-pt-res-canonical-layer25-top_mean_vals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       gemma-scope-2b-pt-res-canonical-layer19-top_mean_ids  \\\n",
       "count                                                0.0      \n",
       "mean                                                 NaN      \n",
       "std                                                  NaN      \n",
       "min                                                  NaN      \n",
       "25%                                                  NaN      \n",
       "50%                                                  NaN      \n",
       "75%                                                  NaN      \n",
       "max                                                  NaN      \n",
       "\n",
       "       gemma-scope-2b-pt-res-canonical-layer19-top_mean_vals  \\\n",
       "count                                                0.0       \n",
       "mean                                                 NaN       \n",
       "std                                                  NaN       \n",
       "min                                                  NaN       \n",
       "25%                                                  NaN       \n",
       "50%                                                  NaN       \n",
       "75%                                                  NaN       \n",
       "max                                                  NaN       \n",
       "\n",
       "       gemma-scope-2b-pt-res-canonical-layer20-top_mean_ids  \\\n",
       "count                                                0.0      \n",
       "mean                                                 NaN      \n",
       "std                                                  NaN      \n",
       "min                                                  NaN      \n",
       "25%                                                  NaN      \n",
       "50%                                                  NaN      \n",
       "75%                                                  NaN      \n",
       "max                                                  NaN      \n",
       "\n",
       "       gemma-scope-2b-pt-res-canonical-layer20-top_mean_vals  \\\n",
       "count                                                0.0       \n",
       "mean                                                 NaN       \n",
       "std                                                  NaN       \n",
       "min                                                  NaN       \n",
       "25%                                                  NaN       \n",
       "50%                                                  NaN       \n",
       "75%                                                  NaN       \n",
       "max                                                  NaN       \n",
       "\n",
       "       gemma-scope-2b-pt-res-canonical-layer21-top_mean_ids  \\\n",
       "count                                                0.0      \n",
       "mean                                                 NaN      \n",
       "std                                                  NaN      \n",
       "min                                                  NaN      \n",
       "25%                                                  NaN      \n",
       "50%                                                  NaN      \n",
       "75%                                                  NaN      \n",
       "max                                                  NaN      \n",
       "\n",
       "       gemma-scope-2b-pt-res-canonical-layer21-top_mean_vals  \\\n",
       "count                                                0.0       \n",
       "mean                                                 NaN       \n",
       "std                                                  NaN       \n",
       "min                                                  NaN       \n",
       "25%                                                  NaN       \n",
       "50%                                                  NaN       \n",
       "75%                                                  NaN       \n",
       "max                                                  NaN       \n",
       "\n",
       "       gemma-scope-2b-pt-res-canonical-layer22-top_mean_ids  \\\n",
       "count                                                0.0      \n",
       "mean                                                 NaN      \n",
       "std                                                  NaN      \n",
       "min                                                  NaN      \n",
       "25%                                                  NaN      \n",
       "50%                                                  NaN      \n",
       "75%                                                  NaN      \n",
       "max                                                  NaN      \n",
       "\n",
       "       gemma-scope-2b-pt-res-canonical-layer22-top_mean_vals  \\\n",
       "count                                                0.0       \n",
       "mean                                                 NaN       \n",
       "std                                                  NaN       \n",
       "min                                                  NaN       \n",
       "25%                                                  NaN       \n",
       "50%                                                  NaN       \n",
       "75%                                                  NaN       \n",
       "max                                                  NaN       \n",
       "\n",
       "       gemma-scope-2b-pt-res-canonical-layer23-top_mean_ids  \\\n",
       "count                                                0.0      \n",
       "mean                                                 NaN      \n",
       "std                                                  NaN      \n",
       "min                                                  NaN      \n",
       "25%                                                  NaN      \n",
       "50%                                                  NaN      \n",
       "75%                                                  NaN      \n",
       "max                                                  NaN      \n",
       "\n",
       "       gemma-scope-2b-pt-res-canonical-layer23-top_mean_vals  \\\n",
       "count                                                0.0       \n",
       "mean                                                 NaN       \n",
       "std                                                  NaN       \n",
       "min                                                  NaN       \n",
       "25%                                                  NaN       \n",
       "50%                                                  NaN       \n",
       "75%                                                  NaN       \n",
       "max                                                  NaN       \n",
       "\n",
       "       gemma-scope-2b-pt-res-canonical-layer24-top_mean_ids  \\\n",
       "count                                                0.0      \n",
       "mean                                                 NaN      \n",
       "std                                                  NaN      \n",
       "min                                                  NaN      \n",
       "25%                                                  NaN      \n",
       "50%                                                  NaN      \n",
       "75%                                                  NaN      \n",
       "max                                                  NaN      \n",
       "\n",
       "       gemma-scope-2b-pt-res-canonical-layer24-top_mean_vals  \\\n",
       "count                                                0.0       \n",
       "mean                                                 NaN       \n",
       "std                                                  NaN       \n",
       "min                                                  NaN       \n",
       "25%                                                  NaN       \n",
       "50%                                                  NaN       \n",
       "75%                                                  NaN       \n",
       "max                                                  NaN       \n",
       "\n",
       "       gemma-scope-2b-pt-res-canonical-layer25-top_mean_ids  \\\n",
       "count                                                0.0      \n",
       "mean                                                 NaN      \n",
       "std                                                  NaN      \n",
       "min                                                  NaN      \n",
       "25%                                                  NaN      \n",
       "50%                                                  NaN      \n",
       "75%                                                  NaN      \n",
       "max                                                  NaN      \n",
       "\n",
       "       gemma-scope-2b-pt-res-canonical-layer25-top_mean_vals  \n",
       "count                                                0.0      \n",
       "mean                                                 NaN      \n",
       "std                                                  NaN      \n",
       "min                                                  NaN      \n",
       "25%                                                  NaN      \n",
       "50%                                                  NaN      \n",
       "75%                                                  NaN      \n",
       "max                                                  NaN      "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11ee2181-26d0-49a9-b545-427bfec98985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Columns: 110 entries, type to gemma-scope-2b-pt-att-canonical-layer0-token_feature_ids\n",
      "dtypes: float64(14), object(96)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ec6e072-93ef-421b-ad15-ed36e1b502b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hard', 'easy', 'external', 'harder', 'hardest', 'medium',\n",
       "       'codeforces'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['type']=='programming', 'category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02461384-2c04-4175-9685-0add3a33a5f9",
   "metadata": {},
   "source": [
    "# 2. Model + SAE Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "960238c1-84eb-4a2c-9530-9750a02fe5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect() # Python's garbage collector\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60b0dff1-493e-45de-bfdd-abd6eac2698b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/volumes/hpc_data_prj/inf_narrative_msc/e8efa787-4d41-448d-a7aa-814b8f0fac1e/k24086575/jvenv/lib/python3.11/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e250641f6fbf42d8a0e36f7fa585dc38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n",
      "HOOKPOINT:  blocks.25.attn.hook_z\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import SAE\n",
    "import transformer_lens\n",
    "\n",
    "def load_model_and_sae(model_name, release, sae_id, device=\"cuda\"):\n",
    "    # Load model\n",
    "    model = transformer_lens.HookedTransformer.from_pretrained(\n",
    "                                                                model_name, \n",
    "                                                                device=device,\n",
    "                                                                # device_map=\"auto\",          # if using multiple gpu will be distributed\n",
    "                                                                # torch_dtype=torch.float16   # half precision? \n",
    "                                                                # load_in_4bit=True         # 4 bit? \n",
    "    )\n",
    "    # Load SAE for the given hook point\n",
    "    sae = SAE.from_pretrained(release, sae_id)[0]\n",
    "    sae = sae.to(device)\n",
    "    # hook \n",
    "    hook_point = sae.cfg.hook_name\n",
    "\n",
    "    # TODO: multi gpu loading for bigger models \n",
    "    # # ============================================================\n",
    "    # # 2. Load the SAE to the CPU first to determine the hook point.\n",
    "    # sae = SAE.from_pretrained(release, sae_id, device=\"cpu\")[0]\n",
    "    # hook_point = sae.cfg.hook_name\n",
    "    # print(f\"SAE hook point identified: {hook_point}\")\n",
    "\n",
    "    # # 3. Find the actual device of that hook point in the distributed model.\n",
    "    # #    The hook point name corresponds to a module in the model.\n",
    "    # hook_device = model.mod_dict[hook_point].device\n",
    "    # print(f\"Hook point is on device: {hook_device}\")\n",
    "\n",
    "    # # 4. Move the SAE to that specific device.\n",
    "    # sae = sae.to(hook_device)\n",
    "    # print(f\"SAE moved to {sae.device}.\")\n",
    "    # # ============================================================\n",
    "    return model, sae, hook_point\n",
    "\n",
    "\n",
    "# GEMMA 2 2B\n",
    "SAE_TYPE = \"att\"    # res/ mlp/ att\n",
    "MODEL_NAME = \"gemma-2-2b\"\n",
    "RELEASE = f\"gemma-scope-2b-pt-{SAE_TYPE}-canonical\"\n",
    "LAYER = 25\n",
    "SAE_ID = f\"layer_{LAYER}/width_16k/canonical\"\n",
    "model, sae, hook_point = load_model_and_sae(model_name=MODEL_NAME, release=RELEASE, sae_id=SAE_ID, device=\"cuda\")\n",
    "# print(\"MODEL: \", model)\n",
    "# print(\"SAE: \", sae)\n",
    "print(\"HOOKPOINT: \", hook_point)\n",
    "\n",
    "\n",
    "# DEEPSEEK\n",
    "# deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
    "# from sae_lens import SAE\n",
    "# release = \"deepseek-r1-distill-llama-8b-qresearch\"\n",
    "# sae_id = \"blocks.19.hook_resid_post\"\n",
    "# sae = SAE.from_pretrained(release, sae_id)[0]\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ffec91a-5deb-451a-97fd-775cd4127ace",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://neuronpedia.org/quick-list/?name=%0A87%09Let%E2%80%99s%20remember%20the%20problem%20Mountain%20holidays%20%28MOU1H%29.%20The%20story%20of%20Mountain%20holidays%20is%20the%20following.%20Some%20time%20ago%2C%20Chef%20discovered%20that%20more%20and%20more%20people%20have%20started%20climbing%20mountains%20every%20day.%20So%20he%20decided%20to%20build%20a%20restaurant%20in%20the%20Ukrainian%20resort%20Bukovel%20%28Carpathian%20Mountains%29.%20But%20there%20are%20many%20places%20to%20choose%2C%20so%20he%20wants%20to%20choose%20the%20best%20one.%0ABy%20your%20great%20helps%2C%20Chef%20could%20find%20to%20the%20best%20place%20to%20build%20a%20restaurant%2C%20now%20Chef%20becomes%20rich.%20Now%20he%20wants%20to%20build%20a%20restaurant%20in%20the%20next%20mountain.%20Similar%20to%20previous%20one%2C%20the%20next%20mountain%20is%20described%20by%20a%20sequence%20of%20N%20points.%20Here%20the%20points%20are%20numbered%20from%201%20to%20N%2C%20and%20the%20height%20of%20point%20k%20is%20denoted%20by%20Hk.%20Every%20two%20adjacent%20points%20of%20the%20mountain%20are%20connected%20by%20a%20segment.%0A%0A%0AFor%20example%2C%20the%20mountain%20described%20by%20N%20%3D%209%2C%20H%20%3D%20%5B0%2C%202%2C%201%2C%202%2C%201%2C%203%2C%200%2C%201%2C%200%5D%20%20is%20like%20a%20following%3A%0A%0A%0A%20%20%20%20%20%20%20/%20%20/%5C/%5C/%20%20%20/%20%20%20%20%20%20%20%20%5C/%0A%20In%20the%20mountain%2C%20all%20of%20the%20tourists%20will%20go%20from%20the%20point%201%20to%20point%20N.%0A%0A%0AFor%20comfort%20of%20the%20tourists%2C%20Chef%20has%20bought%20teleports%20too.%20Using%20a%20teleport%2C%20a%20tourist%20can%20be%20transferred%20from%20point%20i%20to%20point%20j%2C%20for%20all%20i%20%3C%20j.%20Of%20course%2C%20tourists%20can%20also%20walk%20from%20point%20i%20to%20point%20i%2B1%20on%20foot.%0A%0A%0ANow%20Chef%20%20wants%20to%20calculate%20the%20attractiveness%20of%20the%20mountain%2C%20as%20the%20number%20of%20the%20different%20climbs.%20%0AA%20climb%20is%20defined%20by%20the%20nonempty%20sequence%20%28p1%2C%20p1%2B1%29%2C%20%28p2%2C%20p2%2B1%29%2C%20...%2C%20%28ps%2C%20ps%2B1%29%20of%20the%20moves%20%20on%20foot%20%2C%20where%20pk%2B1%20%E2%89%A4%20pk%2B1%20for%20k%20%3D%201%2C%202%2C%20...%2C%20s%20%E2%88%92%201.%0A%0ATwo%20climbs%2C%20say%20%28p1%2C%20p1%2B1%29%2C%20%28p2%2C%20p2%2B1%29%2C%20...%2C%20%28ps%2C%20ps%2B1%29%20and%20%28q1%2C%20q1%2B1%29%2C%20%28q2%2C%20q2%2B1%29%2C%20...%2C%20%28qt%2C%20qt%2B1%29%20are%20different%20if%20and%20only%20if%0A%0As%20%E2%89%A0%20t%20or%0AThere%20exists%20at%20least%20one%20k%20such%20that%201%20%E2%89%A4%20k%20%3C%20min%28s%2C%20t%29%20and%20Hpk%2B1%20%E2%80%93%20Hpk%20%E2%89%A0%20Hqk%2B1%20%E2%80%93%20Hqk.%0A%0AYou%20are%20given%20the%20array%20H%2C%20find%20the%20number%20of%20the%20different%20climbs%20that%20exist%20on%20the%20mountain.%20Since%20the%20answer%20can%20be%20very%20large%2C%20output%20it%20modulo%2010%5E9%2B9%20%3D%201000000009.%0A%0AInput%0AThe%20first%20line%20of%20input%20contains%20T%2C%20denoting%20the%20number%20of%20test%20cases.%20Then%20T%20test%20cases%20follow.%0AThe%20first%20line%20of%20each%20test%20case%20contains%20an%20integer%20N%2C%20denoting%20the%20number%20of%20points%20on%20the%20mountain.%0AOn%20second%20line%20of%20each%20test%20case%2C%20there%20are%20N%20space-separated%20integers%20H1%2C%20H2%2C%20...%2C%20HN%2C%20denoting%20the%20height%20of%20each%20point.%0A%0AOutput%0AFor%20each%20test%20case%2C%20output%20an%20integer%20denoting%20the%20number%20of%20the%20different%20climbs%20modulo%2010%5E9%2B9%20%3D%201000000009.%0A%0AConstraints%0A%0A1%20%E2%89%A4%20T%20%E2%89%A4%20100000%2C%20that%20is%2C%201%20%E2%89%A4%20T%20%E2%89%A4%2010%5E5%0A2%20%E2%89%A4%20N%20%E2%89%A4%201000000%2C%20that%20is%2C%202%20%E2%89%A4%20N%20%E2%89%A4%2010%5E6%0ASum%20of%20N%20over%20all%20test%20cases%20in%20one%20file%20will%20be%20at%20most%201000000%20%3D%2010%5E6%0A-2000000%20%E2%89%A4%20Hk%20%E2%89%A4%202000000%2C%20that%20is%2C%20%E2%88%922%20%C3%97%2010%5E6%20%E2%89%A4%20Hk%20%E2%89%A4%202%20%C3%97%2010%5E6%0A%0A&features=%5B%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%225364%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%2211788%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%229449%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%223919%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%229449%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%2212637%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%226498%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%2215678%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%227887%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%223883%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%229503%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%22165%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%225233%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%229656%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%2215063%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%224337%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%2215678%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%229449%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%228053%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%225513%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%2216131%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%22251%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%2215678%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%227887%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%222341%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%2215234%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%2215678%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%221527%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%227887%22%7D%2C%20%7B%22modelId%22%3A%20%22gemma-2-2b%22%2C%20%22layer%22%3A%20%2225-gemmascope-att-16k%22%2C%20%22index%22%3A%20%229592%22%7D%5D\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
    "\n",
    "def analyze_prompt_with_sae(prompt, model, sae, hook_point, topk_mean=10, topk_token=3):\n",
    "    with torch.no_grad():   # dont save info for backp\n",
    "        hook_point = sae.cfg.hook_name\n",
    "        # Run model with cache\n",
    "        _, cache = model.run_with_cache(prompt, prepend_bos=True)\n",
    "    \n",
    "        # Encode activations from residuals using SAE\n",
    "        residual_activations = cache[hook_point] \n",
    "        feature_acts = sae.encode(residual_activations)  # output : [batch_size, sequence_length, d_sae]\n",
    "    \n",
    "        # l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
    "\n",
    "        # Get top-k features by mean activation across sequence, for each separate token \n",
    "        # Using dim=0 gives you the top features for each separate token.\n",
    "        # Using dim=1 gives you the single set of top features based on their average strength across the entire sentence.\n",
    "        mean_acts = feature_acts.mean(dim=0)  \n",
    "        top_vals, top_indices = torch.topk(mean_acts, topk_mean)\n",
    "        top_mean_vals = top_vals.tolist()\n",
    "        top_mean_ids = top_indices.tolist()\n",
    "    \n",
    "        # Get top-k features for each token (flattened)\n",
    "        top_token_feats = torch.topk(feature_acts.squeeze(), topk_token).indices  # shape: [seq_len, topk]\n",
    "        token_feature_ids = torch.flatten(top_token_feats).tolist()\n",
    "    \n",
    "        # reduce memory from flaot to int \n",
    "        for i in range(len(top_mean_vals)):\n",
    "            for j in range(len(top_mean_vals[i])):\n",
    "                top_mean_vals[i][j] = round(top_mean_vals[i][j], 2) \n",
    "\n",
    "        del cache, residual_activations, feature_acts\n",
    "        gc.collect() # Python's garbage collector\n",
    "        torch.cuda.empty_cache() # PyTorch's cache cleaner\n",
    "\n",
    "        return top_mean_ids, top_mean_vals, token_feature_ids\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "87\tLets remember the problem Mountain holidays (MOU1H). The story of Mountain holidays is the following. Some time ago, Chef discovered that more and more people have started climbing mountains every day. So he decided to build a restaurant in the Ukrainian resort Bukovel (Carpathian Mountains). But there are many places to choose, so he wants to choose the best one.\n",
    "By your great helps, Chef could find to the best place to build a restaurant, now Chef becomes rich. Now he wants to build a restaurant in the next mountain. Similar to previous one, the next mountain is described by a sequence of N points. Here the points are numbered from 1 to N, and the height of point k is denoted by Hk. Every two adjacent points of the mountain are connected by a segment.\n",
    "\n",
    "\n",
    "For example, the mountain described by N = 9, H = [0, 2, 1, 2, 1, 3, 0, 1, 0]  is like a following:\n",
    "\n",
    "\n",
    "       /\\\n",
    "  /\\/\\/  \\\n",
    " /        \\/\\\n",
    "\n",
    " In the mountain, all of the tourists will go from the point 1 to point N.\n",
    "\n",
    "\n",
    "For comfort of the tourists, Chef has bought teleports too. Using a teleport, a tourist can be transferred from point i to point j, for all i < j. Of course, tourists can also walk from point i to point i+1 on foot.\n",
    "\n",
    "\n",
    "Now Chef  wants to calculate the attractiveness of the mountain, as the number of the different climbs. \n",
    "A climb is defined by the nonempty sequence (p1, p1+1), (p2, p2+1), ..., (ps, ps+1) of the moves  on foot , where pk+1  pk+1 for k = 1, 2, ..., s  1.\n",
    "\n",
    "Two climbs, say (p1, p1+1), (p2, p2+1), ..., (ps, ps+1) and (q1, q1+1), (q2, q2+1), ..., (qt, qt+1) are different if and only if\n",
    "\n",
    "s  t or\n",
    "There exists at least one k such that 1  k < min(s, t) and Hpk+1  Hpk  Hqk+1  Hqk.\n",
    "\n",
    "You are given the array H, find the number of the different climbs that exist on the mountain. Since the answer can be very large, output it modulo 10^9+9 = 1000000009.\n",
    "\n",
    "Input\n",
    "The first line of input contains T, denoting the number of test cases. Then T test cases follow.\n",
    "The first line of each test case contains an integer N, denoting the number of points on the mountain.\n",
    "On second line of each test case, there are N space-separated integers H1, H2, ..., HN, denoting the height of each point.\n",
    "\n",
    "Output\n",
    "For each test case, output an integer denoting the number of the different climbs modulo 10^9+9 = 1000000009.\n",
    "\n",
    "Constraints\n",
    "\n",
    "1  T  100000, that is, 1  T  10^5\n",
    "2  N  1000000, that is, 2  N  10^6\n",
    "Sum of N over all test cases in one file will be at most 1000000 = 10^6\n",
    "-2000000  Hk  2000000, that is, 2  10^6  Hk  2  10^6\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "top_mean_ids, top_mean_vals, token_feature_ids = analyze_prompt_with_sae(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    hook_point=hook_point,\n",
    "    topk_mean=10,\n",
    "    topk_token=3,\n",
    ")\n",
    "\n",
    "# # print(prompt)\n",
    "# # print()\n",
    "# # print(\"Top Activated Feature IDs:\", top_mean_ids)\n",
    "# # print()\n",
    "# # print(\"Top Mean Activation Values:\", top_mean_vals)\n",
    "# # print()\n",
    "# # print(\"All Token-wise Feature IDs:\", token_feature_ids)\n",
    "# # print()\n",
    "print(get_neuronpedia_quick_list(sae=sae, features=token_feature_ids[:30], name=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd556ded-cbb3-4d4c-8e7c-90b1b20f8987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect() # Python's garbage collector\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73e29bb3-8876-42ba-a227-356d64e4bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_process_datasets(df, DATASET_PATH, OUTPUT_PATH, model, sae, hook_point, release, layer):\n",
    "    # top_mean_ids, top_mean_vals, token_feature_ids\n",
    "    # all_top_mean_ids = []\n",
    "    # all_top_mean_vals = []\n",
    "    all_token_feature_ids = []\n",
    "    for prompt in df[\"prompt\"]:\n",
    "        try:\n",
    "            top_mean_ids, top_mean_vals, token_feature_ids = analyze_prompt_with_sae(\n",
    "                prompt=prompt,\n",
    "                model=model,\n",
    "                sae=sae,\n",
    "                hook_point=sae,\n",
    "                topk_mean=10,\n",
    "                topk_token=3,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error on prompt: {prompt[:50]}... -> {e}\")\n",
    "            top_mean_ids, top_mean_vals, token_feature_ids = None, None, None\n",
    "    \n",
    "        # all_top_mean_ids.append(top_mean_ids)\n",
    "        # all_top_mean_vals.append(top_mean_vals)\n",
    "        all_token_feature_ids.append(token_feature_ids)\n",
    "\n",
    "    # print(len(all_token_feature_ids), all_token_feature_ids)\n",
    "    # Save to DataFrame\n",
    "    # SAE_ID = f\"layer_{LAYER}/width_16k/canonical\"\n",
    "    # df[release+ \"-layer\" + str(layer) + \"-\" +\"top_mean_ids\"] = all_top_mean_ids\n",
    "    # df[release+ \"-layer\" + str(layer) + \"-\" +\"top_mean_vals\"] = all_top_mean_vals\n",
    "    df[release+ \"-layer\" + str(layer) + \"-\" +\"token_feature_ids\"] = all_token_feature_ids\n",
    "    \n",
    "    # save to CSV\n",
    "    df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\", quoting=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bb64367-0d23-475b-8a21-a31f94045194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['programming'] ['res']\n"
     ]
    }
   ],
   "source": [
    "# RELEASE + SAE_ID + TYPE\n",
    "# LAYER = 25\n",
    "\n",
    "LAYERS = [i for i in range(19, 26)]\n",
    "# RELEASE = f\"gemma-scope-2b-pt-{SAE_TYPE}-canonical\"\n",
    "SAE_TYPES = [\n",
    "            \"res\", \n",
    "            # \"mlp\", \n",
    "            # \"att\"\n",
    "            ]\n",
    "DATASETS_NAMES = [\n",
    "                # \"emotion\", \n",
    "                # \"math\", \n",
    "                # \"mmlu\", \n",
    "                \"programming\"\n",
    "]\n",
    "\n",
    "print(DATASETS_NAMES, SAE_TYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc3f3a0-909c-45bc-a7e1-9e8e19a6be42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119a2fd77a494ae78da6fcee08a8eaa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n",
      "res 19 blocks.19.hook_resid_post\n"
     ]
    }
   ],
   "source": [
    "for sae_type in SAE_TYPES:\n",
    "    for layer in LAYERS:\n",
    "        # load model \n",
    "        model_name = \"gemma-2-2b\"\n",
    "        release = f\"gemma-scope-2b-pt-{sae_type}-canonical\"\n",
    "        sae_id = f\"layer_{layer}/width_16k/canonical\"\n",
    "        model, sae, hook_point = load_model_and_sae(model_name=model_name, release=release, sae_id=sae_id, device=\"cuda\")\n",
    "        print(sae_type, layer, hook_point)\n",
    "        \n",
    "        for DATASET_NAME in DATASETS_NAMES:\n",
    "            DATASET_PATH = f\"./{DATASET_NAME}.csv\"\n",
    "            OUTPUT_PATH =  f\"./{DATASET_NAME}.csv\"\n",
    "            df = load_dataset(DATASET_PATH)\n",
    "        \n",
    "            # print(\"PROCESSING: \", DATASET_NAME)\n",
    "        \n",
    "            pipeline_process_datasets(df, DATASET_PATH, OUTPUT_PATH, model, sae, hook_point, release, layer)\n",
    "        \n",
    "        # free this SAE right away\n",
    "        del sae\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # free the model before moving on to the next SAE type\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5527ba-8020-478d-a22f-ed1341de3547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
