{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88ed6e84-371a-4233-a800-7e611e24910b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jul 26 21:40:48 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              53W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1479b6e9-b5b6-4d34-80cd-acf83e5d1285",
   "metadata": {},
   "source": [
    "# Via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb25d7d3-0473-4fe4-9840-0b01c80b5641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'STEERED': {'raw': '<bos><start_of_turn>user\\nhi<end_of_turn>\\n<start_of_turn>model\\nMeow! 👋  How can I help you today?<end_of_turn><eos>', 'chatTemplate': [{'content': 'hi', 'role': 'user'}, {'content': 'Meow! 👋  How can I help you today?', 'role': 'model'}]}, 'DEFAULT': {'raw': '<bos><start_of_turn>user\\nhi<end_of_turn>\\n<start_of_turn>model\\nHello! 👋\\n\\nHow can I help you today? 😊<end_of_turn><eos>', 'chatTemplate': [{'content': 'hi', 'role': 'user'}, {'content': 'Hello! 👋\\n\\nHow can I help you today? 😊', 'role': 'model'}]}, 'id': 'cm3j3ci17002zem3pxl7zg5jy', 'shareUrl': 'https://www.neuronpedia.org/steer/cm3j3ci17002zem3pxl7zg5jy', 'limit': '294', 'settings': {'temperature': 0.5, 'n_tokens': 48, 'freq_penalty': 2, 'seed': 16, 'strength_multiplier': 4, 'steer_special_tokens': True, 'steer_method': 'SIMPLE_ADDITIVE'}}\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "\n",
    "req = requests.post(\n",
    "    \"https://www.neuronpedia.org/api/steer-chat\",\n",
    "    headers={\n",
    "      \"Content-Type\": \"application/json\"\n",
    "    },\n",
    "    json={\n",
    "      \"defaultChatMessages\": [\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"hi\"\n",
    "        }\n",
    "      ],\n",
    "      \"steeredChatMessages\": [\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"hi\"\n",
    "        }\n",
    "      ],\n",
    "      \"modelId\": \"gemma-2-9b-it\",\n",
    "      \"features\": [\n",
    "        {\n",
    "          \"modelId\": \"gemma-2-9b-it\",\n",
    "          \"layer\": \"9-gemmascope-res-131k\",\n",
    "          \"index\": 62610,\n",
    "          \"strength\": 48\n",
    "        }\n",
    "      ],\n",
    "      \"temperature\": 0.5,\n",
    "      \"n_tokens\": 48,\n",
    "      \"freq_penalty\": 2,\n",
    "      \"seed\": 16,\n",
    "      \"strength_multiplier\": 4,\n",
    "      \"steer_special_tokens\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "data = req.json()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbc05d9c-61db-412f-8805-469b5e2e0252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP 500\n",
      "{\n",
      "  \"message\": \"Unknown Error\"\n",
      "}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a86fbde7-aaf9-4cf5-8efb-83c882b01b73",
   "metadata": {},
   "source": [
    "# Chat completion - RES - API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8289ed1e-e6a8-46f4-a48c-a657d99bf3d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (523861238.py, line 17)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m}\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ONLY WORKS WITH RES/MLP CHAT COMPLETION\n",
    "import requests \n",
    "\n",
    "req = requests.post(\"https://www.neuronpedia.org/api/steer\",\n",
    "    headers={\n",
    "      \"Content-Type\": \"application/json\"\n",
    "    },\n",
    "    json={\n",
    "      \"prompt\": \"How are you feelings\",\n",
    "      \"modelId\": \"gemma-2-2b\",\n",
    "      \"features\": [\n",
    "        {\n",
    "          \"modelId\": \"gemma-2-2b\",\n",
    "          \"layer\": \"6-gemmascope-res-16k\",\n",
    "          \"index\": 4948,\n",
    "          \"strength\": 0\n",
    "        }\n",
    "      ],\n",
    "      \"temperature\": 0.5,\n",
    "      \"n_tokens\": 48,\n",
    "      \"freq_penalty\": 2,\n",
    "      \"seed\": 16,\n",
    "      \"strength_multiplier\": 4\n",
    "    }\n",
    ")\n",
    "\n",
    "data = req.json()\n",
    "print(data)\n",
    "print(data[\"STEERED\"])\n",
    "print()\n",
    "print(data[\"DEFAULT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0988254-4de6-4bdc-89b0-61ee2962f6dc",
   "metadata": {},
   "source": [
    "# BY WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "25b46062-b02e-48fd-bdc7-10dab15a75fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 10 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m latent_activations = sae.encode(activation_cache[hook_point])\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# pick the most activated feature index\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m top_idx = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_activations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# decode the SAE direction back to model space\u001b[39;00m\n\u001b[32m     20\u001b[39m steering_vector = sae.W_dec[top_idx]  \u001b[38;5;66;03m# shape [hidden_size]\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: a Tensor with 10 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "from sae_lens import SAE\n",
    "import torch\n",
    "\n",
    "# your initial template\n",
    "model = HookedSAETransformer.from_pretrained(\"gpt2-small\", device=\"cuda\")\n",
    "sae = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",  # this is layer 7's residual-pre hook\n",
    "    device=\"cuda\",\n",
    ")[0]\n",
    "prompt = \" I am happy, what should I do now\"  # note the leading space for GPT‑2\n",
    "logits, activation_cache = model.run_with_cache(prompt, prepend_bos=True)\n",
    "\n",
    "hook_point = \"blocks.7.hook_resid_pre\"                  # e.g. \"blocks.7.hook_resid_pre\"\n",
    "latent_activations = sae.encode(activation_cache[hook_point])\n",
    "# pick the most activated feature index\n",
    "top_idx = torch.topk(latent_activations, k=1).indices.item()\n",
    "\n",
    "# decode the SAE direction back to model space\n",
    "steering_vector = sae.W_dec[top_idx]  # shape [hidden_size]\n",
    "steer_coeff = 0.3  # tune this value; smaller values give subtler effects\n",
    "def steering_hook(resid_pre: torch.Tensor, hook):\n",
    "    # resid_pre has shape [batch, sequence_length, hidden_size]\n",
    "    # broadcast the steering vector across the batch and sequence:\n",
    "    resid_pre += steer_coeff * steering_vector\n",
    "    return resid_pre\n",
    "from functools import partial\n",
    "\n",
    "# wrap the steering hook with the correct hook name\n",
    "hook_list = [(hook_point, steering_hook)]\n",
    "\n",
    "# generate a steered completion for another prompt\n",
    "with model.hooks(fwd_hooks=hook_list):\n",
    "    output_tokens = model.generate(\n",
    "        \"Tell me a joke about animals\",    # any prompt you want to steer\n",
    "        max_new_tokens=50,\n",
    "        temperature=1.0,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "print(model.to_string(output_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fbafc8c2-e817-4ead-9c6c-13da6032fa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Loading SAE...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60b094c113a40e69fd8973da64254e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cfg.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfacf07c5a5b4eabbd4f9503ba162f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sae_weights.safetensors:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b591480f07774a4d8aab85e5125a2f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sparsity.safetensors:   0%|          | 0.00/98.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HookedSAETransformer' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# --- 3. Get the Steering Vector ---\u001b[39;00m\n\u001b[32m     36\u001b[39m steering_vector = sae.W_dec[FEATURE_INDEX] * COEFFICIENT\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m steering_vector = steering_vector.to(\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# --- 4. Define the Hook Function ---\u001b[39;00m\n\u001b[32m     41\u001b[39m hook_point = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTEERING_LAYER\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.hook_resid_pre\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cephfs/volumes/hpc_data_prj/inf_narrative_msc/e8efa787-4d41-448d-a7aa-814b8f0fac1e/k24086575/jvenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'HookedSAETransformer' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "# --- 1. Model and SAE Loading (Updated for GPT-2) ---\n",
    "MODEL_NAME = \"gpt2-small\"\n",
    "# Use the SAE release specifically for gpt2-small\n",
    "RELEASE = \"gpt2-small-res-jb\"\n",
    "# We'll use the SAE trained on layer 9. GPT-2 Small has 12 layers (0-11).\n",
    "SAE_LAYER = 9\n",
    "# The ID format for this release is different\n",
    "SAE_ID = f\"blocks.{SAE_LAYER}.hook_resid_pre\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = HookedSAETransformer.from_pretrained(MODEL_NAME, device=\"cuda\")\n",
    "print(\"Loading SAE...\")\n",
    "\n",
    "# Unpack the tuple to get the SAE object.\n",
    "sae, sae_cfg, sae_sparsity = SAE.from_pretrained(\n",
    "    release=RELEASE,\n",
    "    sae_id=SAE_ID,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "print(\"Setup complete.\")\n",
    "\n",
    "\n",
    "# --- 2. Steering Configuration ---\n",
    "PROMPT = \"Their first meeting was at a cafe in Paris. From that day on, they knew\"\n",
    "# New feature index for the gpt2-small layer 9 SAE.\n",
    "FEATURE_INDEX = 4587 # A feature related to \"romance/relationships\"\n",
    "# For best results, we'll steer at the same layer the SAE was trained on.\n",
    "STEERING_LAYER = 9\n",
    "COEFFICIENT = 20.0 # Adjusted for the new model/feature\n",
    "\n",
    "\n",
    "# --- 3. Get the Steering Vector ---\n",
    "steering_vector = sae.W_dec[FEATURE_INDEX] * COEFFICIENT\n",
    "steering_vector = steering_vector.to(model.device)\n",
    "\n",
    "\n",
    "# --- 4. Define the Hook Function ---\n",
    "hook_point = f\"blocks.{STEERING_LAYER}.hook_resid_pre\"\n",
    "\n",
    "def steering_hook(resid_pre, hook):\n",
    "    \"\"\"Adds the steering vector to the residual stream of the last token.\"\"\"\n",
    "    if resid_pre.shape[1] > 0:\n",
    "        resid_pre[:, -1, :] += steering_vector\n",
    "    return resid_pre\n",
    "\n",
    "\n",
    "# --- 5. Generate Steered and Unsteered Text ---\n",
    "print(\"\\n--- Generating Steered Output (GPT-2) ---\")\n",
    "with model.hooks(fwd_hooks=[(hook_point, steering_hook)]):\n",
    "    steered_output = model.generate(\n",
    "        PROMPT,\n",
    "        max_new_tokens=25,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )\n",
    "print(steered_output)\n",
    "\n",
    "print(\"\\n--- Generating Unsteered Output (for comparison) ---\")\n",
    "unsteered_output = model.generate(\n",
    "    PROMPT,\n",
    "    max_new_tokens=25,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    ")\n",
    "print(unsteered_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0bc5d5a7-143b-41ab-8cb0-2202b439404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect() # Python's garbage collector\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "391306a1-dc2c-469f-a5e7-d63d3e299e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "model = HookedSAETransformer.from_pretrained(\"gpt2-small\", device=\"cuda\")\n",
    "sae, sae_cfg, _ = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",  # <- Release name\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",  # <- SAE id (not always a hook point!)\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# ==========================================================================\n",
    "# SAE_TYPE = \"res\"    # res/ mlp/ att\n",
    "# MODEL_NAME = \"gemma-2-2b\"\n",
    "# RELEASE = f\"gemma-scope-2b-pt-{SAE_TYPE}-canonical\"\n",
    "# LAYER = 25\n",
    "# SAE_ID = f\"layer_{LAYER}/width_16k/canonical\"\n",
    "\n",
    "# model = HookedSAETransformer.from_pretrained(MODEL_NAME, device=\"cuda\")\n",
    "# sae, cfg, _ = SAE.from_pretrained(\n",
    "#     release=RELEASE,  # <- Release name\n",
    "#     sae_id=SAE_ID,  # <- SAE id (not always a hook point!)\n",
    "#     device=\"cuda\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8f0641fb-90aa-42ab-b2c7-b782541d7138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'gpt2-small',\n",
       " 'hook_point': 'blocks.7.hook_resid_pre',\n",
       " 'hook_point_layer': 7,\n",
       " 'hook_point_head_index': None,\n",
       " 'dataset_path': 'Skylion007/openwebtext',\n",
       " 'is_dataset_tokenized': False,\n",
       " 'context_size': 128,\n",
       " 'use_cached_activations': False,\n",
       " 'cached_activations_path': 'activations/Skylion007_openwebtext/gpt2-small/blocks.7.hook_resid_pre',\n",
       " 'd_in': 768,\n",
       " 'n_batches_in_buffer': 128,\n",
       " 'total_training_tokens': 300000000,\n",
       " 'store_batch_size': 32,\n",
       " 'device': 'cuda',\n",
       " 'seed': 42,\n",
       " 'dtype': 'torch.float32',\n",
       " 'b_dec_init_method': 'geometric_median',\n",
       " 'expansion_factor': 32,\n",
       " 'from_pretrained_path': None,\n",
       " 'l1_coefficient': 8e-05,\n",
       " 'lr': 0.0004,\n",
       " 'lr_scheduler_name': None,\n",
       " 'lr_warm_up_steps': 5000,\n",
       " 'train_batch_size': 4096,\n",
       " 'use_ghost_grads': False,\n",
       " 'feature_sampling_window': 1000,\n",
       " 'feature_sampling_method': None,\n",
       " 'resample_batches': 1028,\n",
       " 'feature_reinit_scale': 0.2,\n",
       " 'dead_feature_window': 5000,\n",
       " 'dead_feature_estimation_method': 'no_fire',\n",
       " 'dead_feature_threshold': 1e-08,\n",
       " 'log_to_wandb': True,\n",
       " 'wandb_project': 'mats_sae_training_gpt2_small_resid_pre_5',\n",
       " 'wandb_entity': None,\n",
       " 'wandb_log_frequency': 100,\n",
       " 'n_checkpoints': 10,\n",
       " 'checkpoint_path': 'checkpoints/n6gbpj7s',\n",
       " 'd_sae': 24576,\n",
       " 'tokens_per_buffer': 67108864,\n",
       " 'run_name': '24576-L1-8e-05-LR-0.0004-Tokens-3.000e+08',\n",
       " 'model_from_pretrained_kwargs': {'center_writing_weights': True},\n",
       " 'neuronpedia_id': 'gpt2-small/7-res-jb',\n",
       " 'finetuning_scaling_factor': False,\n",
       " 'prepend_bos': True,\n",
       " 'dataset_trust_remote_code': True,\n",
       " 'apply_b_dec_to_input': True,\n",
       " 'sae_lens_training_version': None,\n",
       " 'activation_fn_str': 'relu',\n",
       " 'architecture': 'standard',\n",
       " 'normalize_activations': 'none'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4eaed727-2594-4181-b21d-f7565dbd069a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAE attaches to hook: blocks.7.hook_resid_pre\n",
      "Steered next token:  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "# # --- Configuration ---\n",
    "# MODEL_NAME = \"gemma-2-2b\"\n",
    "# SAE_TYPE   = \"res\"                  # residual-stream SAE\n",
    "# LAYER      = 0                      # layer to steer\n",
    "# RELEASE    = f\"gemma-scope-2b-pt-{SAE_TYPE}-canonical\"\n",
    "# SAE_ID     = f\"layer_{LAYER}/width_16k/canonical\"\n",
    "\n",
    "# # Choose a feature to steer – find an index on Neuronpedia for layer 0\n",
    "# feature_id = 0      # <-- replace with a real feature index\n",
    "# strength   = 4.0    # multiplier; positive pushes towards the feature\n",
    "# pos        = None   # None => apply at all token positions, or use an int index\n",
    "\n",
    "# # --- Load model and SAE ---\n",
    "# # HookedSAETransformer acts like HookedTransformer but can splice in SAEs:contentReference[oaicite:0]{index=0}.\n",
    "# model: HookedSAETransformer = HookedSAETransformer.from_pretrained(MODEL_NAME, device=\"cuda\")\n",
    "\n",
    "# # SAE.from_pretrained returns (sae, sae_cfg, metadata)\n",
    "# sae, sae_cfg, _ = SAE.from_pretrained(\n",
    "#     release=RELEASE,\n",
    "#     sae_id=SAE_ID,\n",
    "#     device=\"cuda\",\n",
    "# )\n",
    "\n",
    "# Determine the hook name for this SAE (e.g. \"blocks.0.hook_resid_pre\")\n",
    "sae_layer_name = sae_cfg.metadata.hook_name\n",
    "print(\"SAE attaches to hook:\", sae_layer_name)\n",
    "\n",
    "# --- Steering hook function ---\n",
    "def multiply_feature(sae_acts: torch.Tensor, hook, pos: int, feature_id: int, strength: float):\n",
    "    \"\"\"\n",
    "    Multiply a given SAE latent feature by (1 + strength) either at one position or all positions.\n",
    "    sae_acts has shape [batch, seq_len, d_sae].\n",
    "    \"\"\"\n",
    "    if pos is None:\n",
    "        sae_acts[:, :, feature_id] *= (1.0 + strength)\n",
    "    else:\n",
    "        sae_acts[:, pos, feature_id] *= (1.0 + strength)\n",
    "    return sae_acts\n",
    "\n",
    "# --- Run with steering ---\n",
    "prompt = \"The most iconic structure in London is\"  # your prompt here\n",
    "tokens = model.to_tokens(prompt, prepend_bos=True).to(model.cfg.device)\n",
    "\n",
    "# Use run_with_hooks_with_saes to splice in the SAE for this forward pass and modify the latent\n",
    "# activations via a forward hook:contentReference[oaicite:1]{index=1}.\n",
    "logits = model.run_with_hooks_with_saes(\n",
    "    tokens,\n",
    "    saes=[sae],  # list of SAEs to splice in\n",
    "    fwd_hooks=[\n",
    "        (\n",
    "            sae_layer_name + \".hook_sae_acts_post\",  # internal SAE activation hook\n",
    "            partial(multiply_feature, pos=pos, feature_id=feature_id, strength=strength),\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Get the next-token prediction from the steered logits\n",
    "next_token = logits[:, -1, :].argmax(dim=-1)\n",
    "steered_output = model.to_string(next_token)[0]\n",
    "print(\"Steered next token:\", steered_output)\n",
    "\n",
    "# If you want to keep generating more tokens, loop by appending `next_token` to `tokens`\n",
    "# and calling model.run_with_hooks_with_saes again for each new step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ecdc079-93f1-4b47-87ba-2e502046921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate an object to hold activations from a dataset\n",
    "from sae_lens import ActivationsStore\n",
    "\n",
    "# a convenient way to instantiate an activation store is to use the from_sae method\n",
    "activation_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    dataset=sae.cfg.metadata.dataset_path,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    # fairly conservative parameters here so can use same for larger\n",
    "    # models without running out of memory.\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=4096,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b882633-9cfb-4790-86ae-db0cd2b917ee",
   "metadata": {},
   "source": [
    "# Working 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0cc94c8-95f5-4e93-9b26-3333183178a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "SAE attaches to hook: blocks.7.hook_resid_pre\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"gpt2-small\"\n",
    "RELEASE    = \"gpt2-small-res-jb\"\n",
    "SAE_ID     = \"blocks.7.hook_resid_pre\"\n",
    "DEVICE     = \"cuda\"\n",
    "\n",
    "\n",
    "# --- Load model and SAE ---\n",
    "model = HookedSAETransformer.from_pretrained(MODEL_NAME, device=DEVICE)\n",
    "sae, sae_cfg, _ = SAE.from_pretrained(\n",
    "    release=RELEASE,\n",
    "    sae_id=SAE_ID,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "# sae_cfg is a dict, so get the hook name under \"hook_point\"\n",
    "sae_layer_name = sae_cfg[\"hook_point\"]\n",
    "hook_name = sae_cfg[\"hook_point\"]\n",
    "print(\"SAE attaches to hook:\", sae_layer_name)\n",
    "\n",
    "\n",
    "# --- Steering parameters ---\n",
    "# prompt     = \"The most iconic structure in London is\"\n",
    "prompt     = \"I am feeling good\"\n",
    "\n",
    "feature_id = 2     # ← *replace* with a real feature index from Neuronpedia\n",
    "feature_id = 1         # choose an actually dormant or live feature\n",
    "\n",
    "strength   = 20.0       # multiplier (e.g. 4.0 means 5× the original activation)\n",
    "pos        = None      # None=all token positions, or an int for a single position\n",
    "max_new_tokens = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e826919-a303-42d1-8e21-5ab96865c9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steered output:\n",
      " <|endoftext|>I am feeling good about this. I am not sure if I am feeling good about this. I am not sure if I am feeling good about this.\n",
      "\n",
      "I am not sure if I am feeling good about this. I am not sure if I am feeling good about this.\n",
      "\n",
      "I am not sure if I am feeling good\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Hook function to multiply one SAE feature ---\n",
    "def multiply_feature(sae_acts: torch.Tensor, hook, pos: int, feature_id: int, strength: float):\n",
    "    if pos is None:\n",
    "        sae_acts[:, :, feature_id] *= (1.0 + strength)\n",
    "    else:\n",
    "        sae_acts[:, pos, feature_id] *= (1.0 + strength)\n",
    "    return sae_acts\n",
    "\n",
    "# --- Steered generation loop ---\n",
    "def steer_generate(prompt: str, max_new_tokens: int = 20) -> str:\n",
    "    generated = model.to_tokens(prompt, prepend_bos=True).to(DEVICE)\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model.run_with_hooks_with_saes(\n",
    "            generated,\n",
    "            saes=[sae],\n",
    "            fwd_hooks=[\n",
    "                (\n",
    "                    sae_layer_name + \".hook_sae_acts_post\",\n",
    "                    partial(multiply_feature, pos=pos, feature_id=feature_id, strength=strength),\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "        next_tok = logits[:, -1, :].argmax(dim=-1).unsqueeze(-1)\n",
    "        generated = torch.cat((generated, next_tok), dim=1)\n",
    "    return model.to_string(generated)[0]\n",
    "\n",
    "# --- Example usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # out = steer_generate(\"The most iconic structure in London is\")\n",
    "    out = steer_generate(prompt, max_new_tokens=max_new_tokens)\n",
    "    print(\"Steered output:\\n\", out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974617b5-36e0-4622-be57-681b1fe3cd17",
   "metadata": {},
   "source": [
    "# Working 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3a490bb-3a7c-494e-96e2-e02ebad203e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa8879872564c758638c307aa090069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am feeling good and happy. I see that most of my friends are doing well too.\n",
      "\n",
      "I am happy\n",
      "<|endoftext|>I am feeling good about this. I am not sure if I can get a job, but I am sure that I am doing my job. I am not going to be able to get a job. I am not going to be able to get a job. I am not going to be able to get a job. I am not\n"
     ]
    }
   ],
   "source": [
    "# 2) Hook to inject\n",
    "def inject_feature(sae_acts, hook, pos, feature_id, strength):\n",
    "    if pos is None:\n",
    "        sae_acts[:, :, feature_id] = strength\n",
    "    else:\n",
    "        sae_acts[:, pos, feature_id] = strength\n",
    "    return sae_acts\n",
    "\n",
    "# 3) Steered generation\n",
    "def steer_generate(prompt, feature_id, strength, max_new_tokens=10):\n",
    "    tokens = model.to_tokens(prompt, prepend_bos=True).to(\"cuda\")\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model.run_with_hooks_with_saes(\n",
    "            tokens,\n",
    "            saes=[sae],\n",
    "            fwd_hooks=[\n",
    "                (\n",
    "                    hook_name + \".hook_sae_acts_post\",\n",
    "                    partial(inject_feature, pos=None, feature_id=feature_id, strength=strength),\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "        nxt = logits[:, -1, :].argmax(dim=-1).unsqueeze(-1)\n",
    "        tokens = torch.cat((tokens, nxt), dim=1)\n",
    "    return model.to_string(tokens)[0]\n",
    "\n",
    "# 4) Try it\n",
    "baseline = model.generate(prompt, max_new_tokens=20, temperature=0.7, do_sample=True)\n",
    "print(baseline)\n",
    "print(steer_generate(prompt, feature_id, strength, max_new_tokens=max_new_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a20b5d7d-925d-467e-a031-519e7a9d19f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "SAE attaches to hook: blocks.7.hook_resid_pre\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"gpt2-small\"\n",
    "RELEASE    = \"gpt2-small-res-jb\"\n",
    "SAE_ID     = \"blocks.7.hook_resid_pre\"\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Load model and SAE ---\n",
    "model = HookedSAETransformer.from_pretrained(MODEL_NAME, device=DEVICE)\n",
    "sae, sae_cfg, *_ = SAE.from_pretrained(\n",
    "    release=RELEASE,\n",
    "    sae_id=SAE_ID,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "hook_name = sae_cfg[\"hook_point\"]\n",
    "print(f\"SAE attaches to hook: {hook_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba3a589f-e134-47b1-9930-b835ba2a3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# NEURONPEDIA-STYLE STEERING: ADDITION-BASED\n",
    "# =============================================================================\n",
    "\n",
    "def add_feature_steering(sae_acts: torch.Tensor, hook, pos: int, feature_id: int, strength: float):\n",
    "    \"\"\"\n",
    "    Neuronpedia-style feature steering using ADDITION instead of multiplication.\n",
    "    This adds a constant value to the feature activation.\n",
    "    \n",
    "    Args:\n",
    "        sae_acts: SAE activations tensor [batch, seq_len, n_features]\n",
    "        hook: TransformerLens hook object\n",
    "        pos: Position to steer (None for all positions, int for specific position)\n",
    "        feature_id: Which SAE feature to steer\n",
    "        strength: How much to ADD to the feature activation\n",
    "    \"\"\"\n",
    "    if pos is None:\n",
    "        # Steer all positions\n",
    "        sae_acts[:, :, feature_id] += strength\n",
    "    else:\n",
    "        # Steer specific position\n",
    "        sae_acts[:, pos, feature_id] += strength\n",
    "    return sae_acts\n",
    "\n",
    "def neuronpedia_steer_generate(\n",
    "    prompt: str, \n",
    "    feature_id: int, \n",
    "    strength: float, \n",
    "    pos: int = None,\n",
    "    max_new_tokens: int = 20,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text with Neuronpedia-style SAE feature steering.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text prompt\n",
    "        feature_id: SAE feature index to steer\n",
    "        strength: Steering strength (positive = increase, negative = decrease)\n",
    "        pos: Token position to steer (None = all positions, -1 = last token only)\n",
    "        max_new_tokens: Number of tokens to generate\n",
    "        temperature: Sampling temperature (0 = greedy, higher = more random)\n",
    "    \n",
    "    Returns:\n",
    "        Generated text string\n",
    "    \"\"\"\n",
    "    tokens = model.to_tokens(prompt, prepend_bos=True).to(DEVICE)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Run model with SAE and steering hook\n",
    "        logits = model.run_with_hooks_with_saes(\n",
    "            tokens, \n",
    "            saes=[sae],\n",
    "            fwd_hooks=[\n",
    "                (\n",
    "                    hook_name + \".hook_sae_acts_post\",\n",
    "                    partial(add_feature_steering, pos=pos, feature_id=feature_id, strength=strength)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Sample next token\n",
    "        if temperature == 0:\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1).unsqueeze(-1)\n",
    "        else:\n",
    "            probs = torch.softmax(logits[:, -1, :] / temperature, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "        \n",
    "        tokens = torch.cat([tokens, next_token], dim=1)\n",
    "    \n",
    "    return model.to_string(tokens)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad3b0ef4-1e42-4d72-aeec-429575db8175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASELINE (No Steering) ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b682a86f544adf9538c9594ed90d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: I am feeling good about my health right now. I am having trouble getting the vitamins I need. I am feeling better about my health right now. I am feeling better about my health right now. I am feeling better about my health right now. I am feeling better about my health right now. I am feeling better about my health right\n",
      "\n",
      "=== NEURONPEDIA-STYLE STEERING (Addition) ===\n",
      "Addition (+10.0): <|endoftext|>I am feeling good about my life.\n",
      "\n",
      "I'm thinking about the 5 steps I took to become a professional.\n",
      "Addition (-5.0): <|endoftext|>I am feeling good about the state of the game\n",
      "\n",
      "So, if you have read my previous post, you will\n",
      "\n",
      "=== INJECTION STEERING (Set Value) ===\n",
      "Injection (=15.0): <|endoftext|>I am feeling good about my performance, but I have to let go from the situation I have with my mouth.\n",
      "\n",
      "\n",
      "=== POSITION-SPECIFIC STEERING ===\n",
      "Last position only (+8.0): <|endoftext|>I am feeling good about this. I'm really excited about this. I'm not just looking forward to this season.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE USAGE\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"I am feeling good\"\n",
    "    feature_id = 1000  # Replace with actual feature ID from Neuronpedia\n",
    "    \n",
    "    # print(\"=== BASELINE (No Steering) ===\")\n",
    "    # baseline = model.generate(prompt, max_new_tokens=64, temperature=0.5, do_sample=True)\n",
    "    # print(f\"Baseline: {baseline}\")\n",
    "\n",
    "    # TOKENS: 64, TEMP: 0.5, FREQ PENALTY: 1, STRENGTH MULTIPLE: 1, MANUAL SEED: 16\n",
    "    neuronpedia_settings = {\n",
    "        'max_new_tokens': 64,\n",
    "        'temperature': 0.5, \n",
    "        'freq_penalty': 1.0,  # No frequency penalty\n",
    "        'seed': 16  # Manual seed from interface\n",
    "    }\n",
    "    print(\"=== BASELINE (No Steering) ===\")\n",
    "    # Set seed for baseline too\n",
    "    # torch.manual_seed(neuronpedia_settings['seed'])\n",
    "    # if torch.cuda.is_available():\n",
    "    #     torch.cuda.manual_seed(neuronpedia_settings['seed'])\n",
    "    baseline = model.generate(\n",
    "        prompt, \n",
    "        max_new_tokens=neuronpedia_settings['max_new_tokens'], \n",
    "        temperature=neuronpedia_settings['temperature'], \n",
    "        do_sample=True\n",
    "    )\n",
    "    print(f\"Baseline: {baseline}\")\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"\\n=== NEURONPEDIA-STYLE STEERING (Addition) ===\")\n",
    "    # Positive steering - enhance the feature\n",
    "    steered_pos = neuronpedia_steer_generate(\n",
    "        prompt=prompt,\n",
    "        feature_id=feature_id,\n",
    "        strength=10.0,  # Add 10.0 to feature activation\n",
    "        pos=None,       # Steer all positions\n",
    "        max_new_tokens=20,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(f\"Addition (+10.0): {steered_pos}\")\n",
    "    \n",
    "    # Negative steering - suppress the feature\n",
    "    steered_neg = neuronpedia_steer_generate(\n",
    "        prompt=prompt,\n",
    "        feature_id=feature_id,\n",
    "        strength=-5.0,  # Subtract 5.0 from feature activation\n",
    "        pos=None,\n",
    "        max_new_tokens=20,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(f\"Addition (-5.0): {steered_neg}\")\n",
    "    \n",
    "    print(\"\\n=== INJECTION STEERING (Set Value) ===\")\n",
    "    # Injection steering - force specific activation\n",
    "    injected = injection_steer_generate(\n",
    "        prompt=prompt,\n",
    "        feature_id=feature_id,\n",
    "        strength=15.0,  # Set feature activation to exactly 15.0\n",
    "        pos=None,\n",
    "        max_new_tokens=20,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(f\"Injection (=15.0): {injected}\")\n",
    "    \n",
    "    print(\"\\n=== POSITION-SPECIFIC STEERING ===\")\n",
    "    # Steer only the last token position\n",
    "    last_pos_steered = neuronpedia_steer_generate(\n",
    "        prompt=prompt,\n",
    "        feature_id=feature_id,\n",
    "        strength=8.0,\n",
    "        pos=-1,  # Only steer the last token\n",
    "        max_new_tokens=20,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    print(f\"Last position only (+8.0): {last_pos_steered}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c7676-782b-4447-b48f-e8c3092780ca",
   "metadata": {},
   "source": [
    "# ====================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2ff09e10-735e-4e6d-87ac-ce764d11176b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Loading SAE...\n",
      "Setup complete.\n",
      "\n",
      "--- Generating Steered Output (GPT-2) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01afa6c072b4a158c5f8dc70abcf991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most iconic structure in London is now being demolished.\n",
      "\n",
      "Crackhead Building, which is believed to still be in use in London, is the tallest\n",
      "\n",
      "--- Generating Unsteered Output (for comparison) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77f094813024e4497fe240dfa90ded4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most iconic structure in London is the National Building Museum. It's a white building in the middle of the city where the National Gallery of Art is located and\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "# --- 1. Model and SAE Loading ---\n",
    "MODEL_NAME = \"gpt2-small\"\n",
    "RELEASE = \"gpt2-small-res-jb\"\n",
    "SAE_LAYER = 7\n",
    "SAE_ID = f\"blocks.{SAE_LAYER}.hook_resid_pre\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = HookedSAETransformer.from_pretrained(MODEL_NAME, device=\"cuda\")\n",
    "print(\"Loading SAE...\")\n",
    "\n",
    "sae, sae_cfg, sae_sparsity = SAE.from_pretrained(\n",
    "    release=RELEASE,\n",
    "    sae_id=SAE_ID,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "print(\"Setup complete.\")\n",
    "\n",
    "\n",
    "# --- 2. Steering Configuration ---\n",
    "PROMPT = \"The most iconic structure in London is\"\n",
    "FEATURE_INDEX = 1 # A feature related to \"romance/relationships\"\n",
    "STEERING_LAYER = 7\n",
    "COEFFICIENT = 20.0\n",
    "\n",
    "\n",
    "# --- 3. Get the Steering Vector ---\n",
    "steering_vector = sae.W_dec[FEATURE_INDEX] * COEFFICIENT\n",
    "# CORRECTED LINE: Get the device from the model's config object.\n",
    "steering_vector = steering_vector.to(model.cfg.device)\n",
    "\n",
    "\n",
    "# --- 4. Define the Hook Function ---\n",
    "hook_point = f\"blocks.{STEERING_LAYER}.hook_resid_pre\"\n",
    "\n",
    "def steering_hook(resid_pre, hook):\n",
    "    \"\"\"Adds the steering vector to the residual stream of the last token.\"\"\"\n",
    "    if resid_pre.shape[1] > 0:\n",
    "        resid_pre[:, -1, :] += steering_vector\n",
    "    return resid_pre\n",
    "\n",
    "\n",
    "# --- 5. Generate Steered and Unsteered Text ---\n",
    "print(\"\\n--- Generating Steered Output (GPT-2) ---\")\n",
    "with model.hooks(fwd_hooks=[(hook_point, steering_hook)]):\n",
    "    steered_output = model.generate(\n",
    "        PROMPT,\n",
    "        max_new_tokens=25,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )\n",
    "print(steered_output)\n",
    "\n",
    "print(\"\\n--- Generating Unsteered Output (for comparison) ---\")\n",
    "unsteered_output = model.generate(\n",
    "    PROMPT,\n",
    "    max_new_tokens=25,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    ")\n",
    "print(unsteered_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b5b0b3cc-2373-4ae9-8a45-a7decc3ef44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m prepend_bos = sae_cfg.get(\u001b[33m\"\u001b[39m\u001b[33mprepend_bos\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# --- Activation Store Setup ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m activation_store = \u001b[43mActivationsStore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_sae\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43msae\u001b[49m\u001b[43m=\u001b[49m\u001b[43msae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# dataset=sae_cfg[\"dataset_path\"],\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopenwebtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstore_batch_size_prompts\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_batch_size_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_batches_in_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfind_max_activation\u001b[39m(model, sae, sae_cfg, activation_store, feature_idx, num_batches=\u001b[32m100\u001b[39m):\n\u001b[32m     39\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    Find the maximum absolute activation for `feature_idx` across samples.\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cephfs/volumes/hpc_data_prj/inf_narrative_msc/e8efa787-4d41-448d-a7aa-814b8f0fac1e/k24086575/jvenv/lib/python3.11/site-packages/sae_lens/training/activations_store.py:162\u001b[39m, in \u001b[36mActivationsStore.from_sae\u001b[39m\u001b[34m(cls, model, sae, context_size, dataset, streaming, store_batch_size_prompts, n_batches_in_buffer, train_batch_size_tokens, total_tokens, device)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_sae\u001b[39m(\n\u001b[32m    150\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    160\u001b[39m     device: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    161\u001b[39m ) -> ActivationsStore:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43msae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43md_in\u001b[49m\u001b[43m=\u001b[49m\u001b[43msae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43md_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhook_layer\u001b[49m\u001b[43m=\u001b[49m\u001b[43msae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhook_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhook_head_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43msae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhook_head_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43msae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[43m=\u001b[49m\u001b[43msae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstore_batch_size_prompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstore_batch_size_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_batch_size_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_batch_size_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_batches_in_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_batches_in_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_training_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormalize_activations\u001b[49m\u001b[43m=\u001b[49m\u001b[43msae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize_activations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset_trust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43msae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_trust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43msae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseqpos_slice\u001b[49m\u001b[43m=\u001b[49m\u001b[43msae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseqpos_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cephfs/volumes/hpc_data_prj/inf_narrative_msc/e8efa787-4d41-448d-a7aa-814b8f0fac1e/k24086575/jvenv/lib/python3.11/site-packages/sae_lens/training/activations_store.py:213\u001b[39m, in \u001b[36mActivationsStore.__init__\u001b[39m\u001b[34m(self, model, dataset, streaming, hook_name, hook_layer, hook_head_index, context_size, d_in, n_batches_in_buffer, total_training_tokens, store_batch_size_prompts, train_batch_size_tokens, prepend_bos, normalize_activations, device, dtype, cached_activations_path, model_kwargs, autocast_lm, dataset_trust_remote_code, seqpos_slice, exclude_special_tokens)\u001b[39m\n\u001b[32m    210\u001b[39m     model_kwargs = {}\n\u001b[32m    211\u001b[39m \u001b[38;5;28mself\u001b[39m.model_kwargs = model_kwargs\n\u001b[32m    212\u001b[39m \u001b[38;5;28mself\u001b[39m.dataset = (\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_trust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m dataset\n\u001b[32m    221\u001b[39m )\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, (Dataset, DatasetDict)):\n\u001b[32m    224\u001b[39m     \u001b[38;5;28mself\u001b[39m.dataset = cast(Dataset | DatasetDict, \u001b[38;5;28mself\u001b[39m.dataset)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cephfs/volumes/hpc_data_prj/inf_narrative_msc/e8efa787-4d41-448d-a7aa-814b8f0fac1e/k24086575/jvenv/lib/python3.11/site-packages/datasets/load.py:2606\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2601\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   2602\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   2603\u001b[39m )\n\u001b[32m   2605\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2606\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2609\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2612\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2613\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2614\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2617\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2620\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2621\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2623\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   2624\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cephfs/volumes/hpc_data_prj/inf_narrative_msc/e8efa787-4d41-448d-a7aa-814b8f0fac1e/k24086575/jvenv/lib/python3.11/site-packages/datasets/load.py:2277\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[39m\n\u001b[32m   2275\u001b[39m     download_config = download_config.copy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[32m   2276\u001b[39m     download_config.storage_options.update(storage_options)\n\u001b[32m-> \u001b[39m\u001b[32m2277\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2286\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2287\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2288\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2289\u001b[39m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[32m   2290\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cephfs/volumes/hpc_data_prj/inf_narrative_msc/e8efa787-4d41-448d-a7aa-814b8f0fac1e/k24086575/jvenv/lib/python3.11/site-packages/datasets/load.py:1923\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[39m\n\u001b[32m   1918\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[32m   1919\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1920\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or any data file in the same directory. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1921\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1922\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1925\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1926\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or any data file in the same directory.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1927\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cephfs/volumes/hpc_data_prj/inf_narrative_msc/e8efa787-4d41-448d-a7aa-814b8f0fac1e/k24086575/jvenv/lib/python3.11/site-packages/datasets/load.py:1875\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[39m\n\u001b[32m   1873\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m _require_default_config_name:\n\u001b[32m   1874\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m fs.open(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdatasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m-> \u001b[39m\u001b[32m1875\u001b[39m         can_load_config_from_parquet_export = \u001b[33m\"\u001b[39m\u001b[33mDEFAULT_CONFIG_NAME\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1876\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1877\u001b[39m     can_load_config_from_parquet_export = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:322\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import re\n",
    "from sae_lens import SAE, HookedSAETransformer, ActivationsStore\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"gpt2-small\"\n",
    "RELEASE    = \"gpt2-small-res-jb\"\n",
    "SAE_ID     = \"blocks.7.hook_resid_pre\"\n",
    "DEVICE     = \"cuda\"\n",
    "\n",
    "# --- Load model and SAE ---\n",
    "model = HookedSAETransformer.from_pretrained(MODEL_NAME, device=DEVICE)\n",
    "sae, sae_cfg, _ = SAE.from_pretrained(\n",
    "    release=RELEASE,\n",
    "    sae_id=SAE_ID,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "# Extract hook point and prepend_bos flag\n",
    "hook_point = sae_cfg[\"hook_point\"]               # e.g. \"blocks.7.hook_resid_pre\"\n",
    "prepend_bos = sae_cfg.get(\"prepend_bos\", True)\n",
    "\n",
    "# --- Activation Store Setup ---\n",
    "activation_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    # dataset=sae_cfg[\"dataset_path\"],\n",
    "    dataset=\"openwebtext\",\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=4096,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "def find_max_activation(model, sae, sae_cfg, activation_store, feature_idx, num_batches=100):\n",
    "    \"\"\"\n",
    "    Find the maximum absolute activation for `feature_idx` across samples.\n",
    "    \"\"\"\n",
    "    max_act = 0.0\n",
    "    # parse layer number from hook_point\n",
    "    layer = int(re.search(r\"\\.(\\d+)\\.\", hook_point).group(1))\n",
    "    for _ in tqdm(range(num_batches), desc=\"Finding max activation\"):\n",
    "        tokens = activation_store.get_batch_tokens().to(DEVICE)\n",
    "        # run up to just after the hook layer\n",
    "        _, cache = model.run_with_cache(\n",
    "            tokens,\n",
    "            stop_at_layer=layer + 1,\n",
    "            names_filter=[hook_point],\n",
    "        )\n",
    "        sae_in = cache[hook_point]                      # [batch, seq, d_in]\n",
    "        lats, _ = sae.encode(sae_in)                    # [batch, seq, d_sae], _\n",
    "        lats = lats.reshape(-1, lats.shape[-1])         # flatten to [batch*seq, d_sae]\n",
    "        batch_max = lats[:, feature_idx].abs().max().item()\n",
    "        max_act = max(max_act, batch_max)\n",
    "    return max_act\n",
    "\n",
    "def steering_hook(activations, hook, steering_vector, steering_strength, max_act):\n",
    "    \"\"\"\n",
    "    Inject a clamped steering vector into SAE activations.\n",
    "    \"\"\"\n",
    "    sv = steering_vector.to(activations.device)\n",
    "    sv = torch.clamp(sv, -max_act, max_act)\n",
    "    return activations + steering_strength * sv.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "def generate_with_steering(\n",
    "    model,\n",
    "    sae,\n",
    "    sae_cfg,\n",
    "    prompt,\n",
    "    steering_feature,\n",
    "    max_act,\n",
    "    steering_strength=1.0,\n",
    "    max_new_tokens=95,\n",
    "):\n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = model.to_tokens(prompt, prepend_bos=prepend_bos).to(device)\n",
    "    steering_vector = sae.W_dec[steering_feature].to(device)\n",
    "    hook = hook_point + \".hook_sae_acts_post\"\n",
    "    hook_fn = partial(\n",
    "        steering_hook,\n",
    "        steering_vector=steering_vector,\n",
    "        steering_strength=steering_strength,\n",
    "        max_act=max_act,\n",
    "    )\n",
    "    with model.hooks(fwd_hooks=[(hook, hook_fn)]):\n",
    "        out_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            stop_at_eos=True,\n",
    "            prepend_bos=prepend_bos,\n",
    "        )\n",
    "    return model.tokenizer.decode(out_ids[0])\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"Once upon a time\"\n",
    "    steering_feature = 20115  # pick a feature index (e.g. top from Neuronpedia)\n",
    "    max_act = find_max_activation(model, sae, sae_cfg, activation_store, steering_feature, num_batches=50)\n",
    "    print(f\"Max activation for feature {steering_feature}: {max_act:.4f}\\n\")\n",
    "\n",
    "    # Generate without steering\n",
    "    tokens = model.to_tokens(prompt, prepend_bos=prepend_bos).to(DEVICE)\n",
    "    normal_ids = model.generate(tokens, max_new_tokens=95, temperature=0.7, top_p=0.9, stop_at_eos=True, prepend_bos=prepend_bos)\n",
    "    normal_text = model.tokenizer.decode(normal_ids[0])\n",
    "    print(\"Normal generation:\\n\", normal_text, \"\\n\")\n",
    "\n",
    "    # Generate with steering\n",
    "    steered_text = generate_with_steering(\n",
    "        model, sae, sae_cfg, prompt, steering_feature, max_act, steering_strength=2.0, max_new_tokens=95\n",
    "    )\n",
    "    print(\"Steered generation:\\n\", steered_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8f0524-e622-42b6-ae83-01f6033f3b88",
   "metadata": {},
   "source": [
    "# FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f645a347-46f8-4ae6-bf91-338b5b8e7c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "SAE attaches to hook: blocks.7.hook_resid_pre\n",
      "=== BASELINE (No Steering) ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c66d15ba0fd4bb1a338ef62ee2504e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: I am feeling a bit of a little bit of a bit of a bit of a bit of a bit of a bit of a bit of a bit of a bit of a bit of a bit of a bit of a bit of a bit of a bit of a bit of a bit of a bit of a bit of a bit of\n",
      "\n",
      "=== NEURONPEDIA-STYLE STEERING (Exact Match) ===\n",
      "Neuronpedia Match: <|endoftext|>I am feeling a bit of a break-up, and I'm glad I got my first job as a software developer for a new client. I am now an internet journalist, host, programmer, and a writer. I am also a social media manager, a writer, and a producer. I have a living, working, and\n",
      "\n",
      "=== TESTING DIFFERENT STRENGTHS ===\n",
      "Strength  5.0: <|endoftext|>I am feeling a bit of a sense of déjà vu as I feel like I'm going to be taking a break and having a lot of fun. I'll be back in the morning, but I'm still feeling a bit of a hangover. I'm a little tired of being in a job, and I know\n",
      "Strength 10.0: <|endoftext|>I am feeling a bit of a break-up, and I'm glad I got my stuff back together. I hope to do some more work on the other side of the pond. I'm looking forward to doing some more work for the next few months, and I'm looking forward to seeing what I can do for the rest of\n",
      "Strength 20.0: <|endoftext|>I am feeling a bit of a break-up, and I'm glad I got my first job as a software developer for a new client. I am now an internet journalist, host, programmer, and a writer. I am also a social media manager, a writer, and a producer. I have a living, working, and\n",
      "Strength 30.0: <|endoftext|>I am feeling a bit of a break-up, and I'm still trying to figure out what I can do about it. I'm looking for a professional to help me get better.\n",
      "\n",
      "I'm also looking for a professional to help me to find a professional sports player.\n",
      "\n",
      "I have a full time job in the\n",
      "\n",
      "Top 5 active features for prompt: 'I am feeling good'\n",
      "Feature ID | Activation\n",
      "-------------------------\n",
      "     8598 |  116.287\n",
      "    12003 |   95.352\n",
      "    11433 |   83.296\n",
      "    14968 |   78.878\n",
      "    22789 |   78.876\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"gpt2-small\"\n",
    "RELEASE    = \"gpt2-small-res-jb\"\n",
    "SAE_ID     = \"blocks.7.hook_resid_pre\"\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Load model and SAE ---\n",
    "model = HookedSAETransformer.from_pretrained(MODEL_NAME, device=DEVICE)\n",
    "sae, sae_cfg, *_ = SAE.from_pretrained(\n",
    "    release=RELEASE,\n",
    "    sae_id=SAE_ID,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "hook_name = sae_cfg[\"hook_point\"]\n",
    "print(f\"SAE attaches to hook: {hook_name}\")\n",
    "\n",
    "# =============================================================================\n",
    "# NEURONPEDIA-STYLE STEERING: ADDITION-BASED\n",
    "# =============================================================================\n",
    "\n",
    "def add_feature_steering(sae_acts: torch.Tensor, hook, pos: int, feature_id: int, strength: float):\n",
    "    \"\"\"\n",
    "    Neuronpedia-style feature steering using ADDITION instead of multiplication.\n",
    "    This adds a constant value to the feature activation.\n",
    "    \n",
    "    Args:\n",
    "        sae_acts: SAE activations tensor [batch, seq_len, n_features]\n",
    "        hook: TransformerLens hook object\n",
    "        pos: Position to steer (None for all positions, int for specific position)\n",
    "        feature_id: Which SAE feature to steer\n",
    "        strength: How much to ADD to the feature activation\n",
    "    \"\"\"\n",
    "    if pos is None:\n",
    "        # Steer all positions\n",
    "        sae_acts[:, :, feature_id] += strength\n",
    "    else:\n",
    "        # Steer specific position\n",
    "        sae_acts[:, pos, feature_id] += strength\n",
    "    return sae_acts\n",
    "\n",
    "def neuronpedia_steer_generate(\n",
    "    prompt: str, \n",
    "    feature_id: int, \n",
    "    strength: float, \n",
    "    pos: int = None,\n",
    "    max_new_tokens: int = 20,\n",
    "    temperature: float = 0.7,\n",
    "    freq_penalty: float = 1.0,\n",
    "    seed: int = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text with Neuronpedia-style SAE feature steering.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text prompt\n",
    "        feature_id: SAE feature index to steer\n",
    "        strength: Steering strength (positive = increase, negative = decrease)\n",
    "        pos: Token position to steer (None = all positions, -1 = last token only)\n",
    "        max_new_tokens: Number of tokens to generate\n",
    "        temperature: Sampling temperature (0 = greedy, higher = more random)\n",
    "        freq_penalty: Frequency penalty (1.0 = no penalty, >1.0 = penalize repetition)\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Generated text string\n",
    "    \"\"\"\n",
    "    # Set random seed if provided\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    tokens = model.to_tokens(prompt, prepend_bos=True).to(DEVICE)\n",
    "    \n",
    "    # Track token frequencies for frequency penalty\n",
    "    token_counts = {}\n",
    "    \n",
    "    for step in range(max_new_tokens):\n",
    "        # Run model with SAE and steering hook\n",
    "        logits = model.run_with_hooks_with_saes(\n",
    "            tokens, \n",
    "            saes=[sae],\n",
    "            fwd_hooks=[\n",
    "                (\n",
    "                    hook_name + \".hook_sae_acts_post\",\n",
    "                    partial(add_feature_steering, pos=pos, feature_id=feature_id, strength=strength)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Apply frequency penalty\n",
    "        if freq_penalty != 1.0:\n",
    "            for token_id, count in token_counts.items():\n",
    "                logits[0, -1, token_id] /= (freq_penalty ** count)\n",
    "        \n",
    "        # Sample next token\n",
    "        if temperature == 0:\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1).unsqueeze(-1)\n",
    "        else:\n",
    "            probs = torch.softmax(logits[:, -1, :] / temperature, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "        \n",
    "        # Update token count for frequency penalty\n",
    "        token_id = next_token.item()\n",
    "        token_counts[token_id] = token_counts.get(token_id, 0) + 1\n",
    "        \n",
    "        tokens = torch.cat([tokens, next_token], dim=1)\n",
    "    \n",
    "    return model.to_string(tokens)[0]\n",
    "\n",
    "# =============================================================================\n",
    "# INJECTION-BASED STEERING (Alternative approach)\n",
    "# =============================================================================\n",
    "\n",
    "def inject_feature_activation(sae_acts: torch.Tensor, hook, pos: int, feature_id: int, strength: float):\n",
    "    \"\"\"\n",
    "    Injection-based steering: SET the feature activation to a specific value.\n",
    "    This completely overwrites the natural activation.\n",
    "    \"\"\"\n",
    "    if pos is None:\n",
    "        sae_acts[:, :, feature_id] = strength\n",
    "    else:\n",
    "        sae_acts[:, pos, feature_id] = strength\n",
    "    return sae_acts\n",
    "\n",
    "def injection_steer_generate(\n",
    "    prompt: str, \n",
    "    feature_id: int, \n",
    "    strength: float, \n",
    "    pos: int = None,\n",
    "    max_new_tokens: int = 20,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"Generate text with injection-based SAE feature steering.\"\"\"\n",
    "    tokens = model.to_tokens(prompt, prepend_bos=True).to(DEVICE)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model.run_with_hooks_with_saes(\n",
    "            tokens, \n",
    "            saes=[sae],\n",
    "            fwd_hooks=[\n",
    "                (\n",
    "                    hook_name + \".hook_sae_acts_post\",\n",
    "                    partial(inject_feature_activation, pos=pos, feature_id=feature_id, strength=strength)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if temperature == 0:\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1).unsqueeze(-1)\n",
    "        else:\n",
    "            probs = torch.softmax(logits[:, -1, :] / temperature, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "        \n",
    "        tokens = torch.cat([tokens, next_token], dim=1)\n",
    "    \n",
    "    return model.to_string(tokens)[0]\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE USAGE MATCHING NEURONPEDIA SETTINGS\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"I am feeling\"\n",
    "    feature_id = 1000  # Replace with actual feature ID from Neuronpedia\n",
    "    \n",
    "    # Neuronpedia settings from the interface:\n",
    "    # TOKENS: 64, TEMP: 0.5, FREQ PENALTY: 1, STRENGTH MULTIPLE: 1, MANUAL SEED: 16\n",
    "    neuronpedia_settings = {\n",
    "        'max_new_tokens': 64,\n",
    "        'temperature': 0.5, \n",
    "        'freq_penalty': 1.0,  # No frequency penalty\n",
    "        'seed': 16  # Manual seed from interface\n",
    "    }\n",
    "    \n",
    "    print(\"=== BASELINE (No Steering) ===\")\n",
    "    # Set seed for baseline too\n",
    "    torch.manual_seed(neuronpedia_settings['seed'])\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(neuronpedia_settings['seed'])\n",
    "    \n",
    "    baseline = model.generate(\n",
    "        prompt, \n",
    "        max_new_tokens=neuronpedia_settings['max_new_tokens'], \n",
    "        temperature=neuronpedia_settings['temperature'], \n",
    "        do_sample=True\n",
    "    )\n",
    "    print(f\"Baseline: {baseline}\")\n",
    "    \n",
    "    print(\"\\n=== NEURONPEDIA-STYLE STEERING (Exact Match) ===\")\n",
    "    # Feature 1000 with strength 20.0 (from interface: strength=20, multiple=1)\n",
    "    steered_neuronpedia = neuronpedia_steer_generate(\n",
    "        prompt=prompt,\n",
    "        feature_id=feature_id,\n",
    "        strength=20.0,  # Strength from interface\n",
    "        pos=None,       # Steer all positions (Simple Additive mode)\n",
    "        **neuronpedia_settings\n",
    "    )\n",
    "    print(f\"Neuronpedia Match: {steered_neuronpedia}\")\n",
    "    \n",
    "    print(\"\\n=== TESTING DIFFERENT STRENGTHS ===\")\n",
    "    for strength in [5.0, 10.0, 20.0, 30.0]:\n",
    "        steered = neuronpedia_steer_generate(\n",
    "            prompt=prompt,\n",
    "            feature_id=feature_id,\n",
    "            strength=strength,\n",
    "            pos=None,\n",
    "            **neuronpedia_settings\n",
    "        )\n",
    "        print(f\"Strength {strength:4.1f}: {steered}\")\n",
    "\n",
    "# =============================================================================\n",
    "# NEURONPEDIA-SPECIFIC FEATURE UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def neuronpedia_compatible_steering(\n",
    "    prompt: str,\n",
    "    feature_spec: str,  # e.g., \"7-RES-JB:1000\" \n",
    "    strength: float = 20.0,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Steer using Neuronpedia-style feature specification.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text\n",
    "        feature_spec: Feature specification like \"7-RES-JB:1000\" \n",
    "        strength: Steering strength\n",
    "        **kwargs: Additional generation parameters\n",
    "    \"\"\"\n",
    "    # Parse feature spec (simplified - assumes format \"LAYER-SOURCE:FEATURE_ID\")\n",
    "    if ':' in feature_spec:\n",
    "        source_part, feature_id = feature_spec.split(':')\n",
    "        feature_id = int(feature_id)\n",
    "    else:\n",
    "        raise ValueError(\"Feature spec should be in format 'LAYER-SOURCE:FEATURE_ID'\")\n",
    "    \n",
    "    return neuronpedia_steer_generate(\n",
    "        prompt=prompt,\n",
    "        feature_id=feature_id, \n",
    "        strength=strength,\n",
    "        pos=None,  # Neuronpedia default\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE DISCOVERY UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def find_active_features(prompt: str, top_k: int = 10):\n",
    "    \"\"\"\n",
    "    Find the most active SAE features for a given prompt.\n",
    "    Useful for discovering which features to steer.\n",
    "    \"\"\"\n",
    "    tokens = model.to_tokens(prompt, prepend_bos=True).to(DEVICE)\n",
    "    \n",
    "    # Get SAE activations\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache_with_saes(tokens, saes=[sae])\n",
    "        sae_acts = cache[hook_name + \".hook_sae_acts_post\"]\n",
    "    \n",
    "    # Average over sequence length and batch\n",
    "    mean_acts = sae_acts.mean(dim=(0, 1))  # [n_features]\n",
    "    \n",
    "    # Get top-k most active features\n",
    "    top_values, top_indices = torch.topk(mean_acts, top_k)\n",
    "    \n",
    "    print(f\"\\nTop {top_k} active features for prompt: '{prompt}'\")\n",
    "    print(\"Feature ID | Activation\")\n",
    "    print(\"-\" * 25)\n",
    "    for i, (idx, val) in enumerate(zip(top_indices, top_values)):\n",
    "        print(f\"{idx.item():9d} | {val.item():8.3f}\")\n",
    "    \n",
    "    return top_indices.cpu().numpy(), top_values.cpu().numpy()\n",
    "\n",
    "# Example: Find active features for your prompt\n",
    "if __name__ == \"__main__\":\n",
    "    find_active_features(\"I am feeling good\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "005409fc-a6ee-42f2-90d2-270e1101f5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fad07ca139c45538b616b9c4327d67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec22a5705a034ec9804969c91a7e0134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: I am feeling good about my relationship with my daughter. She is a very bright and beautiful girl. She has been my best friend for over two years now. She is always there for me when I need her. She is always there for me when I need her. She is always there for me when I need her. She is always\n",
      "Steered:  I am feeling good about my performance. I am not a professional athlete. I am not a professional athlete. I am not a professional athlete. I am not a professional athlete. I am not a professional athlete. I am not a professional athlete. I am not a professional athlete. I am not a professional athlete. I am not a\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "# grab the decoder vector for that feature\n",
    "steering_vector = sae.W_dec[1000].to(model.cfg.device)\n",
    "\n",
    "# --- 3. Hook that adds it into the residual stream ---\n",
    "def steering_hook(resid_pre, hook):\n",
    "    # only add to the last token’s residual\n",
    "    resid_pre[:, -1, :] += steering_vector * strength\n",
    "    return resid_pre\n",
    "\n",
    "# --- 4. Generate with & without steering ---\n",
    "prompt = \"I am feeling good\"\n",
    "\n",
    "# Unsteered baseline\n",
    "baseline = model.generate(\n",
    "    prompt,\n",
    "    max_new_tokens=64,\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "# Steered via the same SAE feature\n",
    "with model.hooks(fwd_hooks=[(hook_name, steering_hook)]):\n",
    "    steered = model.generate(\n",
    "        prompt,\n",
    "        max_new_tokens=64,\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "print(\"Baseline:\", baseline)\n",
    "print(\"Steered: \", steered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b335fb-30d7-4f97-8a9b-ba486ae1bdae",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f9d20e2-3dd8-402c-be1f-0677389314e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "SAE attaches to hook: blocks.7.hook_resid_pre\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"gpt2-small\"\n",
    "RELEASE    = \"gpt2-small-res-jb\"\n",
    "SAE_ID     = \"blocks.7.hook_resid_pre\"\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Load model and SAE ---\n",
    "model = HookedSAETransformer.from_pretrained(MODEL_NAME, device=DEVICE)\n",
    "sae, sae_cfg, *_ = SAE.from_pretrained(\n",
    "    release=RELEASE,\n",
    "    sae_id=SAE_ID,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "hook_name = sae_cfg[\"hook_point\"]\n",
    "print(f\"SAE attaches to hook: {hook_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b097b2c3-d41f-4a6c-aa8b-9e6b444a44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NEURONPEDIA-STYLE STEERING: ADDITION-BASED\n",
    "# =============================================================================\n",
    "\n",
    "def add_feature_steering(sae_acts: torch.Tensor, hook, pos: int, feature_id: int, strength: float):\n",
    "    \"\"\"\n",
    "    Neuronpedia-style feature steering using ADDITION instead of multiplication.\n",
    "    This adds a constant value to the feature activation.\n",
    "    \n",
    "    Args:\n",
    "        sae_acts: SAE activations tensor [batch, seq_len, n_features]\n",
    "        hook: TransformerLens hook object\n",
    "        pos: Position to steer (None for all positions, int for specific position)\n",
    "        feature_id: Which SAE feature to steer\n",
    "        strength: How much to ADD to the feature activation\n",
    "    \"\"\"\n",
    "    if pos is None:\n",
    "        # Steer all positions\n",
    "        sae_acts[:, :, feature_id] += strength\n",
    "    else:\n",
    "        # Steer specific position\n",
    "        sae_acts[:, pos, feature_id] += strength\n",
    "    return sae_acts\n",
    "\n",
    "def neuronpedia_steer_generate(\n",
    "    prompt: str, \n",
    "    feature_id: int, \n",
    "    strength: float, \n",
    "    pos: int = None,\n",
    "    max_new_tokens: int = 20,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text with Neuronpedia-style SAE feature steering.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text prompt\n",
    "        feature_id: SAE feature index to steer\n",
    "        strength: Steering strength (positive = increase, negative = decrease)\n",
    "        pos: Token position to steer (None = all positions, -1 = last token only)\n",
    "        max_new_tokens: Number of tokens to generate\n",
    "        temperature: Sampling temperature (0 = greedy, higher = more random)\n",
    "    \n",
    "    Returns:\n",
    "        Generated text string\n",
    "    \"\"\"\n",
    "    tokens = model.to_tokens(prompt, prepend_bos=True).to(DEVICE)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Run model with SAE and steering hook\n",
    "        logits = model.run_with_hooks_with_saes(\n",
    "            tokens, \n",
    "            saes=[sae],\n",
    "            fwd_hooks=[\n",
    "                (\n",
    "                    hook_name + \".hook_sae_acts_post\",\n",
    "                    partial(add_feature_steering, pos=pos, feature_id=feature_id, strength=strength)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Sample next token\n",
    "        if temperature == 0:\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1).unsqueeze(-1)\n",
    "        else:\n",
    "            probs = torch.softmax(logits[:, -1, :] / temperature, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "        \n",
    "        tokens = torch.cat([tokens, next_token], dim=1)\n",
    "    \n",
    "    return model.to_string(tokens)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f7a00c1-27e4-4f91-8ff4-ad97e448f8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASELINE (No Steering) ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f93589a6e7c480ebdb9971eda0b9797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: I am feeling good. I am feeling good.\n",
      "\n",
      "I am feeling good.\n",
      "\n",
      "I am feeling good.\n",
      "\n",
      "I am feeling good.\n",
      "\n",
      "I am feeling good.\n",
      "\n",
      "I am feeling good.\n",
      "\n",
      "I am feeling good.\n",
      "\n",
      "I am feeling good.\n",
      "\n",
      "I am feeling good.\n",
      "\n",
      "\n",
      "\n",
      "=== NEURONPEDIA-STYLE STEERING (Addition) ===\n",
      "Addition (+10.0): <|endoftext|>I am feeling good about it, but I really don't want to feel bad about it. I'm not going to get in trouble. I don't care about it. I don't care about what's going on in my life. I don't care about feeling good about it. I don't care about what's going on in\n",
      "Addition (-5.0): <|endoftext|>I am feeling good about this, I am not happy about it. I am not happy about it. I am not happy about my wife's feelings. I am not happy about her feelings. I am not happy about her feelings. I am not happy about my husband's feelings. I am happy about her feelings. I am not happy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"I am feeling good\"\n",
    "    feature_id = 1000  \n",
    "\n",
    "\n",
    "    neuronpedia_settings = {\n",
    "        'max_new_tokens': 64,\n",
    "        'temperature': 0.5, \n",
    "        'freq_penalty': 1.0,  # No frequency penalty\n",
    "        'seed': 16  # Manual seed from interface\n",
    "    }\n",
    "    print(\"=== BASELINE (No Steering) ===\")\n",
    "    baseline = model.generate(\n",
    "        prompt, \n",
    "        max_new_tokens=neuronpedia_settings['max_new_tokens'], \n",
    "        temperature=neuronpedia_settings['temperature'], \n",
    "        do_sample=True\n",
    "    )\n",
    "    print(f\"Baseline: {baseline}\")\n",
    "\n",
    "\n",
    "    STRENGTH = 10\n",
    "    MAX_NEW_TOKENS = 64\n",
    "    TEMPERATURE = 0.5\n",
    "    \n",
    "    print(\"\\n=== NEURONPEDIA-STYLE STEERING (Addition) ===\")\n",
    "    # Positive steering - enhance the feature\n",
    "    steered_pos = neuronpedia_steer_generate(\n",
    "        prompt=prompt,\n",
    "        feature_id=feature_id,\n",
    "        strength=5.0,  # Add 10.0 to feature activation\n",
    "        pos=None,       # Steer all positions\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=TEMPERATURE\n",
    "    )\n",
    "    print(f\"Addition (+10.0): {steered_pos}\")\n",
    "    \n",
    "    # Negative steering - suppress the feature\n",
    "    steered_neg = neuronpedia_steer_generate(\n",
    "        prompt=prompt,\n",
    "        feature_id=feature_id,\n",
    "        strength=-5.0,  # Subtract 5.0 from feature activation\n",
    "        pos=None,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=TEMPERATURE\n",
    "    )\n",
    "    print(f\"Addition (-5.0): {steered_neg}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e5c21c-3c2b-4734-a22d-110bf4f6b1d8",
   "metadata": {},
   "source": [
    "# For a list of neurons steer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4e1e887-eb3e-46b4-9729-9bac015d231f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>layer</th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>Neuron_ID</th>\n",
       "      <th>Contribution</th>\n",
       "      <th>Explanation</th>\n",
       "      <th>llm_decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>att</td>\n",
       "      <td>22</td>\n",
       "      <td>sad</td>\n",
       "      <td>sad</td>\n",
       "      <td>9418</td>\n",
       "      <td>179776.0</td>\n",
       "      <td>attends to emotional tokens from sorrow-relat...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>mlp</td>\n",
       "      <td>22</td>\n",
       "      <td>sad</td>\n",
       "      <td>sad</td>\n",
       "      <td>3164</td>\n",
       "      <td>16384.0</td>\n",
       "      <td>references to the concept of challenges or con...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>mlp</td>\n",
       "      <td>18</td>\n",
       "      <td>sad</td>\n",
       "      <td>sad</td>\n",
       "      <td>4011</td>\n",
       "      <td>15376.0</td>\n",
       "      <td>expressions of sadness or loss</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>mlp</td>\n",
       "      <td>17</td>\n",
       "      <td>sad</td>\n",
       "      <td>sad</td>\n",
       "      <td>8719</td>\n",
       "      <td>11881.0</td>\n",
       "      <td>references to death and dying</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>res</td>\n",
       "      <td>15</td>\n",
       "      <td>sad</td>\n",
       "      <td>sad</td>\n",
       "      <td>13368</td>\n",
       "      <td>11664.0</td>\n",
       "      <td>elements related to internal strength and emot...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type  layer class1 class2  Neuron_ID  Contribution  \\\n",
       "196  att     22    sad    sad       9418      179776.0   \n",
       "735  mlp     22    sad    sad       3164       16384.0   \n",
       "753  mlp     18    sad    sad       4011       15376.0   \n",
       "803  mlp     17    sad    sad       8719       11881.0   \n",
       "814  res     15    sad    sad      13368       11664.0   \n",
       "\n",
       "                                           Explanation llm_decision  \n",
       "196   attends to emotional tokens from sorrow-relat...          yes  \n",
       "735  references to the concept of challenges or con...          yes  \n",
       "753                     expressions of sadness or loss          yes  \n",
       "803                      references to death and dying          yes  \n",
       "814  elements related to internal strength and emot...          yes  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path = \"./datasets/sad_vs_sad.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df = df[df[\"llm_decision\"] == \"yes\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c58ff6b-1999-4b0b-8899-54ef2bf27e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"I'm waiting to hear back from my doctor about my test results.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7cefffa2-d560-4654-9375-f11d399278d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de3ad99f8bb4b4892cd4ca75f578f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline:   I am feeling good. I am feeling good. I am feeling good. I am feeling good. I am feeling good. I am feeling good. I am feeling good. I am feeling good. I am feeling good. I am feeling good. I am feeling good. I am feeling good. I am feeling good. I am feeling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bde7d9819042e8ae91f1307a252bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steered +1: <|endoftext|>I am feeling good about my life. I am feeling good about myself. I am feeling good about everyone I know. I am feeling good about myself. I am feeling good about myself. I am feeling good about myself. I am feeling good about myself. I am feeling good about myself. I am feeling good about myself. I am\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2e5efda9e8480f8867d96a0b9173dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steered -1: <|endoftext|>I am feeling good about my life. I am feeling good about myself. I am feeling good about everyone I know. And I am feeling good about myself. I am feeling good about myself.\n",
      "\n",
      "I am feeling good about myself. I am feeling good about myself. I am feeling good about myself. I am feeling good about myself\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME = \"gpt2-small\"\n",
    "RELEASE    = \"gpt2-small-res-jb\"\n",
    "SAE_ID     = \"blocks.7.hook_resid_pre\"\n",
    "\n",
    "model = HookedSAETransformer.from_pretrained(MODEL_NAME, device=DEVICE)\n",
    "sae, sae_cfg, _ = SAE.from_pretrained(release=RELEASE, sae_id=SAE_ID, device=DEVICE)\n",
    "\n",
    "hook_point = sae_cfg[\"hook_point\"]       # e.g. \"blocks.7.hook_resid_pre\"\n",
    "feature_id = 1000\n",
    "strength   = 1.0\n",
    "\n",
    "# get the decoder vector\n",
    "v = sae.W_dec[feature_id].to(DEVICE)\n",
    "\n",
    "def simple_additive(resid_pre, hook, steering_vector, strength):\n",
    "    resid_pre[:, -1, :] += steering_vector * strength\n",
    "    return resid_pre\n",
    "\n",
    "def steer_simple_add(prompt, strength, max_new_tokens=64, temperature=0.5):\n",
    "    # -- set seed globally --\n",
    "    torch.manual_seed(16)\n",
    "    if DEVICE==\"cuda\":\n",
    "        torch.cuda.manual_seed(16)\n",
    "\n",
    "    tokens = model.to_tokens(prompt, prepend_bos=True).to(DEVICE)\n",
    "    fwd_hook = (hook_point,\n",
    "                partial(simple_additive, steering_vector=v, strength=strength))\n",
    "    with model.hooks(fwd_hooks=[fwd_hook]):\n",
    "        out = model.generate(\n",
    "            tokens,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "        )\n",
    "    return model.tokenizer.decode(out[0])\n",
    "\n",
    "prompt = \"I am feeling good\"\n",
    "print(\"Baseline:  \", model.generate(prompt, max_new_tokens=64, temperature=0.5))\n",
    "print(\"Steered +1:\", steer_simple_add(prompt, strength=5.0))\n",
    "print(\"Steered -1:\", steer_simple_add(prompt, strength=-5.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431bb029-5c0e-40de-a6e2-a9764f599300",
   "metadata": {},
   "source": [
    "# FROM SAELENS SOURCE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8af9814-a3cc-4790-b504-bddfb1d32466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14e18ddbeb14a2ea4b9731e16c40f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "# model = HookedSAETransformer.from_pretrained(\"gpt2-small\", device=\"cuda\")\n",
    "# sae, sae_cfg, _ = SAE.from_pretrained(\n",
    "#     release=\"gpt2-small-res-jb\",  # <- Release name\n",
    "#     sae_id=\"blocks.7.hook_resid_pre\",  # <- SAE id (not always a hook point!)\n",
    "#     device=\"cuda\",\n",
    "# )\n",
    "\n",
    "# ==========================================================================\n",
    "\n",
    "device = \"cuda\"\n",
    "MODEL_NAME = \"gemma-2-2b\"\n",
    "\n",
    "SAE_TYPE = \"res\"    # res/ mlp/ att\n",
    "LAYER = 25\n",
    "SAE_ID = f\"layer_{LAYER}/width_16k/canonical\"\n",
    "RELEASE = f\"gemma-scope-2b-pt-{SAE_TYPE}-canonical\"\n",
    "\n",
    "\n",
    "model = HookedSAETransformer.from_pretrained(MODEL_NAME, device=\"cuda\")\n",
    "sae, cfg, _ = SAE.from_pretrained(\n",
    "    release=RELEASE,  # <- Release name\n",
    "    sae_id=SAE_ID,  # <- SAE id (not always a hook point!)\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e608cc3-d752-4226-b8f2-88946196eff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6147f2413b48425f9068c11c1acb5b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am feeling good about winning the recent National Capital Comic-Con. The announcement was made on May 6, and I have been feeling a little under the weather since then. All I can say: I was so sick after the National Capital event, I had trouble pronouncing the word CHEESE. I thought I had CHEESA. I’\n"
     ]
    }
   ],
   "source": [
    "# Generate text without steering for comparison\n",
    "prompt = \"I am feeling good\"\n",
    "\n",
    "def generate_without_steering(model, sae, prompt, max_new_tokens):\n",
    "    normal_text = model.generate(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        stop_at_eos=False if model.cfg.device == \"mps\" else True,\n",
    "        prepend_bos=sae.cfg.prepend_bos,\n",
    "    )\n",
    "    return normal_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6831d3c7-949d-4ae7-926c-b3f4c1e9693a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1904b0d28669439086667e3e000346fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steered text:\n",
      "<bos>I am feeling good about the way the last week has gone.  I’ve been working on a lot of things and getting a lot done.  I’ve been doing a lot of reading and research and writing.  I’ve been doing a lot of planning and strategizing.  I’ve been doing a lot of\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def steering(\n",
    "    activations, hook, steering_strength=1.0, steering_vector=None, max_act=1.0\n",
    "):\n",
    "    # Note if the feature fires anyway, we'd be adding to that here.\n",
    "    return activations + max_act * steering_strength * steering_vector\n",
    "\n",
    "\n",
    "def generate_with_steering(\n",
    "    model,\n",
    "    sae,\n",
    "    prompt,\n",
    "    steering_feature,\n",
    "    max_act,\n",
    "    steering_strength=1.0,\n",
    "    max_new_tokens=64,\n",
    "):\n",
    "    input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
    "\n",
    "    steering_vector = sae.W_dec[steering_feature].to(model.cfg.device)\n",
    "\n",
    "    steering_hook = partial(\n",
    "        steering,\n",
    "        steering_vector=steering_vector,\n",
    "        steering_strength=steering_strength,\n",
    "        max_act=max_act,\n",
    "    )\n",
    "\n",
    "    # standard transformerlens syntax for a hook context for generation\n",
    "    with model.hooks(fwd_hooks=[(sae.cfg.hook_name, steering_hook)]):\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.5,\n",
    "            top_p=0.9,\n",
    "            stop_at_eos=False if device == \"mps\" else True,\n",
    "            prepend_bos=sae.cfg.prepend_bos,\n",
    "        )\n",
    "\n",
    "    return model.tokenizer.decode(output[0])\n",
    "\n",
    "# Generate text with steering\n",
    "steered_text = generate_with_steering(\n",
    "    model=model, sae=sae, prompt=prompt, steering_feature=1, max_act=1.0, steering_strength=2.0\n",
    ")\n",
    "print(\"Steered text:\")\n",
    "print(steered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04976b7-5837-4b78-9a10-ff4429c52b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without Steering\")\n",
    "print(generate_without_steering(model, sae, prompt, max_new_tokens=64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bd713a-9533-4645-bd82-3de26f5634fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Steered text:\")\n",
    "steered_text = generate_with_steering(\n",
    "    model=model, sae=sae, prompt=prompt, steering_feature=1, max_act=1.0, steering_strength=2.0\n",
    ")\n",
    "print(steered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5305f5-689f-42b1-9f0d-644973e18a48",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a6b165f-a3b3-4e2f-9636-bd56595fe695",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f2eb279-cafd-4acd-af70-423d195b91e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msae_lens\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ActivationsStore\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# a convenient way to instantiate an activation store is to use the from_sae method\u001b[39;00m\n\u001b[32m      5\u001b[39m activation_store = ActivationsStore.from_sae(\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     model=\u001b[43mmodel\u001b[49m,\n\u001b[32m      7\u001b[39m     dataset=sae.cfg.metadata.dataset_path,\n\u001b[32m      8\u001b[39m     sae=sae,\n\u001b[32m      9\u001b[39m     streaming=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# fairly conservative parameters here so can use same for larger\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# models without running out of memory.\u001b[39;00m\n\u001b[32m     12\u001b[39m     store_batch_size_prompts=\u001b[32m8\u001b[39m,\n\u001b[32m     13\u001b[39m     train_batch_size_tokens=\u001b[32m4096\u001b[39m,\n\u001b[32m     14\u001b[39m     n_batches_in_buffer=\u001b[32m32\u001b[39m,\n\u001b[32m     15\u001b[39m     device=device,\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# instantiate an object to hold activations from a dataset\n",
    "from sae_lens import ActivationsStore\n",
    "\n",
    "# a convenient way to instantiate an activation store is to use the from_sae method\n",
    "activation_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    dataset=sae.cfg.metadata.dataset_path,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    # fairly conservative parameters here so can use same for larger\n",
    "    # models without running out of memory.\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=4096,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf9774c-1c35-4d87-8369-da8a1c9e0214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate an object to hold activations from a dataset\n",
    "from sae_lens import ActivationsStore\n",
    "\n",
    "# a convenient way to instantiate an activation store is to use the from_sae method\n",
    "activation_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    dataset=sae.cfg.metadata.dataset_path,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    # fairly conservative parameters here so can use same for larger\n",
    "    # models without running out of memory.\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=4096,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import re\n",
    "\n",
    "\n",
    "def find_max_activation(model, sae, activation_store, feature_idx, num_batches=100):\n",
    "    \"\"\"\n",
    "    Find the maximum activation for a given feature index. This is useful for\n",
    "    calibrating the right amount of the feature to add.\n",
    "    \"\"\"\n",
    "    max_activation = 0.0\n",
    "\n",
    "    pbar = tqdm(range(num_batches))\n",
    "    for _ in pbar:\n",
    "        tokens = activation_store.get_batch_tokens()\n",
    "\n",
    "        layer = int(re.search(r\"\\.(\\d+)\\.\", sae.cfg.metadata.hook_name).group(1))  # type: ignore\n",
    "        _, cache = model.run_with_cache(\n",
    "            tokens,\n",
    "            stop_at_layer=layer + 1,\n",
    "            names_filter=[sae.cfg.metadata.hook_name],\n",
    "        )\n",
    "        sae_in = cache[sae.cfg.metadata.hook_name]\n",
    "        feature_acts = sae.encode(sae_in).squeeze()\n",
    "\n",
    "        feature_acts = feature_acts.flatten(0, 1)\n",
    "        batch_max_activation = feature_acts[:, feature_idx].max().item()\n",
    "        max_activation = max(max_activation, batch_max_activation)\n",
    "\n",
    "        pbar.set_description(f\"Max activation: {max_activation:.4f}\")\n",
    "\n",
    "    return max_activation\n",
    "\n",
    "max_act = find_max_activation(model, sae, activation_store, steering_feature)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
