{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"15JAPIUQAxeuvz12oXpsPD3KJhUVLwNHW","authorship_tag":"ABX9TyPhpJbXc1sgh63WbonnrXXW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"bIPi28HAa0Vq","executionInfo":{"status":"ok","timestamp":1752840263070,"user_tz":-60,"elapsed":1597,"user":{"displayName":"Yagol Xu Chen","userId":"00929278906170525619"}}},"outputs":[],"source":["import pandas as pd\n","import ast\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import ast\n","import csv\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)"]},{"cell_type":"code","source":["FOLDER_PATH = \"/content/drive/MyDrive/LLM Interpretability/\"\n","DATASET_NAMES = [\"emotion\", \"math\", \"mmlu\", \"programming\"]\n","DATASET_PATHS = [f\"{FOLDER_PATH}/{name}_processed.csv\" for name in DATASET_NAMES]\n","# Load and concatenate all datasets\n","df = pd.concat([pd.read_csv(path, encoding=\"utf-8-sig\") for path in DATASET_PATHS], ignore_index=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"NLM2DGnga6tE","executionInfo":{"status":"error","timestamp":1752840526085,"user_tz":-60,"elapsed":42769,"user":{"displayName":"Yagol Xu Chen","userId":"00929278906170525619"}},"outputId":"7d96f168-c88d-48e5-98a5-e56eadc0796e"},"execution_count":4,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/LLM Interpretability//programming_processed.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4-3940879263.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mDATASET_PATHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"{FOLDER_PATH}/{name}_processed.csv\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDATASET_NAMES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load and concatenate all datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8-sig\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDATASET_PATHS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-4-3940879263.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mDATASET_PATHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"{FOLDER_PATH}/{name}_processed.csv\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDATASET_NAMES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load and concatenate all datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8-sig\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDATASET_PATHS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/LLM Interpretability//programming_processed.csv'"]}]},{"cell_type":"code","source":["df.dropna(inplace=True)    # for analysis and vis keep all\n","df.head()"],"metadata":{"id":"LnmscSYHbJ7K","executionInfo":{"status":"aborted","timestamp":1752840526094,"user_tz":-60,"elapsed":2,"user":{"displayName":"Yagol Xu Chen","userId":"00929278906170525619"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["counts = df['type'].value_counts()\n","print(counts)"],"metadata":{"id":"QCbsJKVJbRir","executionInfo":{"status":"aborted","timestamp":1752840526106,"user_tz":-60,"elapsed":1,"user":{"displayName":"Yagol Xu Chen","userId":"00929278906170525619"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.describe()"],"metadata":{"id":"iZjb2JTUbSiP","executionInfo":{"status":"aborted","timestamp":1752840526126,"user_tz":-60,"elapsed":10,"user":{"displayName":"Yagol Xu Chen","userId":"00929278906170525619"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.count()"],"metadata":{"id":"JeUgkPaybUyn","executionInfo":{"status":"aborted","timestamp":1752840526136,"user_tz":-60,"elapsed":1,"user":{"displayName":"Yagol Xu Chen","userId":"00929278906170525619"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create freq of vectors [0, 0, 0, .., 0]\n","VECTOR_SIZE_SAE = 16384\n","def to_vector(specialized_df, col_name, binary_vector=False):\n","    \"\"\"\n","    input:\n","        specialized_df: pandas df\n","        col_name: col name with its layer and type\n","    output:\n","        vector of size 16500\n","            max neuron 16383\n","    \"\"\"\n","    shared_neurons = [0] * VECTOR_SIZE_SAE\n","    for row_list_str in specialized_df[col_name]:      # TODO: optimize with counter\n","        # skip missing entries\n","        if pd.isna(row_list_str):\n","            continue\n","        neuron_list = ast.literal_eval(row_list_str)\n","        for neuron in neuron_list:\n","            if binary_vector:\n","                shared_neurons[neuron] = 1\n","            else:\n","                shared_neurons[neuron] += 1\n","\n","    return shared_neurons\n","\n","# to_vector(df, \"gemma-scope-2b-pt-res-canonical-layer0-token_feature_ids\")\n","# first_list = ast.literal_eval(df[\"gemma-scope-2b-pt-res-canonical-layer0-token_feature_ids\"].iloc[0] )  # now a Python list of ints"],"metadata":{"id":"nGaOIgvZbWOi","executionInfo":{"status":"aborted","timestamp":1752840526144,"user_tz":-60,"elapsed":1,"user":{"displayName":"Yagol Xu Chen","userId":"00929278906170525619"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATASET_TYPES = {\n","    \"empathetic_dialogue\": df.loc[df[\"type\"] == \"empathetic_dialogue\", \"category\"].unique(),\n","    \"math\":                df.loc[df[\"type\"] == \"math\",                \"category\"].unique(),\n","    \"mmlu\":                df.loc[df[\"type\"] == \"mmlu\",                \"category\"].unique(),\n","    \"programming\":         df.loc[df[\"type\"] == \"programming\",         \"category\"].unique(),\n","}\n","print(DATASET_TYPES)\n","\n","\n","# DATASET_TYPES = [\n","#                 \"empathetic_dialogue\",\n","#                 \"math\",\n","#                 \"mmlu\",\n","#                 \"programming\"\n","#                 ]"],"metadata":{"id":"4lmy6ROhbeOw","executionInfo":{"status":"aborted","timestamp":1752840526153,"user_tz":-60,"elapsed":1,"user":{"displayName":"Yagol Xu Chen","userId":"00929278906170525619"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# WITHIN CLASS\n","\n","import itertools\n","\n","TYPE = [\"res\", \"mlp\", \"att\"]\n","LAYERS = range(26)\n","\n","results = []\n","\n","for typ in TYPE:\n","    for layer in LAYERS:\n","        col = f\"gemma-scope-2b-pt-{typ}-canonical-layer_{layer}/width_16k/canonical-token_feature_ids\"\n","\n","        # 2. For each dataset type, loop over all unique (category1, category2) pairs\n","        for dataset_type, categories in DATASET_TYPES.items():\n","            for class1, class2 in itertools.combinations(categories, 2):\n","                # filter down to just those two classes within this dataset type\n","                df1 = df[(df[\"type\"] == dataset_type) & (df[\"category\"] == class1)]\n","                df2 = df[(df[\"type\"] == dataset_type) & (df[\"category\"] == class2)]\n","\n","                vec1 = to_vector(df1, col, False)\n","                vec2 = to_vector(df2, col, False)\n","\n","                common_count, activation_pct = percentage_activation(vec1, vec2)\n","\n","                results.append({\n","                    \"Type\": typ,\n","                    \"Layer\": layer,\n","                    \"DatasetType\": dataset_type,\n","                    \"Class1\": class1,\n","                    \"Class2\": class2,\n","                    \"ActivationPercentage\": activation_pct\n","                })\n","\n","# at the end you can wrap results in a DataFrame if you like:\n","# import pandas as pd\n","# results_df = pd.DataFrame(results)\n","# print(results_df)\n"],"metadata":{"id":"9mAtcAknbejK"},"execution_count":null,"outputs":[]}]}